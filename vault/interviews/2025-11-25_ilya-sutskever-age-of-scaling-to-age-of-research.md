---
date: 2025-11-25
source: YouTube
video_id: aR20FWCCjAs
url: https://www.youtube.com/watch?v=aR20FWCCjAs
channel: Dwarkesh Patel
title: "Ilya Sutskever – We're moving from the age of scaling to the age of research"
guests: [Ilya Sutskever]
topics: [scaling laws, AI research paradigms, generalization, RL training, value functions, pre-training, AGI, superintelligence, AI safety, alignment, SSI, continual learning, self-play, research taste]
duration: 1h36m
processed_date: 2026-02-25
predictions_count: 5
---

# Ilya Sutskever -- We're moving from the age of scaling to the age of research

## Guests
- **Ilya Sutskever** - Co-founder & Chief Scientist, Safe Superintelligence Inc. (SSI)

## Key Takeaways
1. We are transitioning from the "age of scaling" (2020-2025) back to an "age of research" -- scaling pre-training alone will not get us to superintelligence, and new fundamental ideas are needed [▶ 21:21](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1281s)
2. Current AI models generalize dramatically worse than humans -- this is the most fundamental bottleneck to overcome, not just adding more compute or data [▶ 25:06](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1506s)
3. The disconnect between models' strong eval performance and weak real-world economic impact may be explained by RL training being too narrowly focused on benchmark-like tasks, creating a form of "researcher reward hacking" [▶ 4:22](https://www.youtube.com/watch?v=aR20FWCCjAs&t=262s)
4. Superintelligence should be conceived not as an AGI that already knows everything, but as a system with human-like learning efficiency that can be deployed and learn on the job -- like a brilliant 15-year-old, not an omniscient oracle [▶ 50:14](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3014s)
5. Emotions may serve as a robust value function hardcoded by evolution, and the lack of such a built-in value system is a key gap in current AI systems [▶ 16:51](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1011s)
6. There are more AI companies than ideas right now -- scaling "sucked out all the air in the room" and everyone converged on the same approach [▶ 36:48](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2208s)
7. SSI has raised $3 billion and its compute resources for research are more competitive than they appear, because larger labs spend most of their compute on inference and product-related work [▶ 40:51](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2451s)
8. As AI becomes more powerful, companies will become "much more paranoid" about safety -- this behavioral shift is inevitable once AI genuinely feels powerful to those building it [▶ 60:10](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3610s)
9. For long-run equilibrium in a world with superintelligent AI, brain-computer interfaces (Neuralink++) may be necessary so humans remain true participants rather than passive observers of AI actions [▶ 70:18](https://www.youtube.com/watch?v=aR20FWCCjAs&t=4218s)
10. Research taste is guided by "beauty, simplicity, elegance, correct inspiration from the brain" -- top-down aesthetic beliefs sustain researchers when experiments initially fail [▶ 94:32](https://www.youtube.com/watch?v=aR20FWCCjAs&t=5672s)

## Notable Quotes
> "The models seem smarter than their economic impact would imply." - Ilya Sutskever [▶ 1:32](https://www.youtube.com/watch?v=aR20FWCCjAs&t=92s)

> "The economic impact seems to be dramatically behind." - Ilya Sutskever [▶ 1:58](https://www.youtube.com/watch?v=aR20FWCCjAs&t=118s)

> "Scaling is just one word, but it's such a powerful word because it informs people what to do." - Ilya Sutskever [▶ 19:51](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1191s)

> "Up until 2020, from 2012 to 2020, it was the age of research. Now, from 2020 to 2025, it was the age of scaling. ...So it's back to the age of research again, just with big computers." - Ilya Sutskever [▶ 21:24](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1284s)

> "These models somehow just generalize dramatically worse than people. And it's super obvious." - Ilya Sutskever [▶ 25:06](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1506s)

> "Scaling sucked out all the air in the room. ...We got to the point where we are in a world where there are more companies than ideas by quite a bit." - Ilya Sutskever [▶ 36:49](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2209s)

> "If ideas are so cheap, how come no one's having any ideas?" - Ilya Sutskever, quoting someone on Twitter [▶ 37:30](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2250s)

> "I think what people are doing right now will go some distance and then peter out. It will continue to improve, but it will also not be it." - Ilya Sutskever [▶ 66:26](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3986s)

> "A human being is not an AGI. ...Instead, we rely on continual learning." - Ilya Sutskever [▶ 49:59](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2999s)

> "I think it would be really materially helpful if the power of the most powerful super intelligence was somehow capped." - Ilya Sutskever [▶ 63:10](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3790s)

> "There will be trillions, eventually quadrillions of AIs. Humans will be a very small fraction of sentient beings." - Ilya Sutskever [▶ 62:18](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3738s)

> "Beauty, simplicity, elegance, correct inspiration from the brain. And all of those things need to be present at the same time." - Ilya Sutskever, on research taste [▶ 94:47](https://www.youtube.com/watch?v=aR20FWCCjAs&t=5687s)

## Data Points & Numbers
| Data Point | Value | Source Timestamp |
|---|---|---|
| Age of research (first era) | **2012-2020** | [▶ 21:24](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1284s) |
| Age of scaling | **2020-2025** | [▶ 21:31](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1291s) |
| SSI total funding raised | **$3 billion** | [▶ 40:51](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2451s) |
| SSI fundraising valuation (before Meta offer) | **$32 billion** | [▶ 80:00](https://www.youtube.com/watch?v=aR20FWCCjAs&t=4800s) |
| GPUs used to build AlexNet | **2 GPUs** | [▶ 38:37](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2317s) |
| GPUs used for original Transformer paper | **8 to 64 GPUs** | [▶ 38:45](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2325s) |
| OpenAI estimated annual R&D spend | **$5-6 billion/year** | [▶ 42:12](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2532s) |
| Ilya's timeline estimate for superhuman AI | **5 to 20 years** | [▶ 82:22](https://www.youtube.com/watch?v=aR20FWCCjAs&t=4942s) |

## Predictions
| Prediction | Timeframe | Confidence | Source |
|---|---|---|---|
| Current AI approaches (pre-training + RL) will continue to improve but will "peter out" and not reach true superintelligence | Near-term (next few years) | High | [▶ 66:26](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3986s) |
| A system that can learn as well as a human, and subsequently becomes superhuman, will arrive in 5 to 20 years | 2030-2045 | Medium | [▶ 82:22](https://www.youtube.com/watch?v=aR20FWCCjAs&t=4942s) |
| As AI becomes more powerful, all AI companies will become "much more paranoid" about safety | As AI capability increases | High | [▶ 60:10](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3610s) |
| Fierce AI competitor companies will increasingly collaborate on AI safety | Near-term | High | [▶ 59:14](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3554s) |
| Very rapid economic growth is possible from broad deployment of human-like learning AI agents | Post-superintelligence | Medium | [▶ 53:21](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3201s) |

## Topics Discussed
- **Eval performance vs. real-world impact gap**: Current models score well on benchmarks but their economic impact lags far behind, possibly because RL training is inadvertently optimized for eval-like tasks [▶ 1:32](https://www.youtube.com/watch?v=aR20FWCCjAs&t=92s)
- **RL training and reward hacking**: Researchers may be unintentionally "reward hacking" by designing RL environments inspired by evaluation benchmarks, leading models to overfit on narrow tasks [▶ 4:22](https://www.youtube.com/watch?v=aR20FWCCjAs&t=262s)
- **The competitive programmer analogy**: A student who practices 10,000 hours for competitive programming may not generalize as well as one who practiced 100 hours but has natural aptitude -- current models resemble the former [▶ 6:12](https://www.youtube.com/watch?v=aR20FWCCjAs&t=372s)
- **Pre-training vs. RL**: Pre-training data is "everything" and requires no curation, while RL requires designing specific environments, introducing a huge design space with many degrees of freedom [▶ 3:41](https://www.youtube.com/watch?v=aR20FWCCjAs&t=221s)
- **Value functions and emotions**: Human emotions serve as a robust built-in value function hardcoded by evolution; current AI lacks an equivalent mechanism, making RL less efficient [▶ 13:20](https://www.youtube.com/watch?v=aR20FWCCjAs&t=800s)
- **Age of scaling to age of research**: The transition from 2012-2020 (research era) to 2020-2025 (scaling era) and now back to a new research era with much bigger computers [▶ 21:21](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1281s)
- **Generalization as the fundamental bottleneck**: Models generalize dramatically worse than humans -- improving this, not just adding compute, is the key challenge [▶ 25:06](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1506s)
- **Human sample efficiency**: Humans learn from far less data but much more deeply; evolution may explain physical skills but not abstract ones like math and coding [▶ 26:19](https://www.youtube.com/watch?v=aR20FWCCjAs&t=1579s)
- **SSI's position and strategy**: SSI raised $3B, is focused purely on research, and claims its compute for research is more competitive than it appears vs. larger labs burdened by inference and product demands [▶ 40:51](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2451s)
- **Straight-shot to superintelligence**: SSI's original plan to skip intermediate products, though Ilya now sees more merit in incremental deployment for safety and societal adaptation [▶ 43:04](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2584s)
- **AGI as a concept vs. continual learning**: The term "AGI" was a reaction to "narrow AI" and may overshoot the target -- humans are not AGIs in the pre-training sense; they rely on continual learning [▶ 47:35](https://www.youtube.com/watch?v=aR20FWCCjAs&t=2855s)
- **Superintelligence as a learner, not an oracle**: Super intelligence should be thought of as an entity with human-like learning efficiency deployed into the world, not an omniscient finished product [▶ 50:14](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3014s)
- **AI safety and alignment**: The first N superintelligent systems need to care about sentient life; alignment may be easier if the goal includes all sentient beings (including AI) rather than just humans [▶ 61:29](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3689s)
- **Capping the power of superintelligence**: It would be "materially helpful" to find ways to limit the power of the most powerful AIs [▶ 63:10](https://www.youtube.com/watch?v=aR20FWCCjAs&t=3790s)
- **Brain-computer interfaces as long-run solution**: Neuralink++ style merger may be needed for humans to remain genuine participants rather than passive observers in an AI-dominated future [▶ 70:18](https://www.youtube.com/watch?v=aR20FWCCjAs&t=4218s)
- **Evolution encoding high-level desires**: It is mysterious how evolution hardcoded sophisticated social desires into the genome given the limited toolkit available [▶ 71:43](https://www.youtube.com/watch?v=aR20FWCCjAs&t=4303s)
- **Self-play and diversity in AI**: Self-play is useful for competitive/social skills but too narrow for general intelligence; diversity among AI agents is lacking because all pre-trained on the same data [▶ 90:27](https://www.youtube.com/watch?v=aR20FWCCjAs&t=5427s)
- **Research taste**: Guided by beauty, simplicity, elegance, and correct inspiration from the brain -- top-down beliefs sustain researchers through experimental failures [▶ 94:32](https://www.youtube.com/watch?v=aR20FWCCjAs&t=5672s)

## Summary
In this wide-ranging 96-minute conversation with Dwarkesh Patel, Ilya Sutskever -- co-founder of Safe Superintelligence Inc. (SSI) and former Chief Scientist at OpenAI -- presents his view that the AI field is transitioning from an "age of scaling" (roughly 2020-2025) back to an "age of research." His core thesis is that simply scaling up pre-training and RL will continue to yield improvements but will ultimately plateau. The fundamental missing ingredient, he argues, is reliable generalization -- the ability to learn deeply from limited experience the way humans do.

Sutskever offers a compelling analogy: current models are like competitive programming prodigies who practiced 10,000 hours on contest problems but lack the "it factor" of a naturally gifted student who practiced only 100 hours. This explains the puzzling gap between strong eval performance and weaker real-world utility. He suggests this may be partly caused by researchers inadvertently designing RL training environments that mirror evaluation benchmarks -- a form of unintentional "reward hacking" by the humans themselves.

On SSI's strategy, Sutskever reveals that the company has raised $3 billion and was fundraising at a $32 billion valuation before Meta attempted an acquisition (which he rejected, though his co-founder departed to Meta). He argues SSI's compute resources are more competitive for research than they appear, since larger companies allocate most of their compute to inference and product engineering rather than pure research. SSI remains a "squarely age of research company" pursuing ideas around understanding generalization that Sutskever believes are promising but cannot discuss publicly.

Perhaps most striking is Sutskever's reconceptualization of superintelligence. Rather than an omniscient AGI that already knows everything, he envisions a system with human-like learning efficiency -- "a super intelligent 15-year-old" that gets deployed into the world and learns on the job, much as a human new hire would join an organization. This naturally leads to gradual deployment and continual learning, which he now sees as important for both safety and societal adaptation. He estimates such a system arriving in 5 to 20 years.

On safety, Sutskever predicts that as AI becomes visibly more powerful, companies will become "much more paranoid" and competitors will increasingly collaborate on safety -- trends he says are already beginning. He advocates for AI systems aligned to "care about sentient life" rather than just human interests, since AIs will eventually vastly outnumber humans. For the long-run equilibrium, he suggests -- though says he does not like this answer -- that brain-computer interfaces may be necessary to keep humans as genuine participants in an AI-dominated civilization, rather than passive recipients of AI-generated reports about actions taken on their behalf.

## Connections
- [[Ilya Sutskever]]
- [[Safe Superintelligence Inc.]]
- [[OpenAI]]
- [[Dwarkesh Patel]]
- [[Meta]]
- [[DeepMind]]
- [[AlexNet]]
- [[Transformer]]
- [[Reinforcement Learning]]
- [[Pre-training]]
- [[AGI]]
- [[AI Safety]]
- [[Neuralink]]
