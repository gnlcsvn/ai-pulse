---
date: 2026-02-17
source: YouTube
video_id: Z19UEZHJzAg
url: https://www.youtube.com/watch?v=Z19UEZHJzAg
channel: 80,000 Hours
title: "By 2050 we could get 10,000 years of technological progress"
guests: [Ajeya Cotra]
topics: [intelligence explosion, AI safety, AI timelines, Open Philanthropy, AGI, AI R&D automation, crunch time, transparency, effective altruism, AI control, biodefense, AI governance]
duration: 2h57m
processed_date: 2026-02-24
predictions_count: 5
---

# By 2050 we could get 10,000 years of technological progress

## Guests
- **Ajeya Cotra** - Senior Advisor, Open Philanthropy (formerly led technical AI safety grantmaking in 2024; AI researcher focusing on timelines, capability evaluations, and threat modeling since 2018)

## Key Takeaways
1. By 2050 the world could look as different from today as today does from the hunter-gatherer era -- roughly 10,000 years of progress compressed into 25 years -- driven by AI automating all intellectual activity [▶ 6:13](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=373s)
2. Cotra expects "top human expert dominating AI" (systems better than the best human experts at all remote computer tasks) to arrive in the early 2030s, which would trigger rapid acceleration across all domains [▶ 10:37](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=637s)
3. There is an almost unfathomable 1,000-to-10,000-fold disagreement among thoughtful experts on how much AGI will speed up economic growth -- from 0.3 percentage points to 1,000%+ per year at peak [▶ 14:23](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=863s)
4. METR's uplift RCT found that AI actually slowed down experienced software developers' performance, though Cotra does not expect that result to persist as models improve [▶ 24:19](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=1459s)
5. All major frontier AI companies (OpenAI, Anthropic, Google DeepMind) plan to use AI itself as a core part of their safety strategy -- incorporating AI labor into alignment, control, and interpretability research as models improve [▶ 57:34](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=3454s)
6. Cotra advocates for frontier AI companies to release their highest internal benchmark scores on a fixed calendar cadence (e.g., quarterly), rather than only at product launch, to provide early warning of an intelligence explosion [▶ 32:16](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=1936s)
7. The most likely failure mode for the "use AI for safety" plan is not that alignment is impossibly hard, but that companies simply will not redirect enough AI labor from capabilities to safety -- perhaps only 100 out of 100,000 AI-equivalent workers would be assigned to safety [▶ 54:53](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=3293s)
8. Cotra would vote for stretching the intelligence explosion transition from the default 1 year to 10-20 years, preferring a slow continuous approach over a hard pause-then-unpause [▶ 77:38](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=4658s)
9. Open Philanthropy may need to shift from funding human salaries to purchasing API credits and GPU time as AI becomes more capable, and should be tracking what fraction of its spending goes to AI compute [▶ 74:29](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=4469s)
10. Cotra took a 4-month sabbatical in late 2024 after burnout from leading technical AI safety grantmaking, and is now considering roles at Redwood Research or METR while continuing as senior advisor at Open Philanthropy [▶ 107:19](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=6439s)

## Notable Quotes
> "I think that there's a pretty good chance that by 2050 the world will look as different from today as today does from the hunter-gatherer era. It's like 10,000 years of progress rather than 25 years of progress driven by AI automating all intellectual activity." - Ajeya Cotra [▶ 6:13](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=373s)

> "How to create a setup where we use control techniques and alignment techniques and interpretability to the point where we feel good about relying on their outputs is a crucial step to figure out. Because it either bottlenecks our progress, because we're checking on everything all the time and slowing things down, or it doesn't bottleneck our progress, but we hand the AIs the power to take over." - Ajeya Cotra [▶ 0:28](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=28s)

> "If they have a hundred thousand really smart human equivalents, maybe only like a hundred of them are working on AI safety, which is maybe still more than they had before in human labor, but not that much compared to how quickly things are going." - Ajeya Cotra [▶ 80:06](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=4806s)

> "People who are worried about X-risk think that the default course of AI is this extremely explosive thing where it overturns society on all dimensions at once in maybe a year or maybe five years or maybe six months or maybe a week. And they're saying we should slow it down to take 10 years maybe. Meanwhile the accelerationists think that by default diffusing and capturing the benefits of AI will take 50 years or 100 years and they want to speed it up to take 35 years." - Ajeya Cotra [▶ 9:41](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=581s)

> "If a really clear early warning sign triggers that we are about to enter into this intelligence explosion, fast takeoff space, then I would vote for at that time shifting that trajectory to be 10 times longer or even longer than that and trying to make that transition as a society in 10 years instead of one year." - Ajeya Cotra [▶ 77:25](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=4645s)

## Data Points & Numbers
| Data Point | Value | Source Timestamp |
|---|---|---|
| Range of expert disagreement on peak AGI economic impact | **1,000-to-10,000-fold** | [▶ 14:35](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=875s) |
| Low-end expert estimate for AGI growth boost | **+0.3 percentage points** | [▶ 14:23](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=863s) |
| High-end expert estimate for peak economic growth rate | **1,000%+ per year** | [▶ 14:35](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=875s) |
| Cotra's timeline for top human expert dominating AI | **Early 2030s** | [▶ 10:37](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=637s) |
| DealBook panel: believed AGI by 2030 | **7 out of ~10 panelists** | [▶ 3:27](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=207s) |
| DealBook panel: believed AI creates more jobs than destroys (10 yrs) | **8 out of 10 panelists** | [▶ 3:27](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=207s) |
| Historical pre-industrial frontier growth rate | **~0.1% per year** | [▶ 18:31](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=1111s) |
| Modern frontier economy growth rate (last 100-150 yrs) | **~2% per year** | [▶ 15:36](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=936s) |
| AI lines of code reported by some company CEOs | **90%** | [▶ 34:53](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=2093s) |
| Cotra's years at Open Philanthropy before first grant | **6+ years** | [▶ 107:47](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=6467s) |
| Emergency grants made during FTX crisis in ~6 weeks | **~50 grants** | [▶ 109:24](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=6564s) |
| Cotra's preferred slowdown factor for intelligence explosion | **10x (1 year to 10-20 years)** | [▶ 77:52](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=4672s) |

## Predictions
| Prediction | Timeframe | Confidence | Source |
|---|---|---|---|
| By 2050 the world will look as different from today as today does from the hunter-gatherer era -- roughly 10,000 years of progress compressed into 25 years | by 2050 | medium | [▶ 6:13](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=373s) |
| Top human expert dominating AI (systems better than any human expert at all remote computer tasks) will arrive in the early 2030s | probably in the early 2030s | medium | [▶ 10:37](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=637s) |
| Top human expert dominating AIs in the cognitive domain will use human physical labor to build robotic actuators, automating the physical supply chain shortly after cognitive AGI | shortly after top human expert dominating AI (early 2030s) | medium | [▶ 11:25](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=685s) |
| If early warning signs trigger, the intelligence explosion transition should be stretched from ~1 year to 10-20 years | in 10 years instead of one year, or 20 years instead of one year | medium | [▶ 1:17:33](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=4653s) |
| Companies will fail to redirect enough AI labor from capabilities to safety during the intelligence explosion -- most likely failure mode for AI safety | during the intelligence explosion (implied early-to-mid 2030s) | medium | [▶ 1:19:55](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=3293s) |

## Topics Discussed
- **The spectrum of AGI impact expectations**: Enormous disagreement between mainstream economists (AI adds 0.3pp to growth) and AI futurists (1,000%+ annual growth), with Cotra explaining why both sides maintain their positions despite dialogue [▶ 14:23](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=863s)
- **Intelligence explosion mechanics**: Three feedback loops required -- software (AI improving AI architectures), hardware (automating chip manufacturing), and physical (robots making more robots) -- referencing Tom Davidson's "Three Types of Intelligence Explosion" [▶ 13:04](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=784s)
- **Measuring AI's real-world impact**: METR's uplift RCT, the LEAP panel (Forecasting Research Institute), and the need for systematic evidence beyond benchmarks including internal company metrics on AI adoption [▶ 24:19](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=1459s)
- **Transparency requirements for frontier AI labs**: Quarterly benchmark reporting, tracking fraction of AI-written and AI-reviewed pull requests, reporting misalignment incidents, and the tension between public disclosure and competitive sensitivity [▶ 32:16](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=1936s)
- **Using AI for defense during "crunch time"**: Redirecting AI labor toward alignment research, biodefense, cybersecurity, epistemics, and coordination -- the central strategy of all major AI companies' safety plans [▶ 48:38](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=2918s)
- **Failure modes for the AI-for-safety plan**: Companies not redirecting enough labor, misaligned AIs undermining safety work, capability imbalance (AI good at offense before defense), and organizational inertia [▶ 54:53](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=3293s)
- **Pausing vs. slowing the intelligence explosion**: Cotra prefers gradually stretching the transition period (1 year to 10-20 years) rather than a binary pause-then-unpause, using the extra time for protective AI labor [▶ 75:07](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=4507s)
- **Open Philanthropy's evolving strategy**: The potential shift from funding human researchers to buying AI compute, the challenge of scaling spending during crunch time, and hedging compute costs by investing in NVIDIA [▶ 74:04](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=4444s)
- **Cotra's career journey and sabbatical**: Burnout from grant-making under new leadership after Holden Karnofsky's departure, reflections on perfectionism in management, and the tension between depth-oriented research and high-volume philanthropy [▶ 107:04](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=6424s)
- **Effective altruism's role and evolution**: EA as an incubator for speculative cause areas (digital sentience, space governance, value lock-in), erosion of radical transparency norms, and the desire for more spiritual/existential grounding in the community [▶ 151:26](https://www.youtube.com/watch?v=Z19UEZHJzAg&t=9086s)

## Summary
In this nearly three-hour conversation, Rob Wiblin interviews Ajeya Cotra, a senior advisor at Open Philanthropy who has been deeply influential in AI safety circles for her work on timelines, capability evaluations, and threat modeling. The central thesis of the interview is that the world faces a potential "intelligence explosion" that could compress thousands of years of technological progress into a period as short as one to two years, and that the disagreement among thoughtful experts about whether this will happen is staggering -- ranging from predictions of modest 0.3 percentage point growth boosts to 1,000%+ annual economic growth rates.

Cotra lays out her modal expectation: by the early 2030s, AI will reach "top human expert dominating" capability across all remote cognitive tasks, and shortly after, will begin automating the physical supply chain (chip manufacturing, robotics, raw materials) needed to produce more AI. She explains the two competing "outside views" that sustain the disagreement -- mainstream economists point to 150 years of stable 2% growth despite revolutionary technologies, while AI futurists point to 10,000 years of accelerating growth driven by feedback loops between population and innovation. Each side has a built-in "error theory" for why the other is wrong, making convergence difficult.

A major portion of the discussion centers on what Cotra calls "crunch time" -- the window when AI has automated AI R&D but has not yet become uncontrollably powerful. She argues that society's best strategy is to redirect AI labor during this window toward protective activities: alignment research, biodefense, cybersecurity, improved collective decision-making, and coordination between major powers. This is already the stated plan of all major frontier AI labs, but Cotra worries the most likely failure mode is simply that companies will not actually redirect enough resources from capabilities to safety due to competitive pressure. She advocates for transparency requirements -- quarterly benchmark reporting, tracking AI-written code metrics, and disclosing misalignment incidents -- to give the public early warning of an approaching intelligence explosion.

The interview also covers Cotra's personal career journey, including her eight-year tenure at Open Philanthropy, the burnout she experienced after taking over technical AI safety grantmaking following Holden Karnofsky's departure, and her four-month sabbatical in late 2024. She reflects candidly on discovering that her local working environment -- particularly her relationship with her manager -- matters far more to her well-being than she had previously appreciated. She is now exploring roles at Redwood Research and METR while continuing to advise Open Philanthropy under new GCR director Emily Olsen.

The final portion explores effective altruism's evolving role in the AI landscape. Cotra argues that EA's distinctive comparative advantage is its willingness to engage in rigorous but speculative reasoning about unprecedented situations -- a methodology that most mainstream institutions find uncomfortable. She suggests EA should serve as an "incubator" for cause areas like digital sentience, space governance, and value lock-in that are too unconventional for other communities to champion but may become critically important as AI transforms society.

## Connections
- [[Ajeya Cotra]]
- [[80,000 Hours]]
- [[Open Philanthropy]]
- [[Holden Karnofsky]]
- [[Tom Davidson]]
- [[Ryan Greenblatt]]
- [[Redwood Research]]
- [[METR]]
- [[Forethought]]
- [[Forecasting Research Institute]]
- [[Emily Olsen]]
- [[Joe Carlsmith]]
- [[Arvind Narayanan]]
