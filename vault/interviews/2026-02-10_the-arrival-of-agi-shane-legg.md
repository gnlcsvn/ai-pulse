---
date: 2026-02-10
source: YouTube
video_id: l3u_FAv33G0
url: https://www.youtube.com/watch?v=l3u_FAv33G0
channel: Google DeepMind
title: "The arrival of AGI | Shane Legg (co-founder of DeepMind)"
guests: [Shane Legg]
topics: [AGI, AGI definition, AGI timeline, superintelligence, AI safety, AI ethics, system two safety, AI consciousness, economic disruption, labour market, post-AGI society]
duration: 53m
processed_date: 2026-02-25
predictions_count: 5
---

# The Arrival of AGI | Shane Legg (co-founder of DeepMind)

## Guests
- **Shane Legg** - Co-founder & Chief AGI Scientist, Google DeepMind

Host: **Hannah Fry** - Professor & Podcast Host, Google DeepMind

## Key Takeaways
1. Current AI systems are "a lot more than sparks" of AGI -- already superhuman at languages and general knowledge, but still weak at continual learning and visual reasoning [▶ 1:52](https://www.youtube.com/watch?v=l3u_FAv33G0&t=112s)
2. Shane Legg defines "minimal AGI" as an AI that can do all cognitive things people can typically do, and estimates it is about two years away (i.e., ~2028) [▶ 7:04](https://www.youtube.com/watch?v=l3u_FAv33G0&t=424s)
3. He reaffirms his long-standing prediction of a 50-50 chance of (minimal) AGI by 2028, first published on his blog in 2009 [▶ 49:32](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2972s)
4. Full AGI (matching the entire spectrum of human cognition) could follow 3-6 years after minimal AGI, and he believes it will arrive within a decade [▶ 49:57](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2997s)
5. There are no fundamental blockers to AGI -- progress requires a combination of more data, algorithmic innovation, and architectural changes rather than just scaling [▶ 4:31](https://www.youtube.com/watch?v=l3u_FAv33G0&t=271s)
6. He advocates "system two safety" -- making AI reason explicitly about ethics using chain-of-thought monitoring, potentially becoming more ethical than humans [▶ 20:44](https://www.youtube.com/watch?v=l3u_FAv33G0&t=1244s)
7. In software engineering, the fraction of AI-written code will increase dramatically in the next few years, potentially reducing a 100-person team to 20 [▶ 42:39](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2559s)
8. The current economic system where people exchange mental and physical labour for resources "may not work the same anymore" in a post-AGI world [▶ 37:55](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2275s)
9. Data centres offer 6-8 orders of magnitude advantage over the human brain across energy, space, bandwidth, and signal speed -- superintelligence is computationally inevitable [▶ 34:58](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2098s)
10. Jobs that can be done entirely remotely via laptop are most vulnerable to AI disruption; physical/manual work like plumbing is relatively protected in the near term [▶ 45:09](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2709s)

## Notable Quotes
> "I think it's a lot more than sparks." - Shane Legg (on whether current AI shows sparks of AGI) [▶ 1:52](https://www.youtube.com/watch?v=l3u_FAv33G0&t=112s)

> "It could be one year, it could be five years, I'm guessing probably about two or so." - Shane Legg (on minimal AGI timeline) [▶ 7:04](https://www.youtube.com/watch?v=l3u_FAv33G0&t=424s)

> "I don't think there are fundamental blockers on any of these things and we have ideas on how to develop systems that can do these things." - Shane Legg [▶ 4:31](https://www.youtube.com/watch?v=l3u_FAv33G0&t=271s)

> "It means a massive transformation. The current system where people contribute their mental and physical labor in return to access to resources... that may not work the same anymore." - Shane Legg [▶ 37:41](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2261s)

> "This is actually something which is going to structurally change the economy and society and all kinds of things. And we need to think about how do we structure this new world?" - Shane Legg [▶ 43:58](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2638s)

> "If we can harness this capability, this could be a real golden age." - Shane Legg [▶ 44:01](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2641s)

> "So is human intelligence going to be the upper limit of what's possible? I think absolutely not." - Shane Legg [▶ 34:34](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2074s)

> "We need to think really hard about how do we make a superintelligence superethical." - Shane Legg [▶ 36:55](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2215s)

## Data Points & Numbers
| Data Point | Value | Source Timestamp |
|---|---|---|
| Languages current AI can speak | **~150** | [▶ 2:55](https://www.youtube.com/watch?v=l3u_FAv33G0&t=175s) |
| Human brain power consumption | **~20 watts** | [▶ 34:34](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2074s) |
| Data centre power consumption | **200 megawatts** | [▶ 34:58](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2098s) |
| Brain cortical signal frequency | **~100-200 Hz** | [▶ 34:34](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2074s) |
| Data centre channel frequency | **10 billion Hz** | [▶ 34:58](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2098s) |
| Brain signal propagation speed | **30 m/s** (electrochemical) | [▶ 34:52](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2092s) |
| Data centre signal speed | **300,000 km/s** (speed of light) | [▶ 35:12](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2112s) |
| Orders of magnitude advantage (data centre vs brain) | **6-8** across four dimensions | [▶ 34:58](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2098s) |
| Software engineer team reduction example | **100 down to 20** (in a few years) | [▶ 42:50](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2570s) |
| Shane Legg's AGI prediction (from blog) | **50% by 2028** (since 2009) | [▶ 49:32](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2972s) |

## Predictions
| Prediction | Timeframe | Confidence | Source |
|---|---|---|---|
| 50-50 chance of minimal AGI (AI that can do all typical human cognitive tasks) | By 2028 | 50% (stated explicitly) | [▶ 49:32](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2972s) |
| Full AGI (matching the full spectrum of human cognition) | 3-6 years after minimal AGI (~2031-2034), within a decade from now | Medium-High | [▶ 49:57](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2997s) |
| AI will progress toward superintelligence far beyond human intelligence | Within 1-2 decades | High | [▶ 34:01](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2041s) |
| Software engineering teams will shrink from 100 to 20 people using advanced AI tools | Next few years (~2026-2029) | High | [▶ 42:50](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2570s) |
| Physical/manual jobs (e.g., plumbing) will remain protected from AI disruption | Coming years (near-term) | High | [▶ 45:09](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2709s) |

## Topics Discussed
- **Definition of AGI**: Legg's three-tier framework: minimal AGI (typical human cognition), full AGI (full spectrum of human cognition), and ASI (artificial superintelligence beyond human capability) [▶ 1:55](https://www.youtube.com/watch?v=l3u_FAv33G0&t=115s)
- **Current AI capabilities and gaps**: Superhuman at languages and knowledge, but weak at continual learning, visual reasoning, and spatial graph analysis [▶ 2:55](https://www.youtube.com/watch?v=l3u_FAv33G0&t=175s)
- **Path to AGI**: Not just scaling -- requires targeted data, algorithmic innovation, architectural changes (e.g., episodic memory for continual learning) [▶ 4:31](https://www.youtube.com/watch?v=l3u_FAv33G0&t=271s)
- **Origin of the term AGI**: Legg coined it in conversation with Ben Goertzel; originally meant as a field of study, but became a category of artifact [▶ 17:27](https://www.youtube.com/watch?v=l3u_FAv33G0&t=1047s)
- **AI ethics and system two safety**: Using chain-of-thought reasoning for ethical decisions, analogous to Kahneman's System 2 thinking [▶ 20:44](https://www.youtube.com/watch?v=l3u_FAv33G0&t=1244s)
- **AI safety testing**: Testing for bioweapon creation, hacking assistance, and other dangerous capabilities before release [▶ 29:51](https://www.youtube.com/watch?v=l3u_FAv33G0&t=1791s)
- **AI consciousness**: Nobody really knows; experts are divided; some will believe future AGIs are conscious and some won't [▶ 31:06](https://www.youtube.com/watch?v=l3u_FAv33G0&t=1866s)
- **Brain vs data centre comparison**: 6-8 orders of magnitude advantage for computation across energy, space, bandwidth, and speed [▶ 34:34](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2074s)
- **Economic disruption and labour market impact**: Software engineering hit first; cognitive remote-work jobs most vulnerable; physical jobs more protected [▶ 42:39](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2559s)
- **Post-AGI society**: Need to rethink wealth distribution, education, law, medicine, and every field where human intelligence is central [▶ 38:35](https://www.youtube.com/watch?v=l3u_FAv33G0&t=2315s)

## Summary
In this wide-ranging conversation on the Google DeepMind podcast, Shane Legg -- co-founder of DeepMind and the person credited with popularising the term "AGI" -- provides his most detailed public assessment yet of where artificial general intelligence stands and where it is heading. He describes current AI systems as far beyond mere "sparks" of AGI, already superhuman at language and general knowledge, but still weak at continual learning and certain types of visual and spatial reasoning. Critically, he sees no fundamental blockers to closing these gaps, only a "long tail" of cognitive capabilities that need to be addressed through a combination of better data, new algorithms, and architectural innovations.

Legg reaffirms his long-standing prediction -- first published on his blog in 2009 -- that there is a 50-50 chance of "minimal AGI" (an AI that can do all the cognitive things people can typically do) by 2028. He introduces a three-tier framework: minimal AGI, full AGI (matching the entire spectrum of human cognition including extraordinary feats), and artificial superintelligence. He estimates full AGI could arrive 3-6 years after minimal AGI, placing it within a decade from now. On superintelligence, he makes a compelling computational argument: data centres offer 6-8 orders of magnitude advantages over the human brain in energy, space, bandwidth, and signal speed, making superintelligence not a question of "if" but "when."

On safety, Legg advocates for what he calls "system two safety" -- having AI systems explicitly reason about ethical decisions using chain-of-thought processes, analogous to Daniel Kahneman's System 2 thinking. He argues that if done well, AI could actually become more ethical than humans by applying ethical reasoning more consistently and rigorously. He also discusses DeepMind's testing for dangerous capabilities like bioweapon development and hacking before model release.

Perhaps most striking is his candour about societal impact. He warns that the current economic system -- where people exchange cognitive and physical labour for access to resources -- "may not work the same anymore" in a post-AGI world. He predicts that software engineering will be among the first fields dramatically affected, with 100-person teams potentially shrinking to 20 in the next few years. He offers a rule of thumb: if you can do your job entirely remotely on a laptop, it is "probably very much cognitive work" and therefore vulnerable. He urged the UK Russell Group universities' vice-chancellors to have every department seriously consider what AGI means for their field, comparing the current moment to standing on the bend of an exponential curve -- much like the early days of the COVID-19 pandemic when experts could see what was coming but the public had not yet grasped it.

## Connections
- [[Shane Legg]]
- [[Google DeepMind]]
- [[Hannah Fry]]
- [[Ben Goertzel]]
- [[Daniel Kahneman]]
- [[AGI]]
- [[Superintelligence]]
- [[AI Safety]]
- [[System Two Safety]]
- [[AI Ethics]]
- [[AI and Labour Market]]
