{
  "metadata": {
    "video_id": "mYDSSRS-B5U",
    "url": "https://www.youtube.com/watch?v=mYDSSRS-B5U",
    "title": "Anthropic CEO Dario Amodei AI Potential OpenAI Rivalry GenAI Business Doomerism",
    "channel": "Alex Kantrowitz",
    "upload_date": "2026-02-06",
    "duration": "68m",
    "guests": [
      "Dario Amodei"
    ],
    "transcribed_date": "2026-02-25",
    "whisper_model": "mlx-community/whisper-large-v3-turbo"
  },
  "language": "en",
  "text": "I get very angry when people call me a doomer. When someone's like, this guy's a doomer, he wants to slow things down. You heard what I just said. Like, my father died because of cures that could have happened a few years later. I understand the benefit of this technology. I'm sure you've heard the criticism from people like Jensen who say, well, Dario thinks he's the only one who can build this safely and therefore wants to control the entire industry. What do you think about that? I've never said anything like that. That's an outrageous lie. That's the most outrageous lie I've ever heard. Anthropic CEO Dario Amode joins us to talk about the path forward for artificial intelligence, whether generative AI is a good business, and to fire back at those who call him a doomer. And he's here with us in studio at Anthropic headquarters in San Francisco. Dario, it's great to see you again. Welcome to the show. Thank you for having me. So let's recap the past couple of months for you. You said AI could wipe out half of entry-level white collar jobs. You cut off Windsurf's access to Anthropic's top tier models when you learned that OpenAI was going to acquire them. You asked the government for export controls and annoyed NVIDIA CEO Jensen Wang. What's gotten into you? You know, I think myself and Anthropic are always focused on kind of trying to do and say the things that we believe. And I think as we've gotten more close to AI systems that are more powerful, you know, I think I've wanted to say those things, you know, more forcefully, more publicly to make the point clearer. You know, I've been saying for many years that, you know, we have these, we can talk in detail about them, but, you know, we have these scaling laws. AI systems are getting more powerful. They're going from the level of, you know, a few years ago, they were barely coherent. Now, you know, a couple of years ago, they were at the level of a smart high school student. Now we're getting to smart college student, PhD, and they're starting to, they're starting to apply across the economy. And so I think all the issues related to AI, ranging from kind of the national security issues to the economic issues, you know, are starting to become quite near to where, to where we, you know, to where we're actually going to face them. And so, and so I think as these problems have come closer, I've, you know, even though, you know, in some form, Anthropic has been saying these things for a while, I think the urgency of these things has gone up. And, you know, I want to make sure that we, you know, I want to make sure that we say what we believe and that we warn the world about the possible downsides, even though, you know, no one can say what's going to happen. We're, you know, we're saying what we, you know, what we think might happen, what we think is likely to happen. You know, we, we back it up as, as best we can, although it's often, you know, extrapolations about the future where, where no one, where no one can be sure. Um, but, you know, I think we, we see ourselves as kind of, as kind of having the duty to, you know, to kind of warn the world about what's going to happen. And that's not to say, you know, I think there's an incredible number of like positive applications of AI, right? I've, I've kind of continued to talk about that. I read this, I wrote this essay, Machines of Loving Grace. Um, I, I feel in fact that I and Anthropic have often been able to do a better job of articulating the benefits of AI than some of the people who call themselves optimists or accelerationists. Um, so I think we probably appreciate the benefits more than, more than anyone. Um, but for exactly the same reason, because we can have such a good world if we get everything right, I feel obligated to warn about the risks. So all of this is coming from your timeline. Basically you, it seems like you have a shorter timeline than most. And so you were feeling a sense of urgency to get out there because you think that this is imminent. Yes, I'm not sure. Um, you know, I think it's very hard to predict, particularly on the societal side. So if you say, you know, when are people going to deploy AI or when are companies going to use, you know, X, X dollars of spend of AI or, you know, when will, when will AI, um, you know, be, be, be, be used in these applications or when will it drive these medical cures? That's kind of harder, harder to say. I think the underlying technology is more predictable, but still uncertain, still no one knows. But I think on the underlying technology, I've started to become more confident. There isn't no uncertainty about it. You know, I think the, the exponential that we're on could, could kind of still, um, you know, could totally peter out. You know, I think there's maybe, uh, I don't know, 20 or 25% chance that sometime in the next two years, the models just start getting, stop getting better for reasons we don't understand, or maybe reasons we do understand, like, you know, data or compute availability. And then everything I'm saying just, just seems, seems totally silly. And everyone makes fun of me for all the warnings I've made. And, and, you know, I'm just, I'm just totally fine with that, given, given the distribution that, that, that, that, given distribution that I see. And so I should say that this is part, our conversation is part of a profile I'm writing about you. I've spoken with more than two dozen people who've worked with you, who know you, who've competed with you. And I'm going to link that in the show notes. If anybody wants to read it, it's free to read. Uh, but one of the themes that has, uh, come through across everybody I've spoken with is that you have about the shortest timeline of any of the major lab leaders. And you just referenced it, uh, just, just now. So why do you have such a short timeline and why should we believe in yours? Yeah, it, it really depends what you mean by timeline. Um, so one thing, and you know, I've, I've, I've been consistent on this over the years is, you know, there are these terms in the AI world, like AGI and super intelligence. Like you'll hear leaders of companies say, we've achieved AGI, we're moving on to super intelligence. Or like, it's really exciting that someone stopped working on AGI and started working on super intelligence. So I think these terms are totally meaningless. I don't know what AGI is. I don't know what super intelligence is. It, it sounds like a, it sounds like a marketing term. It is marketing. Yeah. It sounds like, you know, something, something designed to activate people's, people's dopamine. So you'll see in public, I never use those terms. And I, you know, I'm, I'm, I'm actually, you know, careful to criticize the use of those terms. Um, but I think, I think despite that, I am, I am indeed one of the most bullish about, about AI capabilities improving very fast. The thing I think is real that I've said over and over again is the exponential. The idea that every few months we get an AI model that is better than the AI model we got before. And that we, we get that by investing more compute in AI models, more data, more new types of training models. Initially, this was done by what's called pre-training, which is when you just feed a bunch of data from the internet into the model. Now we have a second stage that's reinforcement learning or test time compute or reasoning or whatever you want to call it. I think of it as a second stage that involves reinforcement learning. Now, both of those things are scaling up together as we've seen with our models. And as we've seen with models from other, from other companies, and I don't see anything blocking that the further scaling of that. There's some stuff about, you know, how do we broaden the tasks. On the RL side of it, we've seen more progress on, say, math and code, where the models are, you know, getting pretty close to like a high professional level and less on more subjective tasks. But I think that is very much a temporary obstacle. So when I look at it, I see this exponential and I say, look, people aren't very good at making sense of exponentials, right? Like, you know, if something is doubling every six months, then, you know, two years before it happens, it looks like it's only 1 16th of the way there. And so we are sitting here in the middle of 2025. And the models are really starting to explode in terms of the economy, right? If you look at the capabilities of the model, they're starting to saturate all the benchmarks. If you look at revenue, you know, Anthropics revenue every year has grown 10x. Every year, we're kind of conservative and we say, you know, it can't grow 10x. This time, you know, I never assume anything and actually always am very conservative in saying, I think it's going to slow down on the business side. But we went from zero to 100 million in 2023. We went from 100 million to a billion in 2024. And, you know, this year, in this first half of the year, we've gone from 1 billion to, you know, I think as of speaking today, it's well above 4. It might be 4.5. And so if you think about it, you know, suppose that exponential continued for two years. I'm not saying it will. But suppose it continued for two years. You know, you're like well into the 100 billions. I'm not saying that'll happen. I'm saying the situation is that when you're on an exponential, you can really get fooled by it. Two years away from when the exponential goes totally crazy, you know, it looks like it's just starting to be a thing. And so that's the fundamental dynamic. You know, we saw that with the Internet in the 90s, right, where it was like, you know, networking speeds and the underlying speed of the computers were getting fast. And over a few years, it became possible to have to basically build a digital global communications network on top of all this when it wasn't possible just a few years ago. And almost no one except for a few people really saw the implications of that and how fast it would happen. And so that's that's where I'm coming from. That's what I think. Now, I don't know, like if a bunch of satellites crashed, maybe the Internet would have taken longer. If there was an economic crash, maybe it would have taken a little longer. So we can't be sure of the exact timelines. But I think people are getting fooled by the exponential and not realizing how fast it might be, how fast I think it probably will, although I'm not sure. But so many folks in the AI industry are talking about diminishing returns from scaling now. That really doesn't fit with the vision you just laid out. Are they wrong? Yeah, I from what we've seen, I can only speak in terms of the models at Anthropic. But what I've seen in terms of the models at Anthropic, if we look at, you know, let's take coding. Coding is one area where, you know, I think Anthropic models have advanced very quickly. Adoption has been very quick. We're not just a coding company. We're planning to expand to many areas. But if you look at coding, you know, we released 3.5 Sonnet, a model we call 3.5 Sonnet V2, which, you know, let's call it 3.6 Sonnet now. 3.7 Sonnet, and then 4.0 Sonnet and 4.0 Opus. And, you know, that series of four or five models, each one got substantially better at coding than the last. If you want to look at benchmarks, you can look at, you know, SWE Bench growing from, you know, I think 18 months ago, it was at like 3% or something. Growing all the way to, you know, 72% to 80%, depending on how you measure it. And the real usage has grown exponentially as well. We're heading more and more towards autonomously you can just use these models. I think the actual majority of code that's written at Anthropic is, you know, at this point written by or at least with the involvement of one, you know, one of the Claude models. And various other companies have said, you know, have said, you know, similar statements to that. So we see the progress as being very fast and the exponential is continuing, and we don't see any diminishing returns. But there are some liabilities, it seems like, with large language models. For instance, continual learning. We had Dwarke on a couple weeks ago. Here's how he put it, and he wrote about it in his sub stack. The lack of continual learning is a huge, huge problem. The LLM baseline at many tasks might be higher than an average human, but you're stuck with the abilities you get out of the box. So you just make the model, and that's it. It doesn't learn. That seems like a glaring liability. What do you think about that? So first of all, I would say, even if we never solved continual learning, even if we never solved continual learning and memory, I think that the potential for the LLMs to do, you know, incredibly well to, you know, affect things at the scale of the economy will be very high. If I think of the field I used to be in, biology and medicine, like, you know, let's say I had a very smart Nobel Prize winner. And, you know, I said, okay, you know, you've discovered all these things. You have this incredibly smart mind. But, you know, you can't, you know, you can't, like, read new textbooks or absorb any new information. I mean, that would be difficult. But, like, still, if you had, like, 10 million of those, like, they're still going to make a lot of biology breakthroughs. Like, they're going to be limited. They're going to be able to do some things humans can't. And there are some things humans can do that they can't. But even that, even if we impose that as a ceiling, like, man, that's pretty damned impressive and transformative. And even if I said you never solved that, like, I think, you know, I think people are underestimating the impact. But, look, context windows are getting longer. And models actually do learn during the context window, right? So as I, you know, talk to the model during the context window, I have a conversation. It absorbs information. The underlying weights of the model may not change. But, you know, just like I'm talking to you here and we're having a conversation and I listen to the things you say and I, you know, I think and I, like, respond to them, the models are able to do that. And from a machine learning perspective, from an AI perspective, there's no reason we can't make the context length 100 million words today, right? Which is roughly what a human hears in their lifetime. There's no reason that we can't do that. It's really inference support. And so, again, even that fills in many of the gaps. Not all the gaps, but it fills in many of the gaps. And then there are a number of things like learning and memory that do allow us to update the weights. So, you know, there are a number of things around, you know, types of reinforcement learning, learning training. You know, we used to many years ago talk about inner loops and outer loops, right? The inner loop is like I have some episode and I learned some things in that episode and I'm trying to optimize for the lifetime of that episode. And kind of the outer loop is the agents learning over episodes. And so I think maybe that inner loop, outer loop structure is a way to learn the continual learning. One thing we've learned in AI is whenever it feels like there's some fundamental obstacle, like two years ago we thought there was this fundamental obstacle around reasoning. It turned out just to be RL. You just train with RL and you let the model write some stuff down. You know, you let the model write things down to try and figure out objective math problems. Without being too specific, you know, I think, and we already have maybe some, you know, some evidence to suggest that this is another of those problems that is not as difficult as it seems. That will fall to scale plus a slightly different way of thinking about things. Do you think your obsession with scale might blind you to some of the new techniques like Demis Asaba says, you know, to get to AGI or you might call it super powerful AGI? Whatever. Human level intelligence is what we're all talking about. We might need a couple of new techniques for that to happen. If you're so focused on scale, can you get there? We're developing new techniques every day. Okay. You know, Claude is very good at code and we, you know, we don't really talk externally that much about why Claude is so good at code. Why is it so good at code? Like I said, we don't talk externally about it. I have to ask. So, you know, every new version of Claude that we make has, you know, improvements to the architecture, improvements to the data that we put into it, improvements to the methods that we use to train it. So we're developing new techniques all the time. New techniques are a part of every, you know, model that we build. And, you know, that's why we, you know, I've said these things about like, you know, we're trying to optimize for like talent density as much as possible. Like you need that talent density in order to invent the new techniques. You know, there's one thing that's been hanging over this conversation, which is that maybe Anthropic is the company with the right idea, but the wrong resources. Because you look at what's happening with XAI and inside Meta, where Elon's built his massive cluster. Mark Zuckerberg is building this five gigawatt data center. And they are putting so much resources towards scaling up. Is it possible? Well, I mean, Anthropic, obviously, you have raised billions of dollars, but these are trillion dollar companies. Yeah. So we've raised, I think at this point, a little short of $20 billion. It's not bad. So that's not nothing. And I would also say if you look at the size of the data centers that we're building with, for example, Amazon, I don't think our data center scaling is substantially smaller than that of any of the other companies in the space. You know, in many cases, these things are limited by energy. They're limited by capitalization. And, you know, when people talk about, you know, these large amounts of money, they're talking about it over several years. Right. And when you hear some of these announcements, sometimes they're not funded yet. They're, you know, we've seen the size of the data centers that folks are building. And we're actually pretty confident that, you know, we will be within a rough range of the size of the data centers that they built. You talked about talent density. What do you think about what Mark Zuckerberg is doing on the talent density front? I mean, combining that with these massive data centers, it seems like he's going to be able to compete. Yeah. So this is actually very interesting because, you know, one thing we noticed is that relative to other companies, you know, I think a lot fewer people from Anthropic have been caught by these. And it's not for lack of trying. I've talked to plenty of people, you know, who got these offers at Anthropic and who just turned them down, who wouldn't even talk to Mark Zuckerberg, who said, you know, no, I'm staying at Anthropic. And our general response to this was, you know, I posted something to the whole company Slack where I said, look, you know, we are not willing to compromise our, you know, our compensation principles, our principles of fairness to respond individually to these offers. The way things work at Anthropic is there's a series of levels. When candidate comes in, they get assigned a level and we don't negotiate that level because we think it's unfair. We want to have a systematic way. If, you know, if Mark Zuckerberg, you know, throws a dart at a dartboard and hits your name, that doesn't mean that you should be paid 10 times more than the guy next to you who's, you know, who's just as skilled, who's just as talented. And my view of the situation is that, you know, the only way you can really be hurt by this is if you allow it to destroy the culture of your company by panicking, by treating people unfairly in an attempt to defend the company. And I think actually this was a unifying moment for the company where, you know, we didn't give in. We refused to compromise our principles because we had the confidence that people are Anthropic because they truly believe in the mission. And, you know, I think that gets to kind of how I see this. I think that what they are doing is trying to buy something that cannot be bought. And that is alignment with the mission. And, you know, I, you know, I, you know, I think there's selection effects here. Like, you know, I, you know, are they getting the people who are most enthusiastic or most mission aligned, who are most excited to... But they have talent and GPUs. You're not underestimating them? I will see how it will see how it plays out. I am pretty bearish on what they're trying to do. So let's talk a little bit about your business, because a lot of people have been wondering, is the business of Generative AI a real thing? And I'm also curious. I have questions all the time. You talked about how much money you've raised, close to $20 billion. You've raised $3 billion from Google, $8 billion from Amazon, $3.5 billion from a new round led by Lightspeed, who I've spoken with. What is your pitch? Because you are not part of like a big tech company. You're out there on your own. And do you just bring the scaling laws and say, can I have some money? So my view of this has always been that talent is the most important thing. So if you go back three years ago, we were in a position where we had raised mere hundreds of millions. Open AI had already raised $13 billion from Microsoft. And of course, the large hyper-cap tech companies were sitting on $100 billion, $200 billion. And basically, the pitch we made then is we know how to make these models better than others do, right? There may be a curve. There may be a curve of scaling laws. But look, if we are in a position where we can do for $100 million what others can do for $1 billion, and we could do for $10 billion what they can do for $100 billion, then it's 10 times more capital efficient to invest in Anthropic than it is to invest in these other companies. Would you rather be in a position where you can do anything for 10 times cheaper or where you start with a large pile of money? If you can do things 10 times cheaper, the money is a temporary defect that you can remedy. If you have this intrinsic ability to build things for the same price much better than anyone else or as good as anyone else for much lower price, investors aren't idiots, or at least they aren't always idiots. It depends which one you go to. I'm not going to name any names. But they basically understand the concept of capital efficiency. And so we've been in a position three years ago where these differences were like 1,000x. And now you're saying with $20 billion, can you compete with $100 billion? And my answer is basically yes, because of the talent density. You know, I've said this before, but, you know, the Anthropic is actually the fastest growing software company in history at the scale that it's at. So we grew from zero to 100 million in 2023, 100 million to billion in 2024. And this year we've grown from 1 billion to, I think I said this before, 4.5. So that 10x a year, I mean, you know, every year I, like, suspect that we'll grow at that scale. And every year I'm almost afraid to say it publicly because I'm like, no, it couldn't possibly happen again. So, you know, I think the growth at that scale, like, kind of speaks for itself in terms of our ability to compete with the big players. Okay, so CNBC says 60 to 75% of Anthropic sales are through the API. That was according to internal documents. Is that still accurate? I won't give exact numbers, but the majority does come through the API, although we also do have a flourishing apps business. And, you know, I think more recently the, you know, Max Tier, which Power users use, as well as Cloud Code, which coders use. So, you know, I think we have a thriving and fast-growing apps business, but, yes, the majority comes through the API. So you're making the most pure bet on this technology. Like, you know, OpenAI might be betting on ChatGPT and Google might be betting on the fact that no matter where the technology goes, it can, you know, integrate into Gmail and Calendar. So why have you made this bet on this, the pure bet on the tech itself? Yeah, I mean, I would say, I wouldn't quite put it that way. I think we've, I would describe it more as we've bet on business use cases of the model, more so than we've bet on the API per se. And it's just that the first business use cases of the model come through the API. So, you know, as you mentioned, OpenAI is very focused on the consumer side. Google is very focused on kind of the existing products that Google has. Our view is that, if anything, the enterprise use of AI is going to be greater even than the consumer use of AI. I should say the business use because it's enterprise, it's startups, it's developers, and it's kind of, you know, power users using the model for productivity. I also think that being a company that's focused on the business use cases actually gives us better incentives to make the models better. A thought experiment that I think is worth running is, you know, suppose I have this model and it's, you know, it's as good as an undergrad at biochemistry. The biggest deal in the world, right? They might pay 10 times more for something like that. It might have 10 times more value to them. And so the general aim of making the models solve the problems of the world to make them smarter and smarter, but also able to bring many of the positive applications, right? The things I wrote about in, like, machines of loving grace of, like, solving the problems of biomedicine, solving the problems of geopolitics, solving the problems of, you know, of economic development, you know, as well as more prosaic things like finance or legal or productivity or insurance. I think it gives a better incentive to kind of develop the models as far as possible. And I think in many ways it's like it may even be a more positive business. So I would say we're making a bet on the business use of AI because it's most aligned with kind of the exponential. Okay. Then briefly, how did you decide to go with the coding use case? Yes. So, you know, originally, as happens with most things, we're trying to optimize for making the model better at a bunch of stuff. And, you know, coding particularly stood out in terms of how valuable it was. You know, I've worked with thousands of engineers, and there was a point about a year, a year and a half ago where one of the best I'd ever worked with said, you know, every previous coding model has been useless to me. And this one finally was able to do something I wasn't able to do. And then after we released it, it started getting quick adoption. This was around the time that, you know, a lot of the coding companies like Cursor, Windsurf, GitHub, Augment Code started exploding in popularity. And then when we saw how popular it was, we kind of doubled down on it. My view is that coding is particularly interesting because, A, the adoption is fast. And, B, getting better at coding with the models actually helps you to develop the next model. So it has a number of, you know, I would say advantages. And now you're selling your AI coding through Claude Code. But it's very interesting. The pricing model has been confounding to some. You can spend $200 a month and get the equivalent. I spoke to one developer. They got the equivalent of $6,000 a month from your API. Ed Zitron has pointed out the more popular that your models get, the more money you're going to lose if people are super users of this technology. So how does that make sense? So actually, pricing schemes and rate limits are surprisingly complicated. So some of this is basically the result of when we released our â€“ when we released Claude Code in the max tier, which we eventually tied together, actually not fully understanding the implications of, you know, the ways in which people could use the models and how much they were actually able to get. So over the last few days, as of the time of this interview, we've adjusted that, particularly on the larger models like Opus. I think it's no longer possible to spend that much with a $200 subscription. And, you know, it's possible more changes will come in the future. But we're always going to have a distribution of users who use a lot and users who use some amount. And it doesn't necessarily mean we're losing money, that there are some users who get more, you know, who, if you were to measure via API credits, spend, you know, get a better deal on the consumer subscription than they would on the API products. Right? There's a lot of assumptions there. And I can tell you that some of them are wrong. We are not, in fact, losing money. But I guess there's another question about whether you can continue to serve these use cases and not raise prices. So just to give you a couple stats, there are some developers that are upset because using Anthropics' newer models and cursors is costing them more than it ever has. Startups that I've spoken with say Anthropic is down a bunch because they can't get access to the GPUs. At least that's what they imagine is happening. And I was just with Amjad Massad at Replit in an interview that we're going to air next week who said there was a period of time where the price per token, price to use these models was coming down. And it stopped coming down. So is it, is what's happening that these models are just so expensive for Anthropic to run that it's hitting a wall of its own? Again, I think you're making assumptions here. That's why I'm asking the CEO. Yeah, you know, you know, the way I think about it is we think about the models in terms of how much value are they creating, right? So as the models get better and better, I think about how much value they create. And there's a separate question about how the value is distributed between those who make the model, those who make the chips, and, you know, those who make the underlying applications. So again, without being too specific, like, I think there are some assumptions in your question that are not necessarily correct. You know, I... Can you tell me which ones? I, so I'll say this. I do expect the, I expect the price of providing a given level of intelligence to go down. I expect the price of providing the frontier of intelligence, which will, which will provide kind of increasing economic value, that might go up or it might go down. My guess is it probably stays about where it is. But again, the value that's created, you know, goes way up. So two years from now, my guess is that we'll have models that cost of the same order of magnitude that they cost today, except they'll be much more capable of doing work much more autonomously, much more broadly than they are capable of today. One of the things that Amjad mentioned was he thinks that the bigger models are not as intensive to run or more intensive to run, given their size, because of the architecture and some of these techniques that we talked about, that they're lighting up only certain sections of the model. So his idea, I'm hopefully conveying this truthfully, is that Anthropic can run these models without too much bulk on the back end, but is still keeping those prices where they are. And I think the line that I'm going to draw there is maybe that to get to software margins. There were some reports that Anthropic is slightly below software gross margins. You're going to have to charge a little bit more for these models. So, yeah, again, I think larger models cost more to run than smaller models. Okay. You know, I think the technique you're referring to is maybe mixture of experts or something like that. So whether your models are mixture of experts or not, like mixture of experts is like a way to run models more cheaply. They have a given number of parameters. It's a way to train models. But if you're not using that technique, then larger models that don't use that technique cost more to run than smaller models that don't use that technique. And if you're using that technique, larger models that use that technique cost more to run than smaller models that are using that technique. So I think that's sort of a distortion of the situation. Basically, I'm just guessing and I'm trying to find out what the truth is from you. Yeah, look, so I, you know, in terms of like the cost of the models, like one thing you'd be surprised by, people, you know, people kind of impute this thing to like, oh, man, it's going to be really hard to get the margins from like X percent to Y percent. We make improvements all the time that make the models like 50 percent more efficient than they are before. We are just at the beginning of optimizing inference. Inference has improved a huge amount where from from where it was a couple a couple years ago to where it is now. That's why the prices are coming down. And then how long is it going to take to be profitable? Because I think the loss is going to be like three billion this year. That's what I would distinguish different things. There's the cost of running. There's the cost of running the model. Right. So so for every dollar the model makes, it costs a certain amount. That is actually already fairly profitable. There are separate things. There's, you know, the cost of paying people and like buildings. That is actually not that that large in the scheme of things. The big cost is the cost of training the next model. And I think this idea of like the companies losing money and not being profitable is it's a little bit misleading. And you start to understand it better when you when you look at the scaling laws. So as a thought exercise, these numbers are not exact or even close for Anthropic. Let's imagine that in 2023, you train a model that costs 100 million dollars. And then in 2024, you deploy the 2023 model and it makes 200 million dollars in revenue. But you spend a billion dollars to train, you know, to train a new model in 2024. So and then, you know, and then in 2025, the billion dollar model makes two billion in revenue and you spend 10 billion to train the next model. So the company every year is unprofitable. It lost 800 million in 2024 and then 2025, it lost eight billion dollars. So, you know, this looks like a hugely unprofitable enterprise. But if instead I think in terms of is each model profitable, right? Think of each model as a venture. I invested 100 million in the model and then I got that I got that I got 200 million out of the model in the next year. So that model had 50 percent margins and, you know, and like and like made me 100 100 million dollars the next year. You know, the the the company invested a billion dollars and made and made two billion dollars in or sorry, the next model. The company invested a billion dollars and made. So every model is profitable. They're all profitable. But the company is unprofitable every year. I'm not I'm not. Right. This is this is this is a style. I'm not like claiming these numbers for Anthropic or claiming these facts for Anthropic. But this is a general for Anthropic. This general dynamic is is this this general dynamic is in general terms the explanation for what is going on. And so, you know, you know, at any time, if the models stopped getting better or if a company, stopped investing in the next model, you know, you would you know, you would have probably a viable business with the existing models. But everyone is investing in the next model. Yeah. And so eventually it'll get it'll get to some scale. But the fact that we're spending more to this fact that we're spending more to invest in the next model suggests that the scale of the business is going to be larger the next year than it was than it was the year before. Now, of course, what could happen is like the model stopped getting better. And there's this kind of one time cost that's like a boondoggle. And we spend we spend a bunch of money. But then the the you know, the the companies, the industry will kind of return to this, you know, to this plateau, to this level of profitability or the exponential can keep going. So I think I think that's a long winded way to say I don't think it's really the right way to think about things. Right. But what about open source? Because if you stopped, let's say you stopped investing in the models and open source caught up, then people could swap in open source. Now, I'd love to hear your perspective on this, because one of the things people have talked to me about when it comes to the anthropic business is there is that risk eventually that open source gets good enough that you can take anthropic out and put open source in. Yeah. So, you know, people have you know, I think one of the things that's been true of this industry is that and, you know, I saw it early in I saw it early in the history of A.I. Every community that that A.I. has gone through, it has this set of heuristics about how things work. Like back when I was in, you know, A.I. back in 2014, there was an existing kind of A.I. and machine learning research community that like thought about things in a certain way. And we're like, this is just a fad. This is a new thing. This can't work. This can't scale. And then because of the exponential, all those things turned out to be false. Then a similar thing happened with kind of like people deploying A.I. within companies to various applications. Then there was the same thought in the startup ecosystem. And I think now we're at the phase where kind of the world's business leaders, like the investors and the business, they have this whole lexicon of commoditization, you know, modes, which layer is the value going to, you know, which layer is the value going to accrue to. And open source is this idea that you can kind of see everything that's going on, you know, that it has a significance, that it kind of undermines the fact that, you know, the idea that it undermines business. And I actually find as someone who didn't come from that world at all, who never thought in terms of that lexicon, this is one of these situations where not knowing anything often leads you to make better predictions than kind of the people who have their way of thinking about things from the last generation of tech. And, you know, this is all, I think, a long-winded way of saying I don't think open source works the same way in A.I. that it has worked in other areas. Primarily because with open source, you can see the, you know, you can see the source code of the model. Here, we can't see inside the model. You know, it's often called open weights instead of open source to kind of distinguish that. But a lot of the benefits, which is that many people can work on it, that it's kind of additive, it doesn't quite work in the same way. So, you know, I've actually always seen it as a red herring. When I see it, when I see a new model come out, I don't care whether it's open source or not. Like if we talk about DeepSeek, I don't think it mattered that DeepSeek is open source. I think I ask, is it a good model? Is it better than us at, you know, the things that we, that's the only thing that I care about. It actually doesn't matter either way. Because ultimately, you have to host it on the cloud. The people who host it on the cloud do inference. These are big models. They're hard to do inference on. And conversely, many of the things that you can do when you see the weights, you know, we're increasingly offering on clouds where you can fine tune the model. You can, you know, you know, we're even looking at ways to, you know, to kind of, you know, investigate the activations of the model as part of like an interpretability interface. We did some little things around steering last time. So I think it's the wrong axis to think in terms of. When I think about competition, I think about like which models are good at the tasks that we do. I think open source is actually a red herring. But if it's free and cheap to run. It's not free. You have to, you have to, you have to, you have to run it on inference. And someone, someone has to make it fast on inference. All right. So I want to learn a little bit more about Dario the person. Yes. So we have a little bit of time left. So I have some questions for you about early life and then how you became who you are. Yes. So what was it like growing up in San Francisco? Yeah. I, you know, the city, when I first grew up here, had not really, had not really gentrified that much. You know, when I grew up, the tech boom hadn't, hadn't happened. It hadn't happened yet. You know, it happened as, as I was going through high school. And actually I had no interest in it. It was totally, it was totally boring to me. You know, I was interested in being like a scientist. I was interested in physics and math. And, you know, the idea of like, you know, you know, like writing some website actually had no interest to me, to me whatsoever. Like founding a company. Like those weren't things that I was, that I was interested in at all. You know, I was interested in discovering fundamental scientific truth. And I was interested in like, you know, how can I, how can I do something that like makes the world better? So, so, you know, that was, that was kind of more. And, you know, I watched the tech boom happen around me, but I, I feel like, you know, there was all kinds of things I probably could have learned from it that would have been helpful now. But I just actually wasn't paying attention and had no interest in it, even though I was like right at the center of it. So you're the son of a Jewish mother and an Italian father. That is true. From where I'm from in Long Island, we call that a pizza bagel. A pizza bagel. I've never, I've never heard that term before. So what was your relationship with your parents like? Yeah. I mean, you know, I was, I was always, I was always, I was always pretty close with them. You know, I feel like they gave me a sense of, you know, of, of kind of right and wrong and what was important in the world. I feel like, you know, kind of imbuing a strong sense of responsibility is, is maybe the thing that I remember most. You know, they were always people who felt that sense of responsibility and, you know, wanted to, wanted to make the world, wanted to make the world better. And I feel like, you know, that's one of the, one of the main things that I, that I learned from them. You know, it was always a very, a very, a very loving family, a very caring family. I was very close with my sister, Daniela, who of course became my, became my, became my co-founder. And, you know, I think we decided very early that we wanted to work together in some, in some capacity. I don't know if we imagined that it would happen, you know, at quite the scale that, that, that, that it has happened. But it, you know, I think, I think it really, you know, that was, that was something we kind of decided early that we wanted to do. The people that I've spoken with that have known you through the years have told me that your father's illness had a big impact on you. Can you share a little bit about that? Yes. Yes. He was, yeah, you know, he was ill for a long time and eventually died in, eventually died in, in, in 2006. Um, uh, so that, you know, that was actually one of the things that drove me to, you know, I, I don't think we mentioned it yet in this interview, but before, um, you know, before I went into AI, um, you know, I went into biology. So, you know, I'd gone to, uh, I, you know, I'd shown up at, I'd shown up at Princeton, um, uh, wanting to be a theoretical physicist. And, you know, I did some, did some work in, in cosmology for the first few, few months of my time there. Um, you know, and, and, and, you know, that was, that was around the time that, that my father died. And, you know, that did have an influence on me and kind of was one of the things that convinced me, um, you know, to, to go into biology, you know, to try and address, um, uh, uh, you know, human illnesses and biological problems. And so I started talking to some of the folks who worked on biophysics and computational neuroscience in the department that I was at at Princeton. And that was what led to the switch to biology and computational neuroscience. And then, you know, of course, after that, I eventually, I eventually went into AI. And the reason I went into AI was actually a continuation of that motivation, which is that, um, you know, as I spent many years in biology, I realized that the complexity of the underlying problems in biology felt like it was beyond human scale. You know, in order to understand it all, you needed hundreds, thousands of, you know, human researchers. And, you know, they often had a hard time collaborating or sharing their, you know, combining their, their knowledge and AI, which was, I was just starting to see the discoveries in it felt to me like the only technology that could kind of bridge that gap could bring us beyond human scale to, you know, to, to fully understand and solve the problems of biology. So, yeah, there is a through line there. Right. And I could have this wrong. Uh, but one thing I heard was that his illness was, um, largely uncurable when he had it. Yes. There have been advances that have been, can you share a little bit more? Yes. There are advances that have made it much more manageable today. Yes. Yes. That is, uh, that is, uh, that is, that, that, that, that is true. Actually, actually only, um, uh, uh, only in the, um, uh, maybe three or four years after he died, the, the cure rate for the disease that he had went from, uh, went from 50% to, uh, uh, to roughly 95%. Yeah. I mean, it has to have felt so unjust to have your father taken away by something that could have been cured. It, it, of course, of course. Um, but it also tells you of the, the urgency of solving the relevant problems, right? That, um, you know, that, that, that, you know, there, there was someone who worked on the cure to this disease that, you know, managed to cure it and save a bunch of people's lives. But, you know, could have, could have, um, saved even more people's lives if, if, you know, they, they had managed to, to find that, that, to find that cure, you know, a few years earlier than they did. Um, and I think that's, that's one of the tensions here, right? That, you know, um, I think AI has all of these benefits. Um, and, you know, I want everyone to get those benefits as soon as possible. You know, I probably understand, you know, better than almost anyone how urgent those benefits are. Um, and so I really understand the stakes when, you know, when I speak out about AI has these risks and I'm worried about these risks, I get very angry when people call me a doomer. I got really angry when, you know, when, when someone's like, this guy's a doomer, he wants to slow things down. You, you heard what I just said. Like, you know, my, my father died because of, you know, cures that, you know, could have, could have happened a few years later. I understand the benefit of this technology. When I sat down to, to, to write Machines of Loving Grace, you know, I wrote out all the ways that billions of people's lives could be better with this technology. Some of these people, some of these people who on Twitter, you know, cheer for acceleration, I don't think they have a humanistic sense of the benefit of the technology. Their, their brain's just full of adrenaline and, and they're like, they want to cheer for something. They want to accelerate. I don't get the sense they care. And so when these people call me a doomer, I think, I think they just completely, completely lack any moral credibility in doing that. You know, it really makes me lose respect for them. And I've been wondering what this, this word impact has been because it's come up so often that those who have been around you have said you've been singularly obsessed with having impact. In fact, I spoke with someone who knew you well, who said you wouldn't watch Game of Thrones because it wasn't tied to impact, that it was a waste of time. And you wanted to be focused on impact. Actually, that's not quite right. I wouldn't watch it because it was so negative sum. People were playing, were playing such negative, it was like these people start off and they're partly the situation and partly because they're, they're just horrible people. They like create the situation where at the end of it, everyone is like worse off than everyone was before. I'm really, I'm really excited about like creating positive sum situations. Okay. I recommend you watch it. It's a great, great show. But I hear, I hear your complaints. I have watched some parts of it. I was just very reluctant and didn't watch it for a long time. Let's get back to the impact. Okay. Let's get back to the impact. So that's what impact is, is effectively your career has been this, I, this quest to have that impact to be able, tell me if I'm going too far. To prevent other people from being in similar situations. You know, I, I think, you know, I think, I think that's a piece of it. I mean, you know, I, I have looked at, you know, many, you know, many attempts to help people. And, you know, some of them are more effective than others. And, you know, I think, I think I've always tried to, you know, there should be strategy behind it. There should be brains behind, you know, trying to, trying to help people. You know, which often means that there's a long path to it, right? It can run through a company and, you know, many activities that are technical and not immediately tied to the, the kind of impact, impact that you're trying to have. But, you know, the, the, the, the arc is, I'm always trying to bend the arc towards that. I think, I think that's my, that's my picture of it. That's, that's really why I, that's really why I got into this, right? You know, I think, you know, similar to the reason to get into AI was that, you know, I, I saw the problems of biology as, as almost intractable without it, or at least too slow moving. You know, I think my reason to start a company was that I had worked at other companies and I, I, I just didn't feel like the way those companies were, were run was, was really oriented towards, you know, trying, trying, trying to have that impact. There was a story around it that was often used for recruiting, but it became clear to me over the years that story was not sincere. I'm going to circle around a little bit because it's clear that you're referring to open AI here. From what I understand, you had 50% of open AI's compute. I mean, you ran the GPT-3 project. So if anyone was going to be focused on impact and safety, it wouldn't have been you. Yes, I, I was, you know, there was a period during which, during which that was, that was true. That wasn't true the entire time. That was, for example, when we were scaling up GPT-3. Yeah. So, you know, I, I, when I was at open AI, I, and a lot of my colleagues, including the people who, you know, eventually, eventually founded Anthropic. The pandas. Pandas. That's the name you gave them. That, that, that isn't a name I gave them. The name they took. That isn't a name they took. That's the name other people called them. I, I, maybe it's a name other people called them. That's not a name I ever used for my team. Okay. Sorry. Go ahead. That's a good clarification. Thank you. So, yeah, you know, we were involved in scaling up these models. Actually, the original reason for building GPT-2 and GPT-3, it was an outgrowth of the kind of AI alignment work that we were doing, right? Where myself and Paul Cristiano and some of the Anthropic co-founders had invented this technique called RL from human feedback. And that was designed to help steer models in, you know, in a direction to follow human intent. It was actually a precursor to, you know, we were trying to scale up another method called scalable supervision, which I think is just starting to, to, to work many years later to help models follow more kind of scalable human intent. But what we found is even with the more primitive technique, RL from human feedback, it wasn't working with the small language models with, you know, GPT-1 that we applied it to and that had, had been built by other people at OpenAI. And so the scaling up of GPT-2 and GPT-3 was done in order to kind of study these techniques in order to apply RL from human feedback at scale. You know, this goes to one thing, which is that I think in this field, the alignment of AI systems and the capability of AI systems is intertwined in this way that always ends up being kind of more tied and more intertwined than we think. Actually, what this made me realize is that it's very hard to work on the safety of AI systems and the capability of AI systems separately. It's very hard to work on one and not the other. I actually think the value and the way to inflect the field in a more positive way comes from organizational level decisions. When to release things, when to study things internally, what kind of work to do on systems. And that was one of the things that kind of motivated, you know, me and some of the other, you know, to be anthropic founders to kind of go off and do it our own way. But again, like if you were driving, if you think language, if you think capabilities and safety are interlinked and you were the guy driving the cutting edge models within OpenAI, you know, if you left, you knew they were going to be a company that was still doing this stuff. That's right. It seems like if you're driving the capabilities, you'd be the one in the driver's seat to help it be safe the way that you want it to. Again, I will say, you know, if there's a decision on releasing a model, if there's a decision on the governance of the company, if there's a decision on, you know, how the personnel of the company works, you know, how the company represents itself externally, the decisions that the company makes with respect to deployment, the claims it makes about how it operates with respect to society. You know, many of those things are not things that you control just by training the model. And, you know, I think trust is really important. I think the leaders of a company, they have to be trustworthy people. They have to be people whose motivations are sincere. No matter how much you're driving forward the company technically, if you're working for someone whose motivations are not sincere, who's not an honest person, who does not truly want to make the world better, it's not going to work. You're just contributing to something bad. So, and I'm sure you've heard the criticism from people like Jensen who say, well, Dario thinks he's the only one who can build this safely. And therefore, speaking of that word control, wants to control the entire industry. I've never said anything like that. That's an outrageous lie. That's the most outrageous lie I've ever heard. By the way, I'm sorry if I got Jensen's words wrong. No, no, no. The words were correct. Okay. But the words are outrageous. In fact, I've said multiple times, and I think Anthropics' actions have shown it, that we're aiming for something we call a race to the top. I've said this on podcasts over the years, and I think Anthropics' actions have shown it, where with a race to the bottom, everyone is competing to get things out as fast as possible. And so I say when you have a race to the bottom, it doesn't matter who wins, everyone loses, right? Because you make the unsafe system that, you know, helps your adversary or causes economic problems or, you know, is unsafe from an alignment perspective. The way I think about the race to the top is that it doesn't matter who wins, everyone wins, right? So the way the race to the top works is you set an example for how the field works. You say, you know, we're going to engage in this practice. So a key example of this is responsible scaling policies. We were the first to put out a responsible scaling policy. And, you know, we didn't say everyone else should do this or you're bad guys. We didn't, you know, we didn't, you know, kind of try to use it as an advantage. We put it out and then we encouraged everyone else to do it. And many, and then we discovered in the months after that, that, you know, there were people within the other companies who were trying to put out responsible scaling policies. But the fact that we had done it allowed, you know, gave those people permission, right? Kind of, kind of enabled those people to, you know, to make the argument to leadership, hey, Anthropic is doing this, so we should do it as well. The same has been true of investing in interpretability. We release our interpretability research to everyone and allow other companies to copy it, even though we've seen that it sometimes has commercial advantages. Same with things like constitutional AI, same with the, you know, the measurement of the dangers of our system, dangerous capabilities evals. So we're trying to set an example for the field, but there's an interplay where it helps to be a powerful commercial competitor. I've said nothing that anywhere near resembles the idea that this company should be the only one to build the technology. I don't know how anyone could ever derive that from anything that I've said. It's just, yeah, yeah, it's just an incredible and bad faith distortion. All right, let's see if we can lightning around like one or two before I ask you the last one, which we'll have five minutes for. What happened with SBF? What happened with SBF? I mean, he was one of the, go ahead. I couldn't tell you. What didn't you answer? I probably met the guy four or five times. Okay. So I have no great insight into the, you know, what, you know, into the psychology of SBF or, you know, why he did things as stupid or immoral as he did. I think the only, you know, the only thing I had ever seen ahead of time with SBF was, you know, a couple people mentioned to me that he was like hard to work with, that, you know, he was like a bit of a move fast and break things guy. And I was like, okay, you know, there's like plenty of people. Welcome to Silicon Valley. Yeah, like welcome to Silicon Valley. And so I remember saying, okay, I'm going to give this guy non-voting shares. I'm not going to put him on the board. He sounds like a, you know, he sounds like a bad person to deal with every day. But, you know, he's excited about AI. He's excited about AI safety. He's, you know, he's a bull on AI and he's interested in AI safety. So, you know, seems like a sensible, seems like a sensible thing to do. You know, in retrospect, you know, that, you know, move fast and break things, you know, was turned out to be much, much, much more extreme and bad than, you know, than I ever imagined. Okay. So let's end here. So you found your impact. I mean, you're working the dream pretty much right now. I mean, think about all the ways that AI can be used for biology just to start. You also say that this is a dangerous technology. And I'm curious if your desire for impact could be pushing you to accelerate this technology while, you know, potentially devaluing the possibility that it could, that controlling it might not be feasible. So, you know, I think I have more than anyone else in the industry warned about the dangers of the technology, right? We just spent 10, 20 minutes talking about, you know, the frightening, you know, the large array of, you know, people who run, you know, trillion dollar companies criticizing me for, you know, for talking about the dangers of these technologies. Right. You know, I have U.S. government officials. I have people who run $4 trillion companies criticizing me for talking about the dangers of the technology. Right. Imputing all these bizarre motives that bear no relationship to, you know, to anything I've ever said, not supported in anything I've ever done. And yet I'm going to continue to do it. I actually think that, you know, as the revenues, as the economic business of AI ramps up and it's ramping up exponentially, you know, if I'm right, in a couple years, it'll be the biggest source of revenue in the world. Right. It'll be the biggest industry in the world. And people who run companies already think it. So we actually have this terrifying situation where, you know, hundreds of billions to trillions to, I would say, maybe 20 trillion of capitals on the side of Accelerate AI as fast as possible. We have this, you know, company that's very valuable in absolute terms, but, you know, looks very small compared to that. Right. 60, 60, 60 billion dollars. And I keep speaking up, even if, you know, it makes folks and, you know, there have been these articles, you know, some folks in the U.S. government are upset at us, for example, for opposing the moratorium on AI regulation, for being in favor of export controls for chips on China, for talking about the economic impacts of AI. Every time I do that, I get attacked by many of my peers. Right. But you're still assuming that we can control it. That's what I'm pointing out. But I'm just telling you how much effort, how much persistence, how much despite everything that stacked up, despite all the dangers, despite the risk that it has to the company of being willing to speak up, I'm willing to do it. And that's why I'm saying that, look, if I thought that there was no way to control the technology, right? If I thought, even if I thought this is just a gamble, right? Some people are like, oh, you think there's a 5 or 10 percent chance that AI could go wrong. You're just rolling the dice. That's not the way I think about it. This is a multi-step game. Right. You take one step. You build the next step of most powerful models. You have a more intensive testing regime. As we get closer and closer to the more powerful models, I'm speaking up more and more. And I'm taking more and more drastic actions because I'm concerned that the risks of AI are getting closer and closer. We're working to address them. We've made a certain amount of progress. But when I worry that the progress that we've made on the risks is not fully aligned with the â€“ is not going as fast as we need to go for the speed of the technology, then I speak up louder. And so you're asking, why am I â€“ you started this interview by saying, what's gotten into you? Why are you talking about this? It's because the exponential is getting to the point that I worry that we may have a situation that our ability to handle the risks is not keeping up with the speed of the technology. And that's how I'm responding to it. If I believe that there was no way to control the technology, which I â€“ I see absolutely no evidence for that proposition. We've gotten better at controlling models with every model that we release. Right. All these things go wrong, but like you really â€“ you really have to stress test the models pretty hard. That doesn't mean you can't have emergent bad behavior. And I think, you know, if we got to much more powerful models with only the alignment techniques we have now, then I'd be very concerned. Then I'd be out there saying everyone should stop building these things. Even China should stop building these. I don't think they'd listen to me, which is one reason I think export controls is a better â€“ is a better measure. But if we got a few years ahead in models and had only the alignment and steering techniques we had today, then, you know, I would definitely be advocating for us to, you know, to slow down a lot. The reason I'm warning about the risk is so that we don't have to slow down, so that we can invest in safety techniques and can continue the progress â€“ continue the progress of the field. It would be a huge economic effort. Even if one company was willing to slow down the technology, you know, that doesn't stop all the other companies. That doesn't stop our geopolitical adversaries to whom this is an existential fight, fight, fight, fight for survival. So, you know, there's very little, you know, there's very little latitude here, right? We're stuck between all the benefits of the technology, the race to accelerate it, and the fact that that is a multi-party race. And so I am doing the best thing I can do, which is to invest in safety technology to speed up the progress of safety. I've written essays on the importance of interpretability, on how important various directions in safety are. We release all of our safety work openly because we think that's the thing that's a public good. That's the thing that everyone needs to share. So if you have a better strategy for balancing the benefits, the inevitability of the technology, and the risks that it face, I am very open to hear it. Because I go to sleep every night thinking about it because I have such an incredible understanding of the stakes in terms of the benefits, in terms of, you know, what it can do, the lives that it can save. I've seen that personally. I also have seen the risks personally. We've already seen things go wrong with the models. You know, we have an example of that with Brock. And, you know, people dismiss this, but they're not going to laugh anymore when the models are taking actions, when they're manufacturing, and when they're in charge of, you know, medical, medical, medical interventions, right? People can laugh at the risks when the models are just talking. But I think it's very serious. And so I think what this situation demands is a very serious understanding of both the risks and the benefits. These are high-stakes decisions. They need to be made with a seriousness. And I think something that makes me very concerned is that, on one hand, we have a cadre of people who are just doomers. People call me a doomer. I'm not. But there are doomers out there, people who say they know there's no way to build this safely. You know, I've looked at their arguments. They're a bunch of gobbledygook. The idea that these models have dangers associated with them, including dangers to humanity as a whole, that makes sense to me. The idea that we can kind of logically prove that there's no way to make them safe, that seems like nonsense to me. So I think that is an intellectually and morally unserious way to respond to the situation we're in. I also think it is intellectually and morally unserious for people who are sitting on $20 trillion of capital, who all work together because their incentives are all in the same way. There are dollar signs in all of their eyes, to sit there and say, we shouldn't regulate this technology for 10 years. Anyone who says that we should worry about the safety of these models is someone who just wants to control the technology themselves. That's an outrageous claim. And it's a morally unserious claim. We've sat here and we've done every possible piece of research. We speak up when we believe it's appropriate to do so. We've tried to back up, you know, when we make claims about the economic impact of AI. We have an economic research council. We have an economic index that we use to track the model in real time. And we're giving grants for people to understand the economic impact of the technology. I think for people who are far more financially invested in the success of the technology than I am to just, you know, breezily lob ad hominem attacks. You know, I think that is just as intellectually and morally unserious as the doomer's position. I think what we need here is we need more thoughtfulness. We need more honesty. We need more people willing to go against their interest, willing to not have, you know, breezy Twitter fights, hot takes. We need people to actually invest in understanding the situation, actually do the work, actually put out the research, and actually add some light and some insight to the situation that we're in. I am trying to do that. I don't think I'm doing that perfectly as no human can. I'm trying to do it as well as I can. It would be very helpful if there were others who would try to do the same thing. Well, Dario, I said this off camera, but I want to make sure to say it on as we're wrapping up. I appreciate how much Anthropic publishes. We have learned a ton from the experiments, everything from red teaming the models to vending machine Claude, which we didn't have a chance to speak about today. But I think the world is better off just to hear everything going on here. And to that note, thank you for sitting down with me and spending so much time together. Thanks for having me. Thanks, everybody, for listening and watching, and we'll see you next time on Big Technology Podcast.",
  "segments": [
    {
      "id": 0,
      "start": 0.8,
      "end": 3.36,
      "text": "I get very angry when people call me a doomer."
    },
    {
      "id": 1,
      "start": 3.36,
      "end": 5.76,
      "text": "When someone's like, this guy's a doomer,"
    },
    {
      "id": 2,
      "start": 5.76,
      "end": 7.68,
      "text": "he wants to slow things down."
    },
    {
      "id": 3,
      "start": 7.68,
      "end": 8.84,
      "text": "You heard what I just said."
    },
    {
      "id": 4,
      "start": 8.84,
      "end": 11.92,
      "text": "Like, my father died because of cures"
    },
    {
      "id": 5,
      "start": 11.92,
      "end": 15.280000000000001,
      "text": "that could have happened a few years later."
    },
    {
      "id": 6,
      "start": 15.280000000000001,
      "end": 17.16,
      "text": "I understand the benefit of this technology."
    },
    {
      "id": 7,
      "start": 17.16,
      "end": 18.76,
      "text": "I'm sure you've heard the criticism"
    },
    {
      "id": 8,
      "start": 18.76,
      "end": 20.88,
      "text": "from people like Jensen who say, well,"
    },
    {
      "id": 9,
      "start": 20.88,
      "end": 24.36,
      "text": "Dario thinks he's the only one who can build this safely"
    },
    {
      "id": 10,
      "start": 24.36,
      "end": 26.84,
      "text": "and therefore wants to control the entire industry."
    },
    {
      "id": 11,
      "start": 26.84,
      "end": 27.84,
      "text": "What do you think about that?"
    },
    {
      "id": 12,
      "start": 27.84,
      "end": 29.6,
      "text": "I've never said anything like that."
    },
    {
      "id": 13,
      "start": 29.6,
      "end": 31.080000000000002,
      "text": "That's an outrageous lie."
    },
    {
      "id": 14,
      "start": 31.080000000000002,
      "end": 33.68,
      "text": "That's the most outrageous lie I've ever heard."
    },
    {
      "id": 15,
      "start": 33.68,
      "end": 36.800000000000004,
      "text": "Anthropic CEO Dario Amode joins us to talk about"
    },
    {
      "id": 16,
      "start": 36.800000000000004,
      "end": 39.6,
      "text": "the path forward for artificial intelligence,"
    },
    {
      "id": 17,
      "start": 39.6,
      "end": 42.28,
      "text": "whether generative AI is a good business,"
    },
    {
      "id": 18,
      "start": 42.28,
      "end": 45.480000000000004,
      "text": "and to fire back at those who call him a doomer."
    },
    {
      "id": 19,
      "start": 45.480000000000004,
      "end": 47.120000000000005,
      "text": "And he's here with us in studio"
    },
    {
      "id": 20,
      "start": 47.120000000000005,
      "end": 50.120000000000005,
      "text": "at Anthropic headquarters in San Francisco."
    },
    {
      "id": 21,
      "start": 50.120000000000005,
      "end": 51.8,
      "text": "Dario, it's great to see you again."
    },
    {
      "id": 22,
      "start": 51.8,
      "end": 52.92,
      "text": "Welcome to the show."
    },
    {
      "id": 23,
      "start": 52.92,
      "end": 54.08,
      "text": "Thank you for having me."
    },
    {
      "id": 24,
      "start": 54.08,
      "end": 56.760000000000005,
      "text": "So let's recap the past couple of months for you."
    },
    {
      "id": 25,
      "start": 56.76,
      "end": 60.839999999999996,
      "text": "You said AI could wipe out half of entry-level white collar jobs."
    },
    {
      "id": 26,
      "start": 60.839999999999996,
      "end": 64.84,
      "text": "You cut off Windsurf's access to Anthropic's top tier models"
    },
    {
      "id": 27,
      "start": 64.84,
      "end": 67.96,
      "text": "when you learned that OpenAI was going to acquire them."
    },
    {
      "id": 28,
      "start": 67.96,
      "end": 69.96,
      "text": "You asked the government for export controls"
    },
    {
      "id": 29,
      "start": 69.96,
      "end": 72.96,
      "text": "and annoyed NVIDIA CEO Jensen Wang."
    },
    {
      "id": 30,
      "start": 72.96,
      "end": 74.44,
      "text": "What's gotten into you?"
    },
    {
      "id": 31,
      "start": 74.44,
      "end": 83.36,
      "text": "You know, I think myself and Anthropic are always focused on kind of trying to do and say the"
    },
    {
      "id": 32,
      "start": 83.36,
      "end": 84.24,
      "text": "things that we believe."
    },
    {
      "id": 33,
      "start": 84.24,
      "end": 91.11999999999999,
      "text": "And I think as we've gotten more close to AI systems that are more powerful,"
    },
    {
      "id": 34,
      "start": 92.0,
      "end": 95.11999999999999,
      "text": "you know, I think I've wanted to say those things,"
    },
    {
      "id": 35,
      "start": 96.32,
      "end": 101.19999999999999,
      "text": "you know, more forcefully, more publicly to make the point clearer."
    },
    {
      "id": 36,
      "start": 101.19999999999999,
      "end": 104.88,
      "text": "You know, I've been saying for many years that, you know, we have these,"
    },
    {
      "id": 37,
      "start": 104.88,
      "end": 108.08,
      "text": "we can talk in detail about them, but, you know, we have these scaling laws."
    },
    {
      "id": 38,
      "start": 108.08,
      "end": 110.0,
      "text": "AI systems are getting more powerful."
    },
    {
      "id": 39,
      "start": 110.0,
      "end": 115.04,
      "text": "They're going from the level of, you know, a few years ago, they were barely coherent."
    },
    {
      "id": 40,
      "start": 115.04,
      "end": 118.48,
      "text": "Now, you know, a couple of years ago, they were at the level of a smart high school student."
    },
    {
      "id": 41,
      "start": 118.48,
      "end": 122.56,
      "text": "Now we're getting to smart college student, PhD, and they're starting to,"
    },
    {
      "id": 42,
      "start": 122.56,
      "end": 124.56,
      "text": "they're starting to apply across the economy."
    },
    {
      "id": 43,
      "start": 124.56,
      "end": 131.52,
      "text": "And so I think all the issues related to AI, ranging from kind of the national security issues"
    },
    {
      "id": 44,
      "start": 131.52,
      "end": 138.88000000000002,
      "text": "to the economic issues, you know, are starting to become quite near to where, to where we,"
    },
    {
      "id": 45,
      "start": 139.60000000000002,
      "end": 141.84,
      "text": "you know, to where we're actually going to face them."
    },
    {
      "id": 46,
      "start": 141.84,
      "end": 146.24,
      "text": "And so, and so I think as these problems have come closer, I've, you know, even though,"
    },
    {
      "id": 47,
      "start": 146.88,
      "end": 150.08,
      "text": "you know, in some form, Anthropic has been saying these things for a while,"
    },
    {
      "id": 48,
      "start": 150.08,
      "end": 152.56,
      "text": "I think the urgency of these things has gone up."
    },
    {
      "id": 49,
      "start": 152.56,
      "end": 159.04,
      "text": "And, you know, I want to make sure that we, you know, I want to make sure that we say what we"
    },
    {
      "id": 50,
      "start": 159.04,
      "end": 164.0,
      "text": "believe and that we warn the world about the possible downsides, even though, you know,"
    },
    {
      "id": 51,
      "start": 164.56,
      "end": 166.16,
      "text": "no one can say what's going to happen."
    },
    {
      "id": 52,
      "start": 166.16,
      "end": 172.56,
      "text": "We're, you know, we're saying what we, you know, what we think might happen, what we think is likely"
    },
    {
      "id": 53,
      "start": 172.56,
      "end": 173.12,
      "text": "to happen."
    },
    {
      "id": 54,
      "start": 173.12,
      "end": 177.68,
      "text": "You know, we, we back it up as, as best we can, although it's often, you know,"
    },
    {
      "id": 55,
      "start": 177.68,
      "end": 181.84,
      "text": "extrapolations about the future where, where no one, where no one can be sure."
    },
    {
      "id": 56,
      "start": 182.4,
      "end": 187.6,
      "text": "Um, but, you know, I think we, we see ourselves as kind of, as kind of having the duty to,"
    },
    {
      "id": 57,
      "start": 187.6,
      "end": 190.24,
      "text": "you know, to kind of warn the world about what's going to happen."
    },
    {
      "id": 58,
      "start": 190.24,
      "end": 193.92000000000002,
      "text": "And that's not to say, you know, I think there's an incredible number of like positive"
    },
    {
      "id": 59,
      "start": 193.92000000000002,
      "end": 195.36,
      "text": "applications of AI, right?"
    },
    {
      "id": 60,
      "start": 195.36,
      "end": 197.28,
      "text": "I've, I've kind of continued to talk about that."
    },
    {
      "id": 61,
      "start": 197.28,
      "end": 200.32,
      "text": "I read this, I wrote this essay, Machines of Loving Grace."
    },
    {
      "id": 62,
      "start": 200.32,
      "end": 208.4,
      "text": "Um, I, I feel in fact that I and Anthropic have often been able to do a better job of articulating"
    },
    {
      "id": 63,
      "start": 208.4,
      "end": 213.6,
      "text": "the benefits of AI than some of the people who call themselves optimists or accelerationists."
    },
    {
      "id": 64,
      "start": 213.6,
      "end": 218.0,
      "text": "Um, so I think we probably appreciate the benefits more than, more than anyone."
    },
    {
      "id": 65,
      "start": 218.0,
      "end": 222.88,
      "text": "Um, but for exactly the same reason, because we can have such a good world if we get everything"
    },
    {
      "id": 66,
      "start": 222.88,
      "end": 225.6,
      "text": "right, I feel obligated to warn about the risks."
    },
    {
      "id": 67,
      "start": 225.6,
      "end": 228.56,
      "text": "So all of this is coming from your timeline."
    },
    {
      "id": 68,
      "start": 228.56,
      "end": 231.6,
      "text": "Basically you, it seems like you have a shorter timeline than most."
    },
    {
      "id": 69,
      "start": 231.6,
      "end": 236.08,
      "text": "And so you were feeling a sense of urgency to get out there because you think that this is imminent."
    },
    {
      "id": 70,
      "start": 236.08,
      "end": 237.28,
      "text": "Yes, I'm not sure."
    },
    {
      "id": 71,
      "start": 237.28,
      "end": 242.48,
      "text": "Um, you know, I think it's very hard to predict, particularly on the societal side."
    },
    {
      "id": 72,
      "start": 242.48,
      "end": 248.32,
      "text": "So if you say, you know, when are people going to deploy AI or when are companies going to use,"
    },
    {
      "id": 73,
      "start": 248.32,
      "end": 255.92,
      "text": "you know, X, X dollars of spend of AI or, you know, when will, when will AI, um, you know, be,"
    },
    {
      "id": 74,
      "start": 255.92,
      "end": 260.96,
      "text": "be, be, be used in these applications or when will it drive these medical cures?"
    },
    {
      "id": 75,
      "start": 261.6,
      "end": 263.84,
      "text": "That's kind of harder, harder to say."
    },
    {
      "id": 76,
      "start": 263.84,
      "end": 269.28,
      "text": "I think the underlying technology is more predictable, but still uncertain, still no one knows."
    },
    {
      "id": 77,
      "start": 269.28,
      "end": 273.52,
      "text": "But I think on the underlying technology, I've started to become more confident."
    },
    {
      "id": 78,
      "start": 273.52,
      "end": 275.76,
      "text": "There isn't no uncertainty about it."
    },
    {
      "id": 79,
      "start": 275.76,
      "end": 280.0,
      "text": "You know, I think the, the exponential that we're on could, could kind of still,"
    },
    {
      "id": 80,
      "start": 280.55999999999995,
      "end": 282.4,
      "text": "um, you know, could totally peter out."
    },
    {
      "id": 81,
      "start": 282.4,
      "end": 289.35999999999996,
      "text": "You know, I think there's maybe, uh, I don't know, 20 or 25% chance that sometime in the next two years,"
    },
    {
      "id": 82,
      "start": 289.35999999999996,
      "end": 293.44,
      "text": "the models just start getting, stop getting better for reasons we don't understand, or maybe reasons"
    },
    {
      "id": 83,
      "start": 293.44,
      "end": 297.28,
      "text": "we do understand, like, you know, data or compute availability."
    },
    {
      "id": 84,
      "start": 297.28,
      "end": 300.79999999999995,
      "text": "And then everything I'm saying just, just seems, seems totally silly."
    },
    {
      "id": 85,
      "start": 300.79999999999995,
      "end": 303.12,
      "text": "And everyone makes fun of me for all the warnings I've made."
    },
    {
      "id": 86,
      "start": 303.12,
      "end": 307.52,
      "text": "And, and, you know, I'm just, I'm just totally fine with that, given, given the distribution that,"
    },
    {
      "id": 87,
      "start": 307.52,
      "end": 309.59999999999997,
      "text": "that, that, that, given distribution that I see."
    },
    {
      "id": 88,
      "start": 309.6,
      "end": 314.24,
      "text": "And so I should say that this is part, our conversation is part of a profile I'm writing about you."
    },
    {
      "id": 89,
      "start": 314.24,
      "end": 318.96000000000004,
      "text": "I've spoken with more than two dozen people who've worked with you, who know you, who've competed with you."
    },
    {
      "id": 90,
      "start": 318.96000000000004,
      "end": 320.40000000000003,
      "text": "And I'm going to link that in the show notes."
    },
    {
      "id": 91,
      "start": 320.40000000000003,
      "end": 322.24,
      "text": "If anybody wants to read it, it's free to read."
    },
    {
      "id": 92,
      "start": 322.72,
      "end": 327.04,
      "text": "Uh, but one of the themes that has, uh, come through across everybody I've spoken with"
    },
    {
      "id": 93,
      "start": 327.04,
      "end": 331.28000000000003,
      "text": "is that you have about the shortest timeline of any of the major lab leaders."
    },
    {
      "id": 94,
      "start": 331.28000000000003,
      "end": 334.24,
      "text": "And you just referenced it, uh, just, just now."
    },
    {
      "id": 95,
      "start": 334.24,
      "end": 338.96000000000004,
      "text": "So why do you have such a short timeline and why should we believe in yours?"
    },
    {
      "id": 96,
      "start": 338.96,
      "end": 341.84,
      "text": "Yeah, it, it really depends what you mean by timeline."
    },
    {
      "id": 97,
      "start": 342.32,
      "end": 347.52,
      "text": "Um, so one thing, and you know, I've, I've, I've been consistent on this over the years is,"
    },
    {
      "id": 98,
      "start": 347.52,
      "end": 352.24,
      "text": "you know, there are these terms in the AI world, like AGI and super intelligence."
    },
    {
      "id": 99,
      "start": 352.24,
      "end": 357.67999999999995,
      "text": "Like you'll hear leaders of companies say, we've achieved AGI, we're moving on to super intelligence."
    },
    {
      "id": 100,
      "start": 357.67999999999995,
      "end": 362.4,
      "text": "Or like, it's really exciting that someone stopped working on AGI and started working on super intelligence."
    },
    {
      "id": 101,
      "start": 362.4,
      "end": 364.47999999999996,
      "text": "So I think these terms are totally meaningless."
    },
    {
      "id": 102,
      "start": 364.47999999999996,
      "end": 365.76,
      "text": "I don't know what AGI is."
    },
    {
      "id": 103,
      "start": 365.76,
      "end": 367.84,
      "text": "I don't know what super intelligence is."
    },
    {
      "id": 104,
      "start": 367.84,
      "end": 370.56,
      "text": "It, it sounds like a, it sounds like a marketing term."
    },
    {
      "id": 105,
      "start": 370.56,
      "end": 371.2,
      "text": "It is marketing."
    },
    {
      "id": 106,
      "start": 371.2,
      "end": 371.59999999999997,
      "text": "Yeah."
    },
    {
      "id": 107,
      "start": 371.59999999999997,
      "end": 376.23999999999995,
      "text": "It sounds like, you know, something, something designed to activate people's, people's dopamine."
    },
    {
      "id": 108,
      "start": 376.23999999999995,
      "end": 378.88,
      "text": "So you'll see in public, I never use those terms."
    },
    {
      "id": 109,
      "start": 378.88,
      "end": 384.8,
      "text": "And I, you know, I'm, I'm, I'm actually, you know, careful to criticize the use of those terms."
    },
    {
      "id": 110,
      "start": 384.8,
      "end": 394.0,
      "text": "Um, but I think, I think despite that, I am, I am indeed one of the most bullish about, about AI capabilities improving very fast."
    },
    {
      "id": 111,
      "start": 394.0,
      "end": 398.16,
      "text": "The thing I think is real that I've said over and over again is the exponential."
    },
    {
      "id": 112,
      "start": 398.16,
      "end": 403.52,
      "text": "The idea that every few months we get an AI model that is better than the AI model we got before."
    },
    {
      "id": 113,
      "start": 403.52,
      "end": 412.72,
      "text": "And that we, we get that by investing more compute in AI models, more data, more new types of training models."
    },
    {
      "id": 114,
      "start": 412.72,
      "end": 418.48,
      "text": "Initially, this was done by what's called pre-training, which is when you just feed a bunch of data from the internet into the model."
    },
    {
      "id": 115,
      "start": 418.48,
      "end": 425.44000000000005,
      "text": "Now we have a second stage that's reinforcement learning or test time compute or reasoning or whatever you want to call it."
    },
    {
      "id": 116,
      "start": 425.44000000000005,
      "end": 428.96000000000004,
      "text": "I think of it as a second stage that involves reinforcement learning."
    },
    {
      "id": 117,
      "start": 428.96000000000004,
      "end": 432.48,
      "text": "Now, both of those things are scaling up together as we've seen with our models."
    },
    {
      "id": 118,
      "start": 432.48,
      "end": 441.28000000000003,
      "text": "And as we've seen with models from other, from other companies, and I don't see anything blocking that the further scaling of that."
    },
    {
      "id": 119,
      "start": 441.28,
      "end": 445.44,
      "text": "There's some stuff about, you know, how do we broaden the tasks."
    },
    {
      "id": 120,
      "start": 445.44,
      "end": 457.67999999999995,
      "text": "On the RL side of it, we've seen more progress on, say, math and code, where the models are, you know, getting pretty close to like a high professional level and less on more subjective tasks."
    },
    {
      "id": 121,
      "start": 457.67999999999995,
      "end": 460.71999999999997,
      "text": "But I think that is very much a temporary obstacle."
    },
    {
      "id": 122,
      "start": 462.23999999999995,
      "end": 469.2,
      "text": "So when I look at it, I see this exponential and I say, look, people aren't very good at making sense of exponentials, right?"
    },
    {
      "id": 123,
      "start": 469.2,
      "end": 478.08,
      "text": "Like, you know, if something is doubling every six months, then, you know, two years before it happens, it looks like it's only 1 16th of the way there."
    },
    {
      "id": 124,
      "start": 478.08,
      "end": 482.48,
      "text": "And so we are sitting here in the middle of 2025."
    },
    {
      "id": 125,
      "start": 483.12,
      "end": 487.52,
      "text": "And the models are really starting to explode in terms of the economy, right?"
    },
    {
      "id": 126,
      "start": 487.52,
      "end": 491.59999999999997,
      "text": "If you look at the capabilities of the model, they're starting to saturate all the benchmarks."
    },
    {
      "id": 127,
      "start": 491.6,
      "end": 496.16,
      "text": "If you look at revenue, you know, Anthropics revenue every year has grown 10x."
    },
    {
      "id": 128,
      "start": 497.44,
      "end": 502.0,
      "text": "Every year, we're kind of conservative and we say, you know, it can't grow 10x."
    },
    {
      "id": 129,
      "start": 502.0,
      "end": 509.76000000000005,
      "text": "This time, you know, I never assume anything and actually always am very conservative in saying, I think it's going to slow down on the business side."
    },
    {
      "id": 130,
      "start": 509.76,
      "end": 512.8,
      "text": "But we went from zero to 100 million in 2023."
    },
    {
      "id": 131,
      "start": 513.2,
      "end": 515.76,
      "text": "We went from 100 million to a billion in 2024."
    },
    {
      "id": 132,
      "start": 516.4,
      "end": 524.24,
      "text": "And, you know, this year, in this first half of the year, we've gone from 1 billion to, you know, I think as of speaking today, it's well above 4."
    },
    {
      "id": 133,
      "start": 524.24,
      "end": 525.28,
      "text": "It might be 4.5."
    },
    {
      "id": 134,
      "start": 526.56,
      "end": 530.4,
      "text": "And so if you think about it, you know, suppose that exponential continued for two years."
    },
    {
      "id": 135,
      "start": 530.46,
      "end": 531.52,
      "text": "I'm not saying it will."
    },
    {
      "id": 136,
      "start": 531.76,
      "end": 533.5,
      "text": "But suppose it continued for two years."
    },
    {
      "id": 137,
      "start": 533.92,
      "end": 535.9,
      "text": "You know, you're like well into the 100 billions."
    },
    {
      "id": 138,
      "start": 536.0,
      "end": 537.18,
      "text": "I'm not saying that'll happen."
    },
    {
      "id": 139,
      "start": 537.18,
      "end": 543.0799999999999,
      "text": "I'm saying the situation is that when you're on an exponential, you can really get fooled by it."
    },
    {
      "id": 140,
      "start": 543.18,
      "end": 549.6999999999999,
      "text": "Two years away from when the exponential goes totally crazy, you know, it looks like it's just starting to be a thing."
    },
    {
      "id": 141,
      "start": 550.4799999999999,
      "end": 552.66,
      "text": "And so that's the fundamental dynamic."
    },
    {
      "id": 142,
      "start": 552.8199999999999,
      "end": 562.42,
      "text": "You know, we saw that with the Internet in the 90s, right, where it was like, you know, networking speeds and the underlying speed of the computers were getting fast."
    },
    {
      "id": 143,
      "start": 562.42,
      "end": 573.4399999999999,
      "text": "And over a few years, it became possible to have to basically build a digital global communications network on top of all this when it wasn't possible just a few years ago."
    },
    {
      "id": 144,
      "start": 573.5999999999999,
      "end": 579.4599999999999,
      "text": "And almost no one except for a few people really saw the implications of that and how fast it would happen."
    },
    {
      "id": 145,
      "start": 579.88,
      "end": 581.7199999999999,
      "text": "And so that's that's where I'm coming from."
    },
    {
      "id": 146,
      "start": 581.8199999999999,
      "end": 582.66,
      "text": "That's what I think."
    },
    {
      "id": 147,
      "start": 582.74,
      "end": 587.48,
      "text": "Now, I don't know, like if a bunch of satellites crashed, maybe the Internet would have taken longer."
    },
    {
      "id": 148,
      "start": 587.48,
      "end": 589.9200000000001,
      "text": "If there was an economic crash, maybe it would have taken a little longer."
    },
    {
      "id": 149,
      "start": 590.08,
      "end": 593.24,
      "text": "So we can't be sure of the exact timelines."
    },
    {
      "id": 150,
      "start": 593.4200000000001,
      "end": 602.38,
      "text": "But I think people are getting fooled by the exponential and not realizing how fast it might be, how fast I think it probably will, although I'm not sure."
    },
    {
      "id": 151,
      "start": 603.12,
      "end": 608.32,
      "text": "But so many folks in the AI industry are talking about diminishing returns from scaling now."
    },
    {
      "id": 152,
      "start": 608.46,
      "end": 611.1800000000001,
      "text": "That really doesn't fit with the vision you just laid out."
    },
    {
      "id": 153,
      "start": 611.24,
      "end": 611.72,
      "text": "Are they wrong?"
    },
    {
      "id": 154,
      "start": 611.72,
      "end": 617.4200000000001,
      "text": "Yeah, I from what we've seen, I can only speak in terms of the models at Anthropic."
    },
    {
      "id": 155,
      "start": 618.08,
      "end": 623.6,
      "text": "But what I've seen in terms of the models at Anthropic, if we look at, you know, let's take coding."
    },
    {
      "id": 156,
      "start": 623.74,
      "end": 627.6800000000001,
      "text": "Coding is one area where, you know, I think Anthropic models have advanced very quickly."
    },
    {
      "id": 157,
      "start": 627.8000000000001,
      "end": 629.0,
      "text": "Adoption has been very quick."
    },
    {
      "id": 158,
      "start": 629.24,
      "end": 630.4200000000001,
      "text": "We're not just a coding company."
    },
    {
      "id": 159,
      "start": 630.5,
      "end": 632.3000000000001,
      "text": "We're planning to expand to many areas."
    },
    {
      "id": 160,
      "start": 632.3,
      "end": 646.38,
      "text": "But if you look at coding, you know, we released 3.5 Sonnet, a model we call 3.5 Sonnet V2, which, you know, let's call it 3.6 Sonnet now."
    },
    {
      "id": 161,
      "start": 646.74,
      "end": 651.42,
      "text": "3.7 Sonnet, and then 4.0 Sonnet and 4.0 Opus."
    },
    {
      "id": 162,
      "start": 651.42,
      "end": 658.9,
      "text": "And, you know, that series of four or five models, each one got substantially better at coding than the last."
    },
    {
      "id": 163,
      "start": 658.9599999999999,
      "end": 667.0799999999999,
      "text": "If you want to look at benchmarks, you can look at, you know, SWE Bench growing from, you know, I think 18 months ago, it was at like 3% or something."
    },
    {
      "id": 164,
      "start": 667.54,
      "end": 672.5,
      "text": "Growing all the way to, you know, 72% to 80%, depending on how you measure it."
    },
    {
      "id": 165,
      "start": 672.6999999999999,
      "end": 675.78,
      "text": "And the real usage has grown exponentially as well."
    },
    {
      "id": 166,
      "start": 675.86,
      "end": 679.9,
      "text": "We're heading more and more towards autonomously you can just use these models."
    },
    {
      "id": 167,
      "start": 679.9,
      "end": 694.4,
      "text": "I think the actual majority of code that's written at Anthropic is, you know, at this point written by or at least with the involvement of one, you know, one of the Claude models."
    },
    {
      "id": 168,
      "start": 695.14,
      "end": 703.66,
      "text": "And various other companies have said, you know, have said, you know, similar statements to that."
    },
    {
      "id": 169,
      "start": 703.66,
      "end": 709.5799999999999,
      "text": "So we see the progress as being very fast and the exponential is continuing, and we don't see any diminishing returns."
    },
    {
      "id": 170,
      "start": 709.9,
      "end": 713.6,
      "text": "But there are some liabilities, it seems like, with large language models."
    },
    {
      "id": 171,
      "start": 713.76,
      "end": 715.14,
      "text": "For instance, continual learning."
    },
    {
      "id": 172,
      "start": 715.62,
      "end": 717.3199999999999,
      "text": "We had Dwarke on a couple weeks ago."
    },
    {
      "id": 173,
      "start": 717.48,
      "end": 719.36,
      "text": "Here's how he put it, and he wrote about it in his sub stack."
    },
    {
      "id": 174,
      "start": 719.4399999999999,
      "end": 722.06,
      "text": "The lack of continual learning is a huge, huge problem."
    },
    {
      "id": 175,
      "start": 722.4,
      "end": 729.24,
      "text": "The LLM baseline at many tasks might be higher than an average human, but you're stuck with the abilities you get out of the box."
    },
    {
      "id": 176,
      "start": 729.3,
      "end": 731.4399999999999,
      "text": "So you just make the model, and that's it."
    },
    {
      "id": 177,
      "start": 731.48,
      "end": 732.1,
      "text": "It doesn't learn."
    },
    {
      "id": 178,
      "start": 732.42,
      "end": 734.04,
      "text": "That seems like a glaring liability."
    },
    {
      "id": 179,
      "start": 734.18,
      "end": 734.98,
      "text": "What do you think about that?"
    },
    {
      "id": 180,
      "start": 734.98,
      "end": 753.4200000000001,
      "text": "So first of all, I would say, even if we never solved continual learning, even if we never solved continual learning and memory, I think that the potential for the LLMs to do, you know, incredibly well to, you know, affect things at the scale of the economy will be very high."
    },
    {
      "id": 181,
      "start": 753.42,
      "end": 760.36,
      "text": "If I think of the field I used to be in, biology and medicine, like, you know, let's say I had a very smart Nobel Prize winner."
    },
    {
      "id": 182,
      "start": 761.02,
      "end": 766.64,
      "text": "And, you know, I said, okay, you know, you've discovered all these things."
    },
    {
      "id": 183,
      "start": 766.74,
      "end": 768.74,
      "text": "You have this incredibly smart mind."
    },
    {
      "id": 184,
      "start": 768.88,
      "end": 773.42,
      "text": "But, you know, you can't, you know, you can't, like, read new textbooks or absorb any new information."
    },
    {
      "id": 185,
      "start": 773.78,
      "end": 774.76,
      "text": "I mean, that would be difficult."
    },
    {
      "id": 186,
      "start": 774.76,
      "end": 780.34,
      "text": "But, like, still, if you had, like, 10 million of those, like, they're still going to make a lot of biology breakthroughs."
    },
    {
      "id": 187,
      "start": 780.36,
      "end": 781.3,
      "text": "Like, they're going to be limited."
    },
    {
      "id": 188,
      "start": 781.48,
      "end": 783.52,
      "text": "They're going to be able to do some things humans can't."
    },
    {
      "id": 189,
      "start": 783.74,
      "end": 785.84,
      "text": "And there are some things humans can do that they can't."
    },
    {
      "id": 190,
      "start": 786.06,
      "end": 791.8,
      "text": "But even that, even if we impose that as a ceiling, like, man, that's pretty damned impressive and transformative."
    },
    {
      "id": 191,
      "start": 792.04,
      "end": 796.74,
      "text": "And even if I said you never solved that, like, I think, you know, I think people are underestimating the impact."
    },
    {
      "id": 192,
      "start": 797.36,
      "end": 799.92,
      "text": "But, look, context windows are getting longer."
    },
    {
      "id": 193,
      "start": 800.22,
      "end": 803.68,
      "text": "And models actually do learn during the context window, right?"
    },
    {
      "id": 194,
      "start": 803.68,
      "end": 808.62,
      "text": "So as I, you know, talk to the model during the context window, I have a conversation."
    },
    {
      "id": 195,
      "start": 808.78,
      "end": 809.88,
      "text": "It absorbs information."
    },
    {
      "id": 196,
      "start": 810.3199999999999,
      "end": 812.6999999999999,
      "text": "The underlying weights of the model may not change."
    },
    {
      "id": 197,
      "start": 813.2199999999999,
      "end": 824.9799999999999,
      "text": "But, you know, just like I'm talking to you here and we're having a conversation and I listen to the things you say and I, you know, I think and I, like, respond to them, the models are able to do that."
    },
    {
      "id": 198,
      "start": 824.98,
      "end": 834.4,
      "text": "And from a machine learning perspective, from an AI perspective, there's no reason we can't make the context length 100 million words today, right?"
    },
    {
      "id": 199,
      "start": 834.44,
      "end": 838.22,
      "text": "Which is roughly what a human hears in their lifetime."
    },
    {
      "id": 200,
      "start": 839.08,
      "end": 842.24,
      "text": "There's no reason that we can't do that."
    },
    {
      "id": 201,
      "start": 842.24,
      "end": 843.88,
      "text": "It's really inference support."
    },
    {
      "id": 202,
      "start": 845.04,
      "end": 848.62,
      "text": "And so, again, even that fills in many of the gaps."
    },
    {
      "id": 203,
      "start": 848.78,
      "end": 851.16,
      "text": "Not all the gaps, but it fills in many of the gaps."
    },
    {
      "id": 204,
      "start": 851.3,
      "end": 856.6,
      "text": "And then there are a number of things like learning and memory that do allow us to update the weights."
    },
    {
      "id": 205,
      "start": 856.92,
      "end": 863.6,
      "text": "So, you know, there are a number of things around, you know, types of reinforcement learning, learning training."
    },
    {
      "id": 206,
      "start": 863.98,
      "end": 867.64,
      "text": "You know, we used to many years ago talk about inner loops and outer loops, right?"
    },
    {
      "id": 207,
      "start": 867.64,
      "end": 874.8199999999999,
      "text": "The inner loop is like I have some episode and I learned some things in that episode and I'm trying to optimize for the lifetime of that episode."
    },
    {
      "id": 208,
      "start": 875.3199999999999,
      "end": 878.92,
      "text": "And kind of the outer loop is the agents learning over episodes."
    },
    {
      "id": 209,
      "start": 879.6999999999999,
      "end": 884.6,
      "text": "And so I think maybe that inner loop, outer loop structure is a way to learn the continual learning."
    },
    {
      "id": 210,
      "start": 884.72,
      "end": 895.02,
      "text": "One thing we've learned in AI is whenever it feels like there's some fundamental obstacle, like two years ago we thought there was this fundamental obstacle around reasoning."
    },
    {
      "id": 211,
      "start": 895.02,
      "end": 897.38,
      "text": "It turned out just to be RL."
    },
    {
      "id": 212,
      "start": 897.5,
      "end": 900.1,
      "text": "You just train with RL and you let the model write some stuff down."
    },
    {
      "id": 213,
      "start": 900.1999999999999,
      "end": 905.1999999999999,
      "text": "You know, you let the model write things down to try and figure out objective math problems."
    },
    {
      "id": 214,
      "start": 905.74,
      "end": 920.3,
      "text": "Without being too specific, you know, I think, and we already have maybe some, you know, some evidence to suggest that this is another of those problems that is not as difficult as it seems."
    },
    {
      "id": 215,
      "start": 920.3,
      "end": 925.8599999999999,
      "text": "That will fall to scale plus a slightly different way of thinking about things."
    },
    {
      "id": 216,
      "start": 926.12,
      "end": 934.16,
      "text": "Do you think your obsession with scale might blind you to some of the new techniques like Demis Asaba says, you know, to get to AGI or you might call it super powerful AGI?"
    },
    {
      "id": 217,
      "start": 934.56,
      "end": 934.7199999999999,
      "text": "Whatever."
    },
    {
      "id": 218,
      "start": 934.9399999999999,
      "end": 936.9799999999999,
      "text": "Human level intelligence is what we're all talking about."
    },
    {
      "id": 219,
      "start": 937.3199999999999,
      "end": 939.54,
      "text": "We might need a couple of new techniques for that to happen."
    },
    {
      "id": 220,
      "start": 939.68,
      "end": 942.28,
      "text": "If you're so focused on scale, can you get there?"
    },
    {
      "id": 221,
      "start": 942.28,
      "end": 944.36,
      "text": "We're developing new techniques every day."
    },
    {
      "id": 222,
      "start": 944.64,
      "end": 944.6999999999999,
      "text": "Okay."
    },
    {
      "id": 223,
      "start": 945.72,
      "end": 952.68,
      "text": "You know, Claude is very good at code and we, you know, we don't really talk externally that much about why Claude is so good at code."
    },
    {
      "id": 224,
      "start": 952.6999999999999,
      "end": 953.66,
      "text": "Why is it so good at code?"
    },
    {
      "id": 225,
      "start": 954.9399999999999,
      "end": 957.1999999999999,
      "text": "Like I said, we don't talk externally about it."
    },
    {
      "id": 226,
      "start": 957.26,
      "end": 957.76,
      "text": "I have to ask."
    },
    {
      "id": 227,
      "start": 957.76,
      "end": 970.8199999999999,
      "text": "So, you know, every new version of Claude that we make has, you know, improvements to the architecture, improvements to the data that we put into it, improvements to the methods that we use to train it."
    },
    {
      "id": 228,
      "start": 971.04,
      "end": 973.16,
      "text": "So we're developing new techniques all the time."
    },
    {
      "id": 229,
      "start": 973.8,
      "end": 976.96,
      "text": "New techniques are a part of every, you know, model that we build."
    },
    {
      "id": 230,
      "start": 977.04,
      "end": 982.68,
      "text": "And, you know, that's why we, you know, I've said these things about like, you know, we're trying to optimize for like talent density as much as possible."
    },
    {
      "id": 231,
      "start": 982.68,
      "end": 986.34,
      "text": "Like you need that talent density in order to invent the new techniques."
    },
    {
      "id": 232,
      "start": 986.34,
      "end": 996.1800000000001,
      "text": "You know, there's one thing that's been hanging over this conversation, which is that maybe Anthropic is the company with the right idea, but the wrong resources."
    },
    {
      "id": 233,
      "start": 996.1800000000001,
      "end": 1004.84,
      "text": "Because you look at what's happening with XAI and inside Meta, where Elon's built his massive cluster."
    },
    {
      "id": 234,
      "start": 1005.34,
      "end": 1009.0600000000001,
      "text": "Mark Zuckerberg is building this five gigawatt data center."
    },
    {
      "id": 235,
      "start": 1009.3000000000001,
      "end": 1013.62,
      "text": "And they are putting so much resources towards scaling up."
    },
    {
      "id": 236,
      "start": 1014.32,
      "end": 1015.32,
      "text": "Is it possible?"
    },
    {
      "id": 237,
      "start": 1015.32,
      "end": 1019.9000000000001,
      "text": "Well, I mean, Anthropic, obviously, you have raised billions of dollars, but these are trillion dollar companies."
    },
    {
      "id": 238,
      "start": 1020.32,
      "end": 1020.46,
      "text": "Yeah."
    },
    {
      "id": 239,
      "start": 1020.58,
      "end": 1024.44,
      "text": "So we've raised, I think at this point, a little short of $20 billion."
    },
    {
      "id": 240,
      "start": 1024.96,
      "end": 1025.3600000000001,
      "text": "It's not bad."
    },
    {
      "id": 241,
      "start": 1025.5800000000002,
      "end": 1027.3200000000002,
      "text": "So that's not nothing."
    },
    {
      "id": 242,
      "start": 1027.94,
      "end": 1039.76,
      "text": "And I would also say if you look at the size of the data centers that we're building with, for example, Amazon, I don't think our data center scaling is substantially smaller than that of any of the other companies in the space."
    },
    {
      "id": 243,
      "start": 1039.76,
      "end": 1043.08,
      "text": "You know, in many cases, these things are limited by energy."
    },
    {
      "id": 244,
      "start": 1043.08,
      "end": 1044.82,
      "text": "They're limited by capitalization."
    },
    {
      "id": 245,
      "start": 1044.82,
      "end": 1053.54,
      "text": "And, you know, when people talk about, you know, these large amounts of money, they're talking about it over several years."
    },
    {
      "id": 246,
      "start": 1053.54,
      "end": 1053.82,
      "text": "Right."
    },
    {
      "id": 247,
      "start": 1053.82,
      "end": 1057.72,
      "text": "And when you hear some of these announcements, sometimes they're not funded yet."
    },
    {
      "id": 248,
      "start": 1057.9399999999998,
      "end": 1063.86,
      "text": "They're, you know, we've seen the size of the data centers that folks are building."
    },
    {
      "id": 249,
      "start": 1063.86,
      "end": 1072.54,
      "text": "And we're actually pretty confident that, you know, we will be within a rough range of the size of the data centers that they built."
    },
    {
      "id": 250,
      "start": 1072.6599999999999,
      "end": 1073.78,
      "text": "You talked about talent density."
    },
    {
      "id": 251,
      "start": 1073.8999999999999,
      "end": 1077.2199999999998,
      "text": "What do you think about what Mark Zuckerberg is doing on the talent density front?"
    },
    {
      "id": 252,
      "start": 1077.4399999999998,
      "end": 1081.58,
      "text": "I mean, combining that with these massive data centers, it seems like he's going to be able to compete."
    },
    {
      "id": 253,
      "start": 1081.58,
      "end": 1082.22,
      "text": "Yeah."
    },
    {
      "id": 254,
      "start": 1082.46,
      "end": 1098.4399999999998,
      "text": "So this is actually very interesting because, you know, one thing we noticed is that relative to other companies, you know, I think a lot fewer people from Anthropic have been caught by these."
    },
    {
      "id": 255,
      "start": 1098.5,
      "end": 1099.6999999999998,
      "text": "And it's not for lack of trying."
    },
    {
      "id": 256,
      "start": 1099.82,
      "end": 1107.3,
      "text": "I've talked to plenty of people, you know, who got these offers at Anthropic and who just turned them down, who wouldn't even talk to Mark Zuckerberg,"
    },
    {
      "id": 257,
      "start": 1107.3,
      "end": 1111.86,
      "text": "who said, you know, no, I'm staying at Anthropic."
    },
    {
      "id": 258,
      "start": 1112.1,
      "end": 1126.98,
      "text": "And our general response to this was, you know, I posted something to the whole company Slack where I said, look, you know, we are not willing to compromise our, you know, our compensation principles,"
    },
    {
      "id": 259,
      "start": 1127.1399999999999,
      "end": 1130.4199999999998,
      "text": "our principles of fairness to respond individually to these offers."
    },
    {
      "id": 260,
      "start": 1130.54,
      "end": 1133.72,
      "text": "The way things work at Anthropic is there's a series of levels."
    },
    {
      "id": 261,
      "start": 1133.72,
      "end": 1141.3,
      "text": "When candidate comes in, they get assigned a level and we don't negotiate that level because we think it's unfair."
    },
    {
      "id": 262,
      "start": 1141.48,
      "end": 1142.88,
      "text": "We want to have a systematic way."
    },
    {
      "id": 263,
      "start": 1143.46,
      "end": 1156.32,
      "text": "If, you know, if Mark Zuckerberg, you know, throws a dart at a dartboard and hits your name, that doesn't mean that you should be paid 10 times more than the guy next to you who's, you know, who's just as skilled, who's just as talented."
    },
    {
      "id": 264,
      "start": 1156.32,
      "end": 1168.02,
      "text": "And my view of the situation is that, you know, the only way you can really be hurt by this is if you allow it to destroy the culture of your company by panicking,"
    },
    {
      "id": 265,
      "start": 1168.34,
      "end": 1173.0,
      "text": "by treating people unfairly in an attempt to defend the company."
    },
    {
      "id": 266,
      "start": 1173.0,
      "end": 1180.04,
      "text": "And I think actually this was a unifying moment for the company where, you know, we didn't give in."
    },
    {
      "id": 267,
      "start": 1180.14,
      "end": 1187.0,
      "text": "We refused to compromise our principles because we had the confidence that people are Anthropic because they truly believe in the mission."
    },
    {
      "id": 268,
      "start": 1188.64,
      "end": 1193.58,
      "text": "And, you know, I think that gets to kind of how I see this."
    },
    {
      "id": 269,
      "start": 1193.58,
      "end": 1198.46,
      "text": "I think that what they are doing is trying to buy something that cannot be bought."
    },
    {
      "id": 270,
      "start": 1198.96,
      "end": 1201.22,
      "text": "And that is alignment with the mission."
    },
    {
      "id": 271,
      "start": 1202.34,
      "end": 1205.9399999999998,
      "text": "And, you know, I, you know, I, you know, I think there's selection effects here."
    },
    {
      "id": 272,
      "start": 1206.08,
      "end": 1213.76,
      "text": "Like, you know, I, you know, are they getting the people who are most enthusiastic or most mission aligned, who are most excited to..."
    },
    {
      "id": 273,
      "start": 1213.76,
      "end": 1215.32,
      "text": "But they have talent and GPUs."
    },
    {
      "id": 274,
      "start": 1215.36,
      "end": 1216.32,
      "text": "You're not underestimating them?"
    },
    {
      "id": 275,
      "start": 1216.82,
      "end": 1220.62,
      "text": "I will see how it will see how it plays out."
    },
    {
      "id": 276,
      "start": 1221.4199999999998,
      "end": 1223.5,
      "text": "I am pretty bearish on what they're trying to do."
    },
    {
      "id": 277,
      "start": 1224.1999999999998,
      "end": 1230.56,
      "text": "So let's talk a little bit about your business, because a lot of people have been wondering, is the business of Generative AI a real thing?"
    },
    {
      "id": 278,
      "start": 1230.62,
      "end": 1231.5,
      "text": "And I'm also curious."
    },
    {
      "id": 279,
      "start": 1231.6,
      "end": 1232.54,
      "text": "I have questions all the time."
    },
    {
      "id": 280,
      "start": 1232.96,
      "end": 1235.48,
      "text": "You talked about how much money you've raised, close to $20 billion."
    },
    {
      "id": 281,
      "start": 1236.34,
      "end": 1245.52,
      "text": "You've raised $3 billion from Google, $8 billion from Amazon, $3.5 billion from a new round led by Lightspeed, who I've spoken with."
    },
    {
      "id": 282,
      "start": 1246.56,
      "end": 1247.9199999999998,
      "text": "What is your pitch?"
    },
    {
      "id": 283,
      "start": 1248.0,
      "end": 1250.74,
      "text": "Because you are not part of like a big tech company."
    },
    {
      "id": 284,
      "start": 1250.8999999999999,
      "end": 1252.3999999999999,
      "text": "You're out there on your own."
    },
    {
      "id": 285,
      "start": 1252.4,
      "end": 1255.0400000000002,
      "text": "And do you just bring the scaling laws and say, can I have some money?"
    },
    {
      "id": 286,
      "start": 1255.0400000000002,
      "end": 1262.6000000000001,
      "text": "So my view of this has always been that talent is the most important thing."
    },
    {
      "id": 287,
      "start": 1262.6,
      "end": 1271.06,
      "text": "So if you go back three years ago, we were in a position where we had raised mere hundreds of millions."
    },
    {
      "id": 288,
      "start": 1271.6999999999998,
      "end": 1275.34,
      "text": "Open AI had already raised $13 billion from Microsoft."
    },
    {
      "id": 289,
      "start": 1275.6,
      "end": 1281.6799999999998,
      "text": "And of course, the large hyper-cap tech companies were sitting on $100 billion, $200 billion."
    },
    {
      "id": 290,
      "start": 1281.68,
      "end": 1288.38,
      "text": "And basically, the pitch we made then is we know how to make these models better than others do, right?"
    },
    {
      "id": 291,
      "start": 1288.44,
      "end": 1289.46,
      "text": "There may be a curve."
    },
    {
      "id": 292,
      "start": 1289.6000000000001,
      "end": 1291.4,
      "text": "There may be a curve of scaling laws."
    },
    {
      "id": 293,
      "start": 1291.4,
      "end": 1307.22,
      "text": "But look, if we are in a position where we can do for $100 million what others can do for $1 billion, and we could do for $10 billion what they can do for $100 billion, then it's 10 times more capital efficient to invest in Anthropic than it is to invest in these other companies."
    },
    {
      "id": 294,
      "start": 1307.22,
      "end": 1313.64,
      "text": "Would you rather be in a position where you can do anything for 10 times cheaper or where you start with a large pile of money?"
    },
    {
      "id": 295,
      "start": 1313.92,
      "end": 1321.9,
      "text": "If you can do things 10 times cheaper, the money is a temporary defect that you can remedy."
    },
    {
      "id": 296,
      "start": 1322.02,
      "end": 1333.84,
      "text": "If you have this intrinsic ability to build things for the same price much better than anyone else or as good as anyone else for much lower price, investors aren't idiots, or at least they aren't always idiots."
    },
    {
      "id": 297,
      "start": 1334.1200000000001,
      "end": 1335.1200000000001,
      "text": "It depends which one you go to."
    },
    {
      "id": 298,
      "start": 1335.12,
      "end": 1336.82,
      "text": "I'm not going to name any names."
    },
    {
      "id": 299,
      "start": 1338.2399999999998,
      "end": 1343.7399999999998,
      "text": "But they basically understand the concept of capital efficiency."
    },
    {
      "id": 300,
      "start": 1344.4799999999998,
      "end": 1349.26,
      "text": "And so we've been in a position three years ago where these differences were like 1,000x."
    },
    {
      "id": 301,
      "start": 1349.3,
      "end": 1353.7199999999998,
      "text": "And now you're saying with $20 billion, can you compete with $100 billion?"
    },
    {
      "id": 302,
      "start": 1354.6399999999999,
      "end": 1358.12,
      "text": "And my answer is basically yes, because of the talent density."
    },
    {
      "id": 303,
      "start": 1358.12,
      "end": 1367.2199999999998,
      "text": "You know, I've said this before, but, you know, the Anthropic is actually the fastest growing software company in history at the scale that it's at."
    },
    {
      "id": 304,
      "start": 1367.22,
      "end": 1374.2,
      "text": "So we grew from zero to 100 million in 2023, 100 million to billion in 2024."
    },
    {
      "id": 305,
      "start": 1374.6200000000001,
      "end": 1379.26,
      "text": "And this year we've grown from 1 billion to, I think I said this before, 4.5."
    },
    {
      "id": 306,
      "start": 1379.26,
      "end": 1387.28,
      "text": "So that 10x a year, I mean, you know, every year I, like, suspect that we'll grow at that scale."
    },
    {
      "id": 307,
      "start": 1387.52,
      "end": 1393.2,
      "text": "And every year I'm almost afraid to say it publicly because I'm like, no, it couldn't possibly happen again."
    },
    {
      "id": 308,
      "start": 1393.32,
      "end": 1399.44,
      "text": "So, you know, I think the growth at that scale, like, kind of speaks for itself in terms of our ability to compete with the big players."
    },
    {
      "id": 309,
      "start": 1399.44,
      "end": 1404.76,
      "text": "Okay, so CNBC says 60 to 75% of Anthropic sales are through the API."
    },
    {
      "id": 310,
      "start": 1404.92,
      "end": 1406.8,
      "text": "That was according to internal documents."
    },
    {
      "id": 311,
      "start": 1407.3400000000001,
      "end": 1407.94,
      "text": "Is that still accurate?"
    },
    {
      "id": 312,
      "start": 1408.56,
      "end": 1415.26,
      "text": "I won't give exact numbers, but the majority does come through the API, although we also do have a flourishing apps business."
    },
    {
      "id": 313,
      "start": 1415.64,
      "end": 1423.48,
      "text": "And, you know, I think more recently the, you know, Max Tier, which Power users use, as well as Cloud Code, which coders use."
    },
    {
      "id": 314,
      "start": 1423.48,
      "end": 1429.2,
      "text": "So, you know, I think we have a thriving and fast-growing apps business, but, yes, the majority comes through the API."
    },
    {
      "id": 315,
      "start": 1429.2,
      "end": 1431.8,
      "text": "So you're making the most pure bet on this technology."
    },
    {
      "id": 316,
      "start": 1432.06,
      "end": 1440.9,
      "text": "Like, you know, OpenAI might be betting on ChatGPT and Google might be betting on the fact that no matter where the technology goes, it can, you know, integrate into Gmail and Calendar."
    },
    {
      "id": 317,
      "start": 1441.14,
      "end": 1444.5800000000002,
      "text": "So why have you made this bet on this, the pure bet on the tech itself?"
    },
    {
      "id": 318,
      "start": 1444.94,
      "end": 1447.68,
      "text": "Yeah, I mean, I would say, I wouldn't quite put it that way."
    },
    {
      "id": 319,
      "start": 1447.68,
      "end": 1455.38,
      "text": "I think we've, I would describe it more as we've bet on business use cases of the model, more so than we've bet on the API per se."
    },
    {
      "id": 320,
      "start": 1455.64,
      "end": 1459.1200000000001,
      "text": "And it's just that the first business use cases of the model come through the API."
    },
    {
      "id": 321,
      "start": 1459.64,
      "end": 1462.92,
      "text": "So, you know, as you mentioned, OpenAI is very focused on the consumer side."
    },
    {
      "id": 322,
      "start": 1463.3400000000001,
      "end": 1466.8400000000001,
      "text": "Google is very focused on kind of the existing products that Google has."
    },
    {
      "id": 323,
      "start": 1467.24,
      "end": 1475.48,
      "text": "Our view is that, if anything, the enterprise use of AI is going to be greater even than the consumer use of AI."
    },
    {
      "id": 324,
      "start": 1475.48,
      "end": 1484.3,
      "text": "I should say the business use because it's enterprise, it's startups, it's developers, and it's kind of, you know, power users using the model for productivity."
    },
    {
      "id": 325,
      "start": 1484.3,
      "end": 1495.3799999999999,
      "text": "I also think that being a company that's focused on the business use cases actually gives us better incentives to make the models better."
    },
    {
      "id": 326,
      "start": 1495.38,
      "end": 1506.5800000000002,
      "text": "A thought experiment that I think is worth running is, you know, suppose I have this model and it's, you know, it's as good as an undergrad at biochemistry."
    },
    {
      "id": 327,
      "start": 1506.58,
      "end": 1536.58,
      "text": ""
    },
    {
      "id": 328,
      "start": 1536.58,
      "end": 1537.72,
      "text": "The biggest deal in the world, right?"
    },
    {
      "id": 329,
      "start": 1537.72,
      "end": 1540.34,
      "text": "They might pay 10 times more for something like that."
    },
    {
      "id": 330,
      "start": 1540.3799999999999,
      "end": 1542.72,
      "text": "It might have 10 times more value to them."
    },
    {
      "id": 331,
      "start": 1542.72,
      "end": 1553.54,
      "text": "And so the general aim of making the models solve the problems of the world to make them smarter and smarter, but also able to bring many of the positive applications, right?"
    },
    {
      "id": 332,
      "start": 1553.54,
      "end": 1571.8999999999999,
      "text": "The things I wrote about in, like, machines of loving grace of, like, solving the problems of biomedicine, solving the problems of geopolitics, solving the problems of, you know, of economic development, you know, as well as more prosaic things like finance or legal or productivity or insurance."
    },
    {
      "id": 333,
      "start": 1571.9,
      "end": 1582.72,
      "text": "I think it gives a better incentive to kind of develop the models as far as possible."
    },
    {
      "id": 334,
      "start": 1583.1000000000001,
      "end": 1587.22,
      "text": "And I think in many ways it's like it may even be a more positive business."
    },
    {
      "id": 335,
      "start": 1587.38,
      "end": 1593.52,
      "text": "So I would say we're making a bet on the business use of AI because it's most aligned with kind of the exponential."
    },
    {
      "id": 336,
      "start": 1593.7800000000002,
      "end": 1594.18,
      "text": "Okay."
    },
    {
      "id": 337,
      "start": 1594.24,
      "end": 1596.92,
      "text": "Then briefly, how did you decide to go with the coding use case?"
    },
    {
      "id": 338,
      "start": 1596.92,
      "end": 1597.3600000000001,
      "text": "Yes."
    },
    {
      "id": 339,
      "start": 1597.8400000000001,
      "end": 1605.74,
      "text": "So, you know, originally, as happens with most things, we're trying to optimize for making the model better at a bunch of stuff."
    },
    {
      "id": 340,
      "start": 1606.24,
      "end": 1610.02,
      "text": "And, you know, coding particularly stood out in terms of how valuable it was."
    },
    {
      "id": 341,
      "start": 1610.1200000000001,
      "end": 1622.26,
      "text": "You know, I've worked with thousands of engineers, and there was a point about a year, a year and a half ago where one of the best I'd ever worked with said, you know, every previous coding model has been useless to me."
    },
    {
      "id": 342,
      "start": 1622.38,
      "end": 1625.66,
      "text": "And this one finally was able to do something I wasn't able to do."
    },
    {
      "id": 343,
      "start": 1625.66,
      "end": 1628.46,
      "text": "And then after we released it, it started getting quick adoption."
    },
    {
      "id": 344,
      "start": 1628.66,
      "end": 1637.6200000000001,
      "text": "This was around the time that, you know, a lot of the coding companies like Cursor, Windsurf, GitHub, Augment Code started exploding in popularity."
    },
    {
      "id": 345,
      "start": 1637.98,
      "end": 1641.48,
      "text": "And then when we saw how popular it was, we kind of doubled down on it."
    },
    {
      "id": 346,
      "start": 1642.48,
      "end": 1648.28,
      "text": "My view is that coding is particularly interesting because, A, the adoption is fast."
    },
    {
      "id": 347,
      "start": 1648.28,
      "end": 1654.54,
      "text": "And, B, getting better at coding with the models actually helps you to develop the next model."
    },
    {
      "id": 348,
      "start": 1654.96,
      "end": 1658.6399999999999,
      "text": "So it has a number of, you know, I would say advantages."
    },
    {
      "id": 349,
      "start": 1659.2,
      "end": 1663.68,
      "text": "And now you're selling your AI coding through Claude Code."
    },
    {
      "id": 350,
      "start": 1663.68,
      "end": 1665.22,
      "text": "But it's very interesting."
    },
    {
      "id": 351,
      "start": 1665.3200000000002,
      "end": 1668.74,
      "text": "The pricing model has been confounding to some."
    },
    {
      "id": 352,
      "start": 1668.88,
      "end": 1671.88,
      "text": "You can spend $200 a month and get the equivalent."
    },
    {
      "id": 353,
      "start": 1671.94,
      "end": 1672.9,
      "text": "I spoke to one developer."
    },
    {
      "id": 354,
      "start": 1673.26,
      "end": 1677.0800000000002,
      "text": "They got the equivalent of $6,000 a month from your API."
    },
    {
      "id": 355,
      "start": 1678.3,
      "end": 1686.54,
      "text": "Ed Zitron has pointed out the more popular that your models get, the more money you're going to lose if people are super users of this technology."
    },
    {
      "id": 356,
      "start": 1686.66,
      "end": 1687.52,
      "text": "So how does that make sense?"
    },
    {
      "id": 357,
      "start": 1687.52,
      "end": 1692.92,
      "text": "So actually, pricing schemes and rate limits are surprisingly complicated."
    },
    {
      "id": 358,
      "start": 1693.82,
      "end": 1710.34,
      "text": "So some of this is basically the result of when we released our â€“ when we released Claude Code in the max tier, which we eventually tied together, actually not fully understanding the implications of, you know, the ways in which people could use the models and how much they were actually able to get."
    },
    {
      "id": 359,
      "start": 1710.34,
      "end": 1718.24,
      "text": "So over the last few days, as of the time of this interview, we've adjusted that, particularly on the larger models like Opus."
    },
    {
      "id": 360,
      "start": 1718.4199999999998,
      "end": 1725.9199999999998,
      "text": "I think it's no longer possible to spend that much with a $200 subscription."
    },
    {
      "id": 361,
      "start": 1726.28,
      "end": 1729.82,
      "text": "And, you know, it's possible more changes will come in the future."
    },
    {
      "id": 362,
      "start": 1730.02,
      "end": 1736.9599999999998,
      "text": "But we're always going to have a distribution of users who use a lot and users who use some amount."
    },
    {
      "id": 363,
      "start": 1736.96,
      "end": 1755.8,
      "text": "And it doesn't necessarily mean we're losing money, that there are some users who get more, you know, who, if you were to measure via API credits, spend, you know, get a better deal on the consumer subscription than they would on the API products."
    },
    {
      "id": 364,
      "start": 1755.8400000000001,
      "end": 1755.96,
      "text": "Right?"
    },
    {
      "id": 365,
      "start": 1756.0,
      "end": 1757.54,
      "text": "There's a lot of assumptions there."
    },
    {
      "id": 366,
      "start": 1757.54,
      "end": 1761.42,
      "text": "And I can tell you that some of them are wrong."
    },
    {
      "id": 367,
      "start": 1762.32,
      "end": 1764.5,
      "text": "We are not, in fact, losing money."
    },
    {
      "id": 368,
      "start": 1764.7,
      "end": 1771.1399999999999,
      "text": "But I guess there's another question about whether you can continue to serve these use cases and not raise prices."
    },
    {
      "id": 369,
      "start": 1771.14,
      "end": 1780.1200000000001,
      "text": "So just to give you a couple stats, there are some developers that are upset because using Anthropics' newer models and cursors is costing them more than it ever has."
    },
    {
      "id": 370,
      "start": 1781.0400000000002,
      "end": 1787.24,
      "text": "Startups that I've spoken with say Anthropic is down a bunch because they can't get access to the GPUs."
    },
    {
      "id": 371,
      "start": 1787.3000000000002,
      "end": 1791.0200000000002,
      "text": "At least that's what they imagine is happening."
    },
    {
      "id": 372,
      "start": 1791.02,
      "end": 1801.84,
      "text": "And I was just with Amjad Massad at Replit in an interview that we're going to air next week who said there was a period of time where the price per token, price to use these models was coming down."
    },
    {
      "id": 373,
      "start": 1802.22,
      "end": 1803.46,
      "text": "And it stopped coming down."
    },
    {
      "id": 374,
      "start": 1804.0,
      "end": 1812.0,
      "text": "So is it, is what's happening that these models are just so expensive for Anthropic to run that it's hitting a wall of its own?"
    },
    {
      "id": 375,
      "start": 1812.1399999999999,
      "end": 1814.32,
      "text": "Again, I think you're making assumptions here."
    },
    {
      "id": 376,
      "start": 1814.34,
      "end": 1815.34,
      "text": "That's why I'm asking the CEO."
    },
    {
      "id": 377,
      "start": 1815.34,
      "end": 1824.74,
      "text": "Yeah, you know, you know, the way I think about it is we think about the models in terms of how much value are they creating, right?"
    },
    {
      "id": 378,
      "start": 1824.9199999999998,
      "end": 1828.76,
      "text": "So as the models get better and better, I think about how much value they create."
    },
    {
      "id": 379,
      "start": 1828.82,
      "end": 1840.1999999999998,
      "text": "And there's a separate question about how the value is distributed between those who make the model, those who make the chips, and, you know, those who make the underlying applications."
    },
    {
      "id": 380,
      "start": 1840.2,
      "end": 1846.16,
      "text": "So again, without being too specific, like, I think there are some assumptions in your question that are not necessarily correct."
    },
    {
      "id": 381,
      "start": 1847.74,
      "end": 1848.2,
      "text": "You know, I..."
    },
    {
      "id": 382,
      "start": 1848.2,
      "end": 1849.14,
      "text": "Can you tell me which ones?"
    },
    {
      "id": 383,
      "start": 1849.42,
      "end": 1850.98,
      "text": "I, so I'll say this."
    },
    {
      "id": 384,
      "start": 1851.16,
      "end": 1860.54,
      "text": "I do expect the, I expect the price of providing a given level of intelligence to go down."
    },
    {
      "id": 385,
      "start": 1860.54,
      "end": 1870.78,
      "text": "I expect the price of providing the frontier of intelligence, which will, which will provide kind of increasing economic value, that might go up or it might go down."
    },
    {
      "id": 386,
      "start": 1870.84,
      "end": 1873.04,
      "text": "My guess is it probably stays about where it is."
    },
    {
      "id": 387,
      "start": 1873.04,
      "end": 1876.46,
      "text": "But again, the value that's created, you know, goes way up."
    },
    {
      "id": 388,
      "start": 1876.58,
      "end": 1889.98,
      "text": "So two years from now, my guess is that we'll have models that cost of the same order of magnitude that they cost today, except they'll be much more capable of doing work much more autonomously, much more broadly than they are capable of today."
    },
    {
      "id": 389,
      "start": 1889.98,
      "end": 1904.52,
      "text": "One of the things that Amjad mentioned was he thinks that the bigger models are not as intensive to run or more intensive to run, given their size, because of the architecture and some of these techniques that we talked about, that they're lighting up only certain sections of the model."
    },
    {
      "id": 390,
      "start": 1905.14,
      "end": 1917.4,
      "text": "So his idea, I'm hopefully conveying this truthfully, is that Anthropic can run these models without too much bulk on the back end, but is still keeping those prices where they are."
    },
    {
      "id": 391,
      "start": 1917.4,
      "end": 1923.7,
      "text": "And I think the line that I'm going to draw there is maybe that to get to software margins."
    },
    {
      "id": 392,
      "start": 1923.8600000000001,
      "end": 1927.7,
      "text": "There were some reports that Anthropic is slightly below software gross margins."
    },
    {
      "id": 393,
      "start": 1927.8600000000001,
      "end": 1930.52,
      "text": "You're going to have to charge a little bit more for these models."
    },
    {
      "id": 394,
      "start": 1931.4,
      "end": 1937.8200000000002,
      "text": "So, yeah, again, I think larger models cost more to run than smaller models."
    },
    {
      "id": 395,
      "start": 1937.82,
      "end": 1938.22,
      "text": "Okay."
    },
    {
      "id": 396,
      "start": 1939.22,
      "end": 1944.26,
      "text": "You know, I think the technique you're referring to is maybe mixture of experts or something like that."
    },
    {
      "id": 397,
      "start": 1944.5,
      "end": 1951.12,
      "text": "So whether your models are mixture of experts or not, like mixture of experts is like a way to run models more cheaply."
    },
    {
      "id": 398,
      "start": 1951.1799999999998,
      "end": 1952.34,
      "text": "They have a given number of parameters."
    },
    {
      "id": 399,
      "start": 1952.54,
      "end": 1953.6599999999999,
      "text": "It's a way to train models."
    },
    {
      "id": 400,
      "start": 1953.66,
      "end": 1960.98,
      "text": "But if you're not using that technique, then larger models that don't use that technique cost more to run than smaller models that don't use that technique."
    },
    {
      "id": 401,
      "start": 1961.0600000000002,
      "end": 1968.48,
      "text": "And if you're using that technique, larger models that use that technique cost more to run than smaller models that are using that technique."
    },
    {
      "id": 402,
      "start": 1968.5800000000002,
      "end": 1973.28,
      "text": "So I think that's sort of a distortion of the situation."
    },
    {
      "id": 403,
      "start": 1974.42,
      "end": 1977.44,
      "text": "Basically, I'm just guessing and I'm trying to find out what the truth is from you."
    },
    {
      "id": 404,
      "start": 1977.44,
      "end": 1994.18,
      "text": "Yeah, look, so I, you know, in terms of like the cost of the models, like one thing you'd be surprised by, people, you know, people kind of impute this thing to like, oh, man, it's going to be really hard to get the margins from like X percent to Y percent."
    },
    {
      "id": 405,
      "start": 1994.68,
      "end": 1999.68,
      "text": "We make improvements all the time that make the models like 50 percent more efficient than they are before."
    },
    {
      "id": 406,
      "start": 1999.8400000000001,
      "end": 2002.48,
      "text": "We are just at the beginning of optimizing inference."
    },
    {
      "id": 407,
      "start": 2002.48,
      "end": 2009.68,
      "text": "Inference has improved a huge amount where from from where it was a couple a couple years ago to where it is now."
    },
    {
      "id": 408,
      "start": 2009.8,
      "end": 2011.32,
      "text": "That's why the prices are coming down."
    },
    {
      "id": 409,
      "start": 2012.1,
      "end": 2014.6,
      "text": "And then how long is it going to take to be profitable?"
    },
    {
      "id": 410,
      "start": 2014.6,
      "end": 2017.6,
      "text": "Because I think the loss is going to be like three billion this year."
    },
    {
      "id": 411,
      "start": 2017.82,
      "end": 2020.66,
      "text": "That's what I would distinguish different things."
    },
    {
      "id": 412,
      "start": 2021.42,
      "end": 2024.06,
      "text": "There's the cost of running."
    },
    {
      "id": 413,
      "start": 2024.22,
      "end": 2026.28,
      "text": "There's the cost of running the model."
    },
    {
      "id": 414,
      "start": 2026.3600000000001,
      "end": 2026.52,
      "text": "Right."
    },
    {
      "id": 415,
      "start": 2026.58,
      "end": 2029.92,
      "text": "So so for every dollar the model makes, it costs a certain amount."
    },
    {
      "id": 416,
      "start": 2029.92,
      "end": 2034.8200000000002,
      "text": "That is actually already fairly profitable."
    },
    {
      "id": 417,
      "start": 2035.78,
      "end": 2037.16,
      "text": "There are separate things."
    },
    {
      "id": 418,
      "start": 2037.3000000000002,
      "end": 2040.76,
      "text": "There's, you know, the cost of paying people and like buildings."
    },
    {
      "id": 419,
      "start": 2040.76,
      "end": 2043.94,
      "text": "That is actually not that that large in the scheme of things."
    },
    {
      "id": 420,
      "start": 2044.38,
      "end": 2046.8600000000001,
      "text": "The big cost is the cost of training the next model."
    },
    {
      "id": 421,
      "start": 2047.5800000000002,
      "end": 2053.1,
      "text": "And I think this idea of like the companies losing money and not being profitable is it's a little bit misleading."
    },
    {
      "id": 422,
      "start": 2053.4,
      "end": 2057.0,
      "text": "And you start to understand it better when you when you look at the scaling laws."
    },
    {
      "id": 423,
      "start": 2057.0,
      "end": 2062.02,
      "text": "So as a thought exercise, these numbers are not exact or even close for Anthropic."
    },
    {
      "id": 424,
      "start": 2062.54,
      "end": 2066.52,
      "text": "Let's imagine that in 2023, you train a model that costs 100 million dollars."
    },
    {
      "id": 425,
      "start": 2067.3,
      "end": 2072.56,
      "text": "And then in 2024, you deploy the 2023 model and it makes 200 million dollars in revenue."
    },
    {
      "id": 426,
      "start": 2072.56,
      "end": 2077.46,
      "text": "But you spend a billion dollars to train, you know, to train a new model in 2024."
    },
    {
      "id": 427,
      "start": 2077.46,
      "end": 2084.96,
      "text": "So and then, you know, and then in 2025, the billion dollar model makes two billion in revenue and you spend 10 billion to train the next model."
    },
    {
      "id": 428,
      "start": 2085.56,
      "end": 2088.1,
      "text": "So the company every year is unprofitable."
    },
    {
      "id": 429,
      "start": 2088.18,
      "end": 2093.16,
      "text": "It lost 800 million in 2024 and then 2025, it lost eight billion dollars."
    },
    {
      "id": 430,
      "start": 2093.7400000000002,
      "end": 2096.84,
      "text": "So, you know, this looks like a hugely unprofitable enterprise."
    },
    {
      "id": 431,
      "start": 2096.84,
      "end": 2100.48,
      "text": "But if instead I think in terms of is each model profitable, right?"
    },
    {
      "id": 432,
      "start": 2100.54,
      "end": 2102.2400000000002,
      "text": "Think of each model as a venture."
    },
    {
      "id": 433,
      "start": 2102.7400000000002,
      "end": 2108.52,
      "text": "I invested 100 million in the model and then I got that I got that I got 200 million out of the model in the next year."
    },
    {
      "id": 434,
      "start": 2108.6000000000004,
      "end": 2115.6000000000004,
      "text": "So that model had 50 percent margins and, you know, and like and like made me 100 100 million dollars the next year."
    },
    {
      "id": 435,
      "start": 2116.2200000000003,
      "end": 2123.42,
      "text": "You know, the the the company invested a billion dollars and made and made two billion dollars in or sorry, the next model."
    },
    {
      "id": 436,
      "start": 2123.42,
      "end": 2125.58,
      "text": "The company invested a billion dollars and made."
    },
    {
      "id": 437,
      "start": 2125.82,
      "end": 2127.36,
      "text": "So every model is profitable."
    },
    {
      "id": 438,
      "start": 2127.6,
      "end": 2127.96,
      "text": "They're all profitable."
    },
    {
      "id": 439,
      "start": 2127.96,
      "end": 2129.82,
      "text": "But the company is unprofitable every year."
    },
    {
      "id": 440,
      "start": 2129.96,
      "end": 2130.7000000000003,
      "text": "I'm not I'm not."
    },
    {
      "id": 441,
      "start": 2130.8,
      "end": 2130.9,
      "text": "Right."
    },
    {
      "id": 442,
      "start": 2130.96,
      "end": 2132.9,
      "text": "This is this is this is a style."
    },
    {
      "id": 443,
      "start": 2133.02,
      "end": 2136.7400000000002,
      "text": "I'm not like claiming these numbers for Anthropic or claiming these facts for Anthropic."
    },
    {
      "id": 444,
      "start": 2136.7400000000002,
      "end": 2138.02,
      "text": "But this is a general for Anthropic."
    },
    {
      "id": 445,
      "start": 2138.02,
      "end": 2146.12,
      "text": "This general dynamic is is this this general dynamic is in general terms the explanation for what is going on."
    },
    {
      "id": 446,
      "start": 2146.28,
      "end": 2153.4,
      "text": "And so, you know, you know, at any time, if the models stopped getting better or if a company,"
    },
    {
      "id": 447,
      "start": 2153.42,
      "end": 2161.82,
      "text": "stopped investing in the next model, you know, you would you know, you would have probably a viable business with the existing models."
    },
    {
      "id": 448,
      "start": 2161.82,
      "end": 2163.92,
      "text": "But everyone is investing in the next model."
    },
    {
      "id": 449,
      "start": 2163.92,
      "end": 2164.3,
      "text": "Yeah."
    },
    {
      "id": 450,
      "start": 2164.3,
      "end": 2167.28,
      "text": "And so eventually it'll get it'll get to some scale."
    },
    {
      "id": 451,
      "start": 2167.28,
      "end": 2179.2200000000003,
      "text": "But the fact that we're spending more to this fact that we're spending more to invest in the next model suggests that the scale of the business is going to be larger the next year than it was than it was the year before."
    },
    {
      "id": 452,
      "start": 2179.22,
      "end": 2183.1,
      "text": "Now, of course, what could happen is like the model stopped getting better."
    },
    {
      "id": 453,
      "start": 2183.1,
      "end": 2186.04,
      "text": "And there's this kind of one time cost that's like a boondoggle."
    },
    {
      "id": 454,
      "start": 2186.04,
      "end": 2187.7999999999997,
      "text": "And we spend we spend a bunch of money."
    },
    {
      "id": 455,
      "start": 2187.7999999999997,
      "end": 2198.58,
      "text": "But then the the you know, the the companies, the industry will kind of return to this, you know, to this plateau, to this level of profitability or the exponential can keep going."
    },
    {
      "id": 456,
      "start": 2199.12,
      "end": 2203.3799999999997,
      "text": "So I think I think that's a long winded way to say I don't think it's really the right way to think about things."
    },
    {
      "id": 457,
      "start": 2203.38,
      "end": 2212.9,
      "text": "Right. But what about open source? Because if you stopped, let's say you stopped investing in the models and open source caught up, then people could swap in open source."
    },
    {
      "id": 458,
      "start": 2213.04,
      "end": 2224.76,
      "text": "Now, I'd love to hear your perspective on this, because one of the things people have talked to me about when it comes to the anthropic business is there is that risk eventually that open source gets good enough that you can take anthropic out and put open source in."
    },
    {
      "id": 459,
      "start": 2224.76,
      "end": 2237.94,
      "text": "Yeah. So, you know, people have you know, I think one of the things that's been true of this industry is that and, you know, I saw it early in I saw it early in the history of A.I."
    },
    {
      "id": 460,
      "start": 2237.94,
      "end": 2244.82,
      "text": "Every community that that A.I. has gone through, it has this set of heuristics about how things work."
    },
    {
      "id": 461,
      "start": 2244.9,
      "end": 2255.46,
      "text": "Like back when I was in, you know, A.I. back in 2014, there was an existing kind of A.I. and machine learning research community that like thought about things in a certain way."
    },
    {
      "id": 462,
      "start": 2255.48,
      "end": 2260.04,
      "text": "And we're like, this is just a fad. This is a new thing. This can't work. This can't scale."
    },
    {
      "id": 463,
      "start": 2260.26,
      "end": 2263.8,
      "text": "And then because of the exponential, all those things turned out to be false."
    },
    {
      "id": 464,
      "start": 2263.8,
      "end": 2269.7000000000003,
      "text": "Then a similar thing happened with kind of like people deploying A.I. within companies to various applications."
    },
    {
      "id": 465,
      "start": 2270.02,
      "end": 2272.42,
      "text": "Then there was the same thought in the startup ecosystem."
    },
    {
      "id": 466,
      "start": 2273.1400000000003,
      "end": 2289.6200000000003,
      "text": "And I think now we're at the phase where kind of the world's business leaders, like the investors and the business, they have this whole lexicon of commoditization, you know, modes, which layer is the value going to, you know, which layer is the value going to accrue to."
    },
    {
      "id": 467,
      "start": 2289.62,
      "end": 2302.7599999999998,
      "text": "And open source is this idea that you can kind of see everything that's going on, you know, that it has a significance, that it kind of undermines the fact that, you know, the idea that it undermines business."
    },
    {
      "id": 468,
      "start": 2302.94,
      "end": 2318.8599999999997,
      "text": "And I actually find as someone who didn't come from that world at all, who never thought in terms of that lexicon, this is one of these situations where not knowing anything often leads you to make better predictions than kind of the people who have their way of thinking about things from the last generation of tech."
    },
    {
      "id": 469,
      "start": 2319.62,
      "end": 2330.2799999999997,
      "text": "And, you know, this is all, I think, a long-winded way of saying I don't think open source works the same way in A.I. that it has worked in other areas."
    },
    {
      "id": 470,
      "start": 2331.02,
      "end": 2337.04,
      "text": "Primarily because with open source, you can see the, you know, you can see the source code of the model."
    },
    {
      "id": 471,
      "start": 2337.18,
      "end": 2338.96,
      "text": "Here, we can't see inside the model."
    },
    {
      "id": 472,
      "start": 2338.96,
      "end": 2343.7400000000002,
      "text": "You know, it's often called open weights instead of open source to kind of distinguish that."
    },
    {
      "id": 473,
      "start": 2344.14,
      "end": 2351.34,
      "text": "But a lot of the benefits, which is that many people can work on it, that it's kind of additive, it doesn't quite work in the same way."
    },
    {
      "id": 474,
      "start": 2352.1,
      "end": 2354.96,
      "text": "So, you know, I've actually always seen it as a red herring."
    },
    {
      "id": 475,
      "start": 2355.02,
      "end": 2359.08,
      "text": "When I see it, when I see a new model come out, I don't care whether it's open source or not."
    },
    {
      "id": 476,
      "start": 2359.08,
      "end": 2363.0,
      "text": "Like if we talk about DeepSeek, I don't think it mattered that DeepSeek is open source."
    },
    {
      "id": 477,
      "start": 2363.4,
      "end": 2365.14,
      "text": "I think I ask, is it a good model?"
    },
    {
      "id": 478,
      "start": 2365.44,
      "end": 2370.5,
      "text": "Is it better than us at, you know, the things that we, that's the only thing that I care about."
    },
    {
      "id": 479,
      "start": 2370.84,
      "end": 2373.98,
      "text": "It actually doesn't matter either way."
    },
    {
      "id": 480,
      "start": 2374.2599999999998,
      "end": 2377.1,
      "text": "Because ultimately, you have to host it on the cloud."
    },
    {
      "id": 481,
      "start": 2377.38,
      "end": 2379.12,
      "text": "The people who host it on the cloud do inference."
    },
    {
      "id": 482,
      "start": 2379.46,
      "end": 2380.36,
      "text": "These are big models."
    },
    {
      "id": 483,
      "start": 2380.48,
      "end": 2382.0,
      "text": "They're hard to do inference on."
    },
    {
      "id": 484,
      "start": 2382.0,
      "end": 2392.12,
      "text": "And conversely, many of the things that you can do when you see the weights, you know, we're increasingly offering on clouds where you can fine tune the model."
    },
    {
      "id": 485,
      "start": 2392.34,
      "end": 2401.76,
      "text": "You can, you know, you know, we're even looking at ways to, you know, to kind of, you know, investigate the activations of the model as part of like an interpretability interface."
    },
    {
      "id": 486,
      "start": 2402.02,
      "end": 2404.92,
      "text": "We did some little things around steering last time."
    },
    {
      "id": 487,
      "start": 2405.16,
      "end": 2407.9,
      "text": "So I think it's the wrong axis to think in terms of."
    },
    {
      "id": 488,
      "start": 2407.9,
      "end": 2413.42,
      "text": "When I think about competition, I think about like which models are good at the tasks that we do."
    },
    {
      "id": 489,
      "start": 2413.84,
      "end": 2415.92,
      "text": "I think open source is actually a red herring."
    },
    {
      "id": 490,
      "start": 2416.12,
      "end": 2417.7400000000002,
      "text": "But if it's free and cheap to run."
    },
    {
      "id": 491,
      "start": 2417.7400000000002,
      "end": 2418.3,
      "text": "It's not free."
    },
    {
      "id": 492,
      "start": 2418.44,
      "end": 2420.58,
      "text": "You have to, you have to, you have to, you have to run it on inference."
    },
    {
      "id": 493,
      "start": 2420.58,
      "end": 2423.14,
      "text": "And someone, someone has to make it fast on inference."
    },
    {
      "id": 494,
      "start": 2423.56,
      "end": 2423.82,
      "text": "All right."
    },
    {
      "id": 495,
      "start": 2423.98,
      "end": 2426.88,
      "text": "So I want to learn a little bit more about Dario the person."
    },
    {
      "id": 496,
      "start": 2427.14,
      "end": 2427.36,
      "text": "Yes."
    },
    {
      "id": 497,
      "start": 2427.44,
      "end": 2429.34,
      "text": "So we have a little bit of time left."
    },
    {
      "id": 498,
      "start": 2429.84,
      "end": 2434.02,
      "text": "So I have some questions for you about early life and then how you became who you are."
    },
    {
      "id": 499,
      "start": 2434.14,
      "end": 2434.4,
      "text": "Yes."
    },
    {
      "id": 500,
      "start": 2435.0,
      "end": 2437.6,
      "text": "So what was it like growing up in San Francisco?"
    },
    {
      "id": 501,
      "start": 2438.2400000000002,
      "end": 2438.48,
      "text": "Yeah."
    },
    {
      "id": 502,
      "start": 2439.08,
      "end": 2444.7200000000003,
      "text": "I, you know, the city, when I first grew up here, had not really, had not really gentrified that much."
    },
    {
      "id": 503,
      "start": 2444.78,
      "end": 2447.04,
      "text": "You know, when I grew up, the tech boom hadn't, hadn't happened."
    },
    {
      "id": 504,
      "start": 2447.54,
      "end": 2448.64,
      "text": "It hadn't happened yet."
    },
    {
      "id": 505,
      "start": 2449.14,
      "end": 2451.82,
      "text": "You know, it happened as, as I was going through high school."
    },
    {
      "id": 506,
      "start": 2451.82,
      "end": 2453.4,
      "text": "And actually I had no interest in it."
    },
    {
      "id": 507,
      "start": 2453.76,
      "end": 2455.92,
      "text": "It was totally, it was totally boring to me."
    },
    {
      "id": 508,
      "start": 2456.28,
      "end": 2458.6,
      "text": "You know, I was interested in being like a scientist."
    },
    {
      "id": 509,
      "start": 2458.6,
      "end": 2460.32,
      "text": "I was interested in physics and math."
    },
    {
      "id": 510,
      "start": 2460.32,
      "end": 2468.02,
      "text": "And, you know, the idea of like, you know, you know, like writing some website actually had no interest to me, to me whatsoever."
    },
    {
      "id": 511,
      "start": 2468.02,
      "end": 2469.36,
      "text": "Like founding a company."
    },
    {
      "id": 512,
      "start": 2469.36,
      "end": 2472.7000000000003,
      "text": "Like those weren't things that I was, that I was interested in at all."
    },
    {
      "id": 513,
      "start": 2472.7,
      "end": 2477.08,
      "text": "You know, I was interested in discovering fundamental scientific truth."
    },
    {
      "id": 514,
      "start": 2477.08,
      "end": 2481.4,
      "text": "And I was interested in like, you know, how can I, how can I do something that like makes the world better?"
    },
    {
      "id": 515,
      "start": 2482.44,
      "end": 2484.74,
      "text": "So, so, you know, that was, that was kind of more."
    },
    {
      "id": 516,
      "start": 2484.94,
      "end": 2492.08,
      "text": "And, you know, I watched the tech boom happen around me, but I, I feel like, you know, there was all kinds of things I probably could have learned from it that would have been helpful now."
    },
    {
      "id": 517,
      "start": 2492.08,
      "end": 2497.52,
      "text": "But I just actually wasn't paying attention and had no interest in it, even though I was like right at the center of it."
    },
    {
      "id": 518,
      "start": 2497.86,
      "end": 2501.38,
      "text": "So you're the son of a Jewish mother and an Italian father."
    },
    {
      "id": 519,
      "start": 2501.48,
      "end": 2501.96,
      "text": "That is true."
    },
    {
      "id": 520,
      "start": 2502.1,
      "end": 2504.58,
      "text": "From where I'm from in Long Island, we call that a pizza bagel."
    },
    {
      "id": 521,
      "start": 2504.8199999999997,
      "end": 2505.7,
      "text": "A pizza bagel."
    },
    {
      "id": 522,
      "start": 2505.7799999999997,
      "end": 2507.92,
      "text": "I've never, I've never heard that term before."
    },
    {
      "id": 523,
      "start": 2508.2599999999998,
      "end": 2510.36,
      "text": "So what was your relationship with your parents like?"
    },
    {
      "id": 524,
      "start": 2510.58,
      "end": 2510.88,
      "text": "Yeah."
    },
    {
      "id": 525,
      "start": 2510.98,
      "end": 2514.96,
      "text": "I mean, you know, I was, I was always, I was always, I was always pretty close with them."
    },
    {
      "id": 526,
      "start": 2514.96,
      "end": 2522.78,
      "text": "You know, I feel like they gave me a sense of, you know, of, of kind of right and wrong and what was important in the world."
    },
    {
      "id": 527,
      "start": 2522.88,
      "end": 2529.96,
      "text": "I feel like, you know, kind of imbuing a strong sense of responsibility is, is maybe the thing that I remember most."
    },
    {
      "id": 528,
      "start": 2530.6,
      "end": 2539.88,
      "text": "You know, they were always people who felt that sense of responsibility and, you know, wanted to, wanted to make the world, wanted to make the world better."
    },
    {
      "id": 529,
      "start": 2539.88,
      "end": 2544.48,
      "text": "And I feel like, you know, that's one of the, one of the main things that I, that I learned from them."
    },
    {
      "id": 530,
      "start": 2544.54,
      "end": 2548.82,
      "text": "You know, it was always a very, a very, a very loving family, a very caring family."
    },
    {
      "id": 531,
      "start": 2549.2200000000003,
      "end": 2554.44,
      "text": "I was very close with my sister, Daniela, who of course became my, became my, became my co-founder."
    },
    {
      "id": 532,
      "start": 2554.6400000000003,
      "end": 2559.38,
      "text": "And, you know, I think we decided very early that we wanted to work together in some, in some capacity."
    },
    {
      "id": 533,
      "start": 2559.5,
      "end": 2564.98,
      "text": "I don't know if we imagined that it would happen, you know, at quite the scale that, that, that, that it has happened."
    },
    {
      "id": 534,
      "start": 2564.98,
      "end": 2573.8,
      "text": "But it, you know, I think, I think it really, you know, that was, that was something we kind of decided early that we wanted to do."
    },
    {
      "id": 535,
      "start": 2574.22,
      "end": 2579.86,
      "text": "The people that I've spoken with that have known you through the years have told me that your father's illness had a big impact on you."
    },
    {
      "id": 536,
      "start": 2580.0,
      "end": 2581.22,
      "text": "Can you share a little bit about that?"
    },
    {
      "id": 537,
      "start": 2581.78,
      "end": 2582.18,
      "text": "Yes."
    },
    {
      "id": 538,
      "start": 2582.38,
      "end": 2582.68,
      "text": "Yes."
    },
    {
      "id": 539,
      "start": 2582.76,
      "end": 2592.62,
      "text": "He was, yeah, you know, he was ill for a long time and eventually died in, eventually died in, in, in 2006."
    },
    {
      "id": 540,
      "start": 2592.62,
      "end": 2606.4,
      "text": "Um, uh, so that, you know, that was actually one of the things that drove me to, you know, I, I don't think we mentioned it yet in this interview, but before, um, you know, before I went into AI, um, you know, I went into biology."
    },
    {
      "id": 541,
      "start": 2606.4,
      "end": 2613.68,
      "text": "So, you know, I'd gone to, uh, I, you know, I'd shown up at, I'd shown up at Princeton, um, uh, wanting to be a theoretical physicist."
    },
    {
      "id": 542,
      "start": 2613.68,
      "end": 2619.44,
      "text": "And, you know, I did some, did some work in, in cosmology for the first few, few months of my time there."
    },
    {
      "id": 543,
      "start": 2619.8999999999996,
      "end": 2623.5,
      "text": "Um, you know, and, and, and, you know, that was, that was around the time that, that my father died."
    },
    {
      "id": 544,
      "start": 2623.96,
      "end": 2638.2999999999997,
      "text": "And, you know, that did have an influence on me and kind of was one of the things that convinced me, um, you know, to, to go into biology, you know, to try and address, um, uh, uh, you know, human illnesses and biological problems."
    },
    {
      "id": 545,
      "start": 2638.3,
      "end": 2645.46,
      "text": "And so I started talking to some of the folks who worked on biophysics and computational neuroscience in the department that I was at at Princeton."
    },
    {
      "id": 546,
      "start": 2645.46,
      "end": 2650.32,
      "text": "And that was what led to the switch to biology and computational neuroscience."
    },
    {
      "id": 547,
      "start": 2650.32,
      "end": 2654.5800000000004,
      "text": "And then, you know, of course, after that, I eventually, I eventually went into AI."
    },
    {
      "id": 548,
      "start": 2654.58,
      "end": 2671.12,
      "text": "And the reason I went into AI was actually a continuation of that motivation, which is that, um, you know, as I spent many years in biology, I realized that the complexity of the underlying problems in biology felt like it was beyond human scale."
    },
    {
      "id": 549,
      "start": 2671.12,
      "end": 2676.44,
      "text": "You know, in order to understand it all, you needed hundreds, thousands of, you know, human researchers."
    },
    {
      "id": 550,
      "start": 2676.44,
      "end": 2695.42,
      "text": "And, you know, they often had a hard time collaborating or sharing their, you know, combining their, their knowledge and AI, which was, I was just starting to see the discoveries in it felt to me like the only technology that could kind of bridge that gap could bring us beyond human scale to, you know, to, to fully understand and solve the problems of biology."
    },
    {
      "id": 551,
      "start": 2695.42,
      "end": 2696.94,
      "text": "So, yeah, there is a through line there."
    },
    {
      "id": 552,
      "start": 2697.14,
      "end": 2697.2200000000003,
      "text": "Right."
    },
    {
      "id": 553,
      "start": 2697.32,
      "end": 2698.42,
      "text": "And I could have this wrong."
    },
    {
      "id": 554,
      "start": 2698.62,
      "end": 2704.82,
      "text": "Uh, but one thing I heard was that his illness was, um, largely uncurable when he had it."
    },
    {
      "id": 555,
      "start": 2704.82,
      "end": 2705.2200000000003,
      "text": "Yes."
    },
    {
      "id": 556,
      "start": 2705.22,
      "end": 2707.7799999999997,
      "text": "There have been advances that have been, can you share a little bit more?"
    },
    {
      "id": 557,
      "start": 2707.8599999999997,
      "end": 2708.12,
      "text": "Yes."
    },
    {
      "id": 558,
      "start": 2708.24,
      "end": 2710.74,
      "text": "There are advances that have made it much more manageable today."
    },
    {
      "id": 559,
      "start": 2711.06,
      "end": 2711.3799999999997,
      "text": "Yes."
    },
    {
      "id": 560,
      "start": 2711.54,
      "end": 2711.8599999999997,
      "text": "Yes."
    },
    {
      "id": 561,
      "start": 2712.0,
      "end": 2715.5,
      "text": "That is, uh, that is, uh, that is, that, that, that, that is true."
    },
    {
      "id": 562,
      "start": 2715.6,
      "end": 2730.3599999999997,
      "text": "Actually, actually only, um, uh, uh, only in the, um, uh, maybe three or four years after he died, the, the cure rate for the disease that he had went from, uh, went from 50% to, uh, uh, to roughly 95%."
    },
    {
      "id": 563,
      "start": 2730.3599999999997,
      "end": 2730.64,
      "text": "Yeah."
    },
    {
      "id": 564,
      "start": 2730.64,
      "end": 2735.06,
      "text": "I mean, it has to have felt so unjust to have your father taken away by something that could have been cured."
    },
    {
      "id": 565,
      "start": 2735.22,
      "end": 2737.12,
      "text": "It, it, of course, of course."
    },
    {
      "id": 566,
      "start": 2737.3399999999997,
      "end": 2743.1,
      "text": "Um, but it also tells you of the, the urgency of solving the relevant problems, right?"
    },
    {
      "id": 567,
      "start": 2743.1,
      "end": 2752.72,
      "text": "That, um, you know, that, that, that, you know, there, there was someone who worked on the cure to this disease that, you know, managed to cure it and save a bunch of people's lives."
    },
    {
      "id": 568,
      "start": 2752.72,
      "end": 2762.7799999999997,
      "text": "But, you know, could have, could have, um, saved even more people's lives if, if, you know, they, they had managed to, to find that, that, to find that cure, you know, a few years earlier than they did."
    },
    {
      "id": 569,
      "start": 2762.78,
      "end": 2765.76,
      "text": "Um, and I think that's, that's one of the tensions here, right?"
    },
    {
      "id": 570,
      "start": 2765.76,
      "end": 2769.26,
      "text": "That, you know, um, I think AI has all of these benefits."
    },
    {
      "id": 571,
      "start": 2769.26,
      "end": 2773.2000000000003,
      "text": "Um, and, you know, I want everyone to get those benefits as soon as possible."
    },
    {
      "id": 572,
      "start": 2773.52,
      "end": 2779.46,
      "text": "You know, I probably understand, you know, better than almost anyone how urgent those benefits are."
    },
    {
      "id": 573,
      "start": 2779.46,
      "end": 2789.34,
      "text": "Um, and so I really understand the stakes when, you know, when I speak out about AI has these risks and I'm worried about these risks, I get very angry when people call me a doomer."
    },
    {
      "id": 574,
      "start": 2789.34,
      "end": 2794.56,
      "text": "I got really angry when, you know, when, when someone's like, this guy's a doomer, he wants to slow things down."
    },
    {
      "id": 575,
      "start": 2795.7,
      "end": 2796.84,
      "text": "You, you heard what I just said."
    },
    {
      "id": 576,
      "start": 2796.9,
      "end": 2802.86,
      "text": "Like, you know, my, my father died because of, you know, cures that, you know, could have, could have happened a few years later."
    },
    {
      "id": 577,
      "start": 2803.1,
      "end": 2805.08,
      "text": "I understand the benefit of this technology."
    },
    {
      "id": 578,
      "start": 2805.08,
      "end": 2814.22,
      "text": "When I sat down to, to, to write Machines of Loving Grace, you know, I wrote out all the ways that billions of people's lives could be better with this technology."
    },
    {
      "id": 579,
      "start": 2814.5,
      "end": 2822.12,
      "text": "Some of these people, some of these people who on Twitter, you know, cheer for acceleration, I don't think they have a humanistic sense of the benefit of the technology."
    },
    {
      "id": 580,
      "start": 2822.74,
      "end": 2827.06,
      "text": "Their, their brain's just full of adrenaline and, and they're like, they want to cheer for something."
    },
    {
      "id": 581,
      "start": 2827.18,
      "end": 2828.02,
      "text": "They want to accelerate."
    },
    {
      "id": 582,
      "start": 2828.36,
      "end": 2829.84,
      "text": "I don't get the sense they care."
    },
    {
      "id": 583,
      "start": 2829.84,
      "end": 2837.92,
      "text": "And so when these people call me a doomer, I think, I think they just completely, completely lack any moral credibility in doing that."
    },
    {
      "id": 584,
      "start": 2838.6600000000003,
      "end": 2840.6400000000003,
      "text": "You know, it really makes me lose respect for them."
    },
    {
      "id": 585,
      "start": 2841.1200000000003,
      "end": 2852.46,
      "text": "And I've been wondering what this, this word impact has been because it's come up so often that those who have been around you have said you've been singularly obsessed with having impact."
    },
    {
      "id": 586,
      "start": 2852.46,
      "end": 2859.46,
      "text": "In fact, I spoke with someone who knew you well, who said you wouldn't watch Game of Thrones because it wasn't tied to impact, that it was a waste of time."
    },
    {
      "id": 587,
      "start": 2859.84,
      "end": 2861.76,
      "text": "And you wanted to be focused on impact."
    },
    {
      "id": 588,
      "start": 2862.2,
      "end": 2863.7200000000003,
      "text": "Actually, that's not quite right."
    },
    {
      "id": 589,
      "start": 2863.8,
      "end": 2866.98,
      "text": "I wouldn't watch it because it was so negative sum."
    },
    {
      "id": 590,
      "start": 2867.08,
      "end": 2875.96,
      "text": "People were playing, were playing such negative, it was like these people start off and they're partly the situation and partly because they're, they're just horrible people."
    },
    {
      "id": 591,
      "start": 2875.96,
      "end": 2881.18,
      "text": "They like create the situation where at the end of it, everyone is like worse off than everyone was before."
    },
    {
      "id": 592,
      "start": 2881.9,
      "end": 2885.98,
      "text": "I'm really, I'm really excited about like creating positive sum situations."
    },
    {
      "id": 593,
      "start": 2886.34,
      "end": 2886.44,
      "text": "Okay."
    },
    {
      "id": 594,
      "start": 2886.46,
      "end": 2887.7,
      "text": "I recommend you watch it."
    },
    {
      "id": 595,
      "start": 2887.8,
      "end": 2888.9,
      "text": "It's a great, great show."
    },
    {
      "id": 596,
      "start": 2889.0,
      "end": 2890.36,
      "text": "But I hear, I hear your complaints."
    },
    {
      "id": 597,
      "start": 2890.36,
      "end": 2890.94,
      "text": "I have watched some parts of it."
    },
    {
      "id": 598,
      "start": 2890.94,
      "end": 2893.7200000000003,
      "text": "I was just very reluctant and didn't watch it for a long time."
    },
    {
      "id": 599,
      "start": 2893.7400000000002,
      "end": 2894.7200000000003,
      "text": "Let's get back to the impact."
    },
    {
      "id": 600,
      "start": 2894.7200000000003,
      "end": 2895.02,
      "text": "Okay."
    },
    {
      "id": 601,
      "start": 2895.04,
      "end": 2896.1,
      "text": "Let's get back to the impact."
    },
    {
      "id": 602,
      "start": 2896.2400000000002,
      "end": 2905.84,
      "text": "So that's what impact is, is effectively your career has been this, I, this quest to have that impact to be able, tell me if I'm going too far."
    },
    {
      "id": 603,
      "start": 2905.96,
      "end": 2908.88,
      "text": "To prevent other people from being in similar situations."
    },
    {
      "id": 604,
      "start": 2909.2200000000003,
      "end": 2912.08,
      "text": "You know, I, I think, you know, I think, I think that's a piece of it."
    },
    {
      "id": 605,
      "start": 2912.16,
      "end": 2920.04,
      "text": "I mean, you know, I, I have looked at, you know, many, you know, many attempts to help people."
    },
    {
      "id": 606,
      "start": 2920.52,
      "end": 2923.26,
      "text": "And, you know, some of them are more effective than others."
    },
    {
      "id": 607,
      "start": 2924.34,
      "end": 2929.86,
      "text": "And, you know, I think, I think I've always tried to, you know, there should be strategy behind it."
    },
    {
      "id": 608,
      "start": 2929.86,
      "end": 2935.2,
      "text": "There should be brains behind, you know, trying to, trying to help people."
    },
    {
      "id": 609,
      "start": 2935.96,
      "end": 2938.26,
      "text": "You know, which often means that there's a long path to it, right?"
    },
    {
      "id": 610,
      "start": 2938.28,
      "end": 2946.18,
      "text": "It can run through a company and, you know, many activities that are technical and not immediately tied to the, the kind of impact, impact that you're trying to have."
    },
    {
      "id": 611,
      "start": 2946.18,
      "end": 2950.46,
      "text": "But, you know, the, the, the, the arc is, I'm always trying to bend the arc towards that."
    },
    {
      "id": 612,
      "start": 2950.52,
      "end": 2953.06,
      "text": "I think, I think that's my, that's my picture of it."
    },
    {
      "id": 613,
      "start": 2953.06,
      "end": 2956.2799999999997,
      "text": "That's, that's really why I, that's really why I got into this, right?"
    },
    {
      "id": 614,
      "start": 2956.38,
      "end": 2968.7999999999997,
      "text": "You know, I think, you know, similar to the reason to get into AI was that, you know, I, I saw the problems of biology as, as almost intractable without it, or at least too slow moving."
    },
    {
      "id": 615,
      "start": 2968.8,
      "end": 2984.9,
      "text": "You know, I think my reason to start a company was that I had worked at other companies and I, I, I just didn't feel like the way those companies were, were run was, was really oriented towards, you know, trying, trying, trying to have that impact."
    },
    {
      "id": 616,
      "start": 2984.9,
      "end": 2991.26,
      "text": "There was a story around it that was often used for recruiting, but it became clear to me over the years that story was not sincere."
    },
    {
      "id": 617,
      "start": 2991.46,
      "end": 2995.36,
      "text": "I'm going to circle around a little bit because it's clear that you're referring to open AI here."
    },
    {
      "id": 618,
      "start": 2997.02,
      "end": 3000.86,
      "text": "From what I understand, you had 50% of open AI's compute."
    },
    {
      "id": 619,
      "start": 3001.02,
      "end": 3003.64,
      "text": "I mean, you ran the GPT-3 project."
    },
    {
      "id": 620,
      "start": 3003.86,
      "end": 3007.98,
      "text": "So if anyone was going to be focused on impact and safety, it wouldn't have been you."
    },
    {
      "id": 621,
      "start": 3008.92,
      "end": 3014.6800000000003,
      "text": "Yes, I, I was, you know, there was a period during which, during which that was, that was true."
    },
    {
      "id": 622,
      "start": 3014.68,
      "end": 3016.7,
      "text": "That wasn't true the entire time."
    },
    {
      "id": 623,
      "start": 3016.7799999999997,
      "end": 3019.48,
      "text": "That was, for example, when we were scaling up GPT-3."
    },
    {
      "id": 624,
      "start": 3020.22,
      "end": 3020.62,
      "text": "Yeah."
    },
    {
      "id": 625,
      "start": 3020.7599999999998,
      "end": 3029.8999999999996,
      "text": "So, you know, I, I, when I was at open AI, I, and a lot of my colleagues, including the people who, you know, eventually, eventually founded Anthropic."
    },
    {
      "id": 626,
      "start": 3030.18,
      "end": 3030.66,
      "text": "The pandas."
    },
    {
      "id": 627,
      "start": 3031.74,
      "end": 3032.14,
      "text": "Pandas."
    },
    {
      "id": 628,
      "start": 3032.8399999999997,
      "end": 3033.8799999999997,
      "text": "That's the name you gave them."
    },
    {
      "id": 629,
      "start": 3034.3399999999997,
      "end": 3036.06,
      "text": "That, that, that isn't a name I gave them."
    },
    {
      "id": 630,
      "start": 3036.16,
      "end": 3036.7599999999998,
      "text": "The name they took."
    },
    {
      "id": 631,
      "start": 3037.3999999999996,
      "end": 3038.7799999999997,
      "text": "That isn't a name they took."
    },
    {
      "id": 632,
      "start": 3039.18,
      "end": 3040.66,
      "text": "That's the name other people called them."
    },
    {
      "id": 633,
      "start": 3041.06,
      "end": 3043.96,
      "text": "I, I, maybe it's a name other people called them."
    },
    {
      "id": 634,
      "start": 3043.96,
      "end": 3046.06,
      "text": "That's not a name I ever used for my team."
    },
    {
      "id": 635,
      "start": 3046.16,
      "end": 3046.34,
      "text": "Okay."
    },
    {
      "id": 636,
      "start": 3046.64,
      "end": 3046.9,
      "text": "Sorry."
    },
    {
      "id": 637,
      "start": 3047.1,
      "end": 3047.38,
      "text": "Go ahead."
    },
    {
      "id": 638,
      "start": 3047.78,
      "end": 3048.82,
      "text": "That's a good clarification."
    },
    {
      "id": 639,
      "start": 3049.34,
      "end": 3049.7,
      "text": "Thank you."
    },
    {
      "id": 640,
      "start": 3050.78,
      "end": 3056.52,
      "text": "So, yeah, you know, we were involved in scaling up these models."
    },
    {
      "id": 641,
      "start": 3056.96,
      "end": 3065.8,
      "text": "Actually, the original reason for building GPT-2 and GPT-3, it was an outgrowth of the kind of AI alignment work that we were doing, right?"
    },
    {
      "id": 642,
      "start": 3065.8,
      "end": 3072.7400000000002,
      "text": "Where myself and Paul Cristiano and some of the Anthropic co-founders had invented this technique called RL from human feedback."
    },
    {
      "id": 643,
      "start": 3072.74,
      "end": 3081.7,
      "text": "And that was designed to help steer models in, you know, in a direction to follow human intent."
    },
    {
      "id": 644,
      "start": 3081.7999999999997,
      "end": 3095.8199999999997,
      "text": "It was actually a precursor to, you know, we were trying to scale up another method called scalable supervision, which I think is just starting to, to, to work many years later to help models follow more kind of scalable human intent."
    },
    {
      "id": 645,
      "start": 3095.82,
      "end": 3107.38,
      "text": "But what we found is even with the more primitive technique, RL from human feedback, it wasn't working with the small language models with, you know, GPT-1 that we applied it to and that had, had been built by other people at OpenAI."
    },
    {
      "id": 646,
      "start": 3107.38,
      "end": 3116.26,
      "text": "And so the scaling up of GPT-2 and GPT-3 was done in order to kind of study these techniques in order to apply RL from human feedback at scale."
    },
    {
      "id": 647,
      "start": 3117.6600000000003,
      "end": 3130.38,
      "text": "You know, this goes to one thing, which is that I think in this field, the alignment of AI systems and the capability of AI systems is intertwined in this way that always ends up being kind of more tied and more intertwined than we think."
    },
    {
      "id": 648,
      "start": 3130.38,
      "end": 3140.3,
      "text": "Actually, what this made me realize is that it's very hard to work on the safety of AI systems and the capability of AI systems separately."
    },
    {
      "id": 649,
      "start": 3140.4,
      "end": 3142.38,
      "text": "It's very hard to work on one and not the other."
    },
    {
      "id": 650,
      "start": 3142.7200000000003,
      "end": 3153.2200000000003,
      "text": "I actually think the value and the way to inflect the field in a more positive way comes from organizational level decisions."
    },
    {
      "id": 651,
      "start": 3153.22,
      "end": 3159.7,
      "text": "When to release things, when to study things internally, what kind of work to do on systems."
    },
    {
      "id": 652,
      "start": 3160.2999999999997,
      "end": 3167.7999999999997,
      "text": "And that was one of the things that kind of motivated, you know, me and some of the other, you know, to be anthropic founders to kind of go off and do it our own way."
    },
    {
      "id": 653,
      "start": 3167.8,
      "end": 3183.7200000000003,
      "text": "But again, like if you were driving, if you think language, if you think capabilities and safety are interlinked and you were the guy driving the cutting edge models within OpenAI, you know, if you left, you knew they were going to be a company that was still doing this stuff."
    },
    {
      "id": 654,
      "start": 3183.98,
      "end": 3184.2200000000003,
      "text": "That's right."
    },
    {
      "id": 655,
      "start": 3184.2400000000002,
      "end": 3190.1800000000003,
      "text": "It seems like if you're driving the capabilities, you'd be the one in the driver's seat to help it be safe the way that you want it to."
    },
    {
      "id": 656,
      "start": 3190.18,
      "end": 3214.74,
      "text": "Again, I will say, you know, if there's a decision on releasing a model, if there's a decision on the governance of the company, if there's a decision on, you know, how the personnel of the company works, you know, how the company represents itself externally, the decisions that the company makes with respect to deployment, the claims it makes about how it operates with respect to society."
    },
    {
      "id": 657,
      "start": 3214.74,
      "end": 3222.3999999999996,
      "text": "You know, many of those things are not things that you control just by training the model."
    },
    {
      "id": 658,
      "start": 3222.54,
      "end": 3224.68,
      "text": "And, you know, I think trust is really important."
    },
    {
      "id": 659,
      "start": 3224.7799999999997,
      "end": 3228.8399999999997,
      "text": "I think the leaders of a company, they have to be trustworthy people."
    },
    {
      "id": 660,
      "start": 3229.18,
      "end": 3231.66,
      "text": "They have to be people whose motivations are sincere."
    },
    {
      "id": 661,
      "start": 3231.8999999999996,
      "end": 3244.3199999999997,
      "text": "No matter how much you're driving forward the company technically, if you're working for someone whose motivations are not sincere, who's not an honest person, who does not truly want to make the world better, it's not going to work."
    },
    {
      "id": 662,
      "start": 3244.32,
      "end": 3246.1200000000003,
      "text": "You're just contributing to something bad."
    },
    {
      "id": 663,
      "start": 3246.88,
      "end": 3254.9,
      "text": "So, and I'm sure you've heard the criticism from people like Jensen who say, well, Dario thinks he's the only one who can build this safely."
    },
    {
      "id": 664,
      "start": 3255.04,
      "end": 3259.06,
      "text": "And therefore, speaking of that word control, wants to control the entire industry."
    },
    {
      "id": 665,
      "start": 3259.1800000000003,
      "end": 3261.54,
      "text": "I've never said anything like that."
    },
    {
      "id": 666,
      "start": 3261.9,
      "end": 3263.28,
      "text": "That's an outrageous lie."
    },
    {
      "id": 667,
      "start": 3263.38,
      "end": 3265.46,
      "text": "That's the most outrageous lie I've ever heard."
    },
    {
      "id": 668,
      "start": 3266.2200000000003,
      "end": 3268.54,
      "text": "By the way, I'm sorry if I got Jensen's words wrong."
    },
    {
      "id": 669,
      "start": 3268.8,
      "end": 3269.36,
      "text": "No, no, no."
    },
    {
      "id": 670,
      "start": 3269.4,
      "end": 3270.28,
      "text": "The words were correct."
    },
    {
      "id": 671,
      "start": 3270.36,
      "end": 3270.52,
      "text": "Okay."
    },
    {
      "id": 672,
      "start": 3270.52,
      "end": 3274.7599999999998,
      "text": "But the words are outrageous."
    },
    {
      "id": 673,
      "start": 3274.92,
      "end": 3285.52,
      "text": "In fact, I've said multiple times, and I think Anthropics' actions have shown it, that we're aiming for something we call a race to the top."
    },
    {
      "id": 674,
      "start": 3286.2599999999998,
      "end": 3298.3,
      "text": "I've said this on podcasts over the years, and I think Anthropics' actions have shown it, where with a race to the bottom, everyone is competing to get things out as fast as possible."
    },
    {
      "id": 675,
      "start": 3298.3,
      "end": 3302.98,
      "text": "And so I say when you have a race to the bottom, it doesn't matter who wins, everyone loses, right?"
    },
    {
      "id": 676,
      "start": 3303.02,
      "end": 3312.5,
      "text": "Because you make the unsafe system that, you know, helps your adversary or causes economic problems or, you know, is unsafe from an alignment perspective."
    },
    {
      "id": 677,
      "start": 3312.7400000000002,
      "end": 3323.04,
      "text": "The way I think about the race to the top is that it doesn't matter who wins, everyone wins, right?"
    },
    {
      "id": 678,
      "start": 3323.04,
      "end": 3326.7799999999997,
      "text": "So the way the race to the top works is you set an example for how the field works."
    },
    {
      "id": 679,
      "start": 3326.88,
      "end": 3331.38,
      "text": "You say, you know, we're going to engage in this practice."
    },
    {
      "id": 680,
      "start": 3331.52,
      "end": 3334.44,
      "text": "So a key example of this is responsible scaling policies."
    },
    {
      "id": 681,
      "start": 3334.88,
      "end": 3337.56,
      "text": "We were the first to put out a responsible scaling policy."
    },
    {
      "id": 682,
      "start": 3337.92,
      "end": 3341.46,
      "text": "And, you know, we didn't say everyone else should do this or you're bad guys."
    },
    {
      "id": 683,
      "start": 3341.46,
      "end": 3345.06,
      "text": "We didn't, you know, we didn't, you know, kind of try to use it as an advantage."
    },
    {
      "id": 684,
      "start": 3345.16,
      "end": 3347.78,
      "text": "We put it out and then we encouraged everyone else to do it."
    },
    {
      "id": 685,
      "start": 3348.8,
      "end": 3357.86,
      "text": "And many, and then we discovered in the months after that, that, you know, there were people within the other companies who were trying to put out responsible scaling policies."
    },
    {
      "id": 686,
      "start": 3357.86,
      "end": 3363.08,
      "text": "But the fact that we had done it allowed, you know, gave those people permission, right?"
    },
    {
      "id": 687,
      "start": 3363.2000000000003,
      "end": 3371.58,
      "text": "Kind of, kind of enabled those people to, you know, to make the argument to leadership, hey, Anthropic is doing this, so we should do it as well."
    },
    {
      "id": 688,
      "start": 3371.78,
      "end": 3374.26,
      "text": "The same has been true of investing in interpretability."
    },
    {
      "id": 689,
      "start": 3374.5,
      "end": 3383.76,
      "text": "We release our interpretability research to everyone and allow other companies to copy it, even though we've seen that it sometimes has commercial advantages."
    },
    {
      "id": 690,
      "start": 3383.76,
      "end": 3392.94,
      "text": "Same with things like constitutional AI, same with the, you know, the measurement of the dangers of our system, dangerous capabilities evals."
    },
    {
      "id": 691,
      "start": 3393.2000000000003,
      "end": 3400.5200000000004,
      "text": "So we're trying to set an example for the field, but there's an interplay where it helps to be a powerful commercial competitor."
    },
    {
      "id": 692,
      "start": 3400.84,
      "end": 3409.44,
      "text": "I've said nothing that anywhere near resembles the idea that this company should be the only one to build the technology."
    },
    {
      "id": 693,
      "start": 3409.44,
      "end": 3413.32,
      "text": "I don't know how anyone could ever derive that from anything that I've said."
    },
    {
      "id": 694,
      "start": 3413.76,
      "end": 3420.5,
      "text": "It's just, yeah, yeah, it's just an incredible and bad faith distortion."
    },
    {
      "id": 695,
      "start": 3421.36,
      "end": 3425.5400000000004,
      "text": "All right, let's see if we can lightning around like one or two before I ask you the last one, which we'll have five minutes for."
    },
    {
      "id": 696,
      "start": 3427.2000000000003,
      "end": 3428.5400000000004,
      "text": "What happened with SBF?"
    },
    {
      "id": 697,
      "start": 3429.1000000000004,
      "end": 3430.38,
      "text": "What happened with SBF?"
    },
    {
      "id": 698,
      "start": 3430.44,
      "end": 3432.0,
      "text": "I mean, he was one of the, go ahead."
    },
    {
      "id": 699,
      "start": 3432.1400000000003,
      "end": 3433.1400000000003,
      "text": "I couldn't tell you."
    },
    {
      "id": 700,
      "start": 3433.5,
      "end": 3435.7400000000002,
      "text": "What didn't you answer?"
    },
    {
      "id": 701,
      "start": 3435.74,
      "end": 3438.4399999999996,
      "text": "I probably met the guy four or five times."
    },
    {
      "id": 702,
      "start": 3438.6,
      "end": 3438.7599999999998,
      "text": "Okay."
    },
    {
      "id": 703,
      "start": 3439.72,
      "end": 3454.6,
      "text": "So I have no great insight into the, you know, what, you know, into the psychology of SBF or, you know, why he did things as stupid or immoral as he did."
    },
    {
      "id": 704,
      "start": 3454.6,
      "end": 3472.12,
      "text": "I think the only, you know, the only thing I had ever seen ahead of time with SBF was, you know, a couple people mentioned to me that he was like hard to work with, that, you know, he was like a bit of a move fast and break things guy."
    },
    {
      "id": 705,
      "start": 3472.68,
      "end": 3475.04,
      "text": "And I was like, okay, you know, there's like plenty of people."
    },
    {
      "id": 706,
      "start": 3475.24,
      "end": 3475.86,
      "text": "Welcome to Silicon Valley."
    },
    {
      "id": 707,
      "start": 3475.86,
      "end": 3478.34,
      "text": "Yeah, like welcome to Silicon Valley."
    },
    {
      "id": 708,
      "start": 3479.08,
      "end": 3482.2000000000003,
      "text": "And so I remember saying, okay, I'm going to give this guy non-voting shares."
    },
    {
      "id": 709,
      "start": 3482.36,
      "end": 3483.56,
      "text": "I'm not going to put him on the board."
    },
    {
      "id": 710,
      "start": 3483.86,
      "end": 3486.86,
      "text": "He sounds like a, you know, he sounds like a bad person to deal with every day."
    },
    {
      "id": 711,
      "start": 3487.52,
      "end": 3490.32,
      "text": "But, you know, he's excited about AI."
    },
    {
      "id": 712,
      "start": 3490.52,
      "end": 3491.88,
      "text": "He's excited about AI safety."
    },
    {
      "id": 713,
      "start": 3492.08,
      "end": 3495.88,
      "text": "He's, you know, he's a bull on AI and he's interested in AI safety."
    },
    {
      "id": 714,
      "start": 3496.1,
      "end": 3500.28,
      "text": "So, you know, seems like a sensible, seems like a sensible thing to do."
    },
    {
      "id": 715,
      "start": 3500.28,
      "end": 3515.02,
      "text": "You know, in retrospect, you know, that, you know, move fast and break things, you know, was turned out to be much, much, much more extreme and bad than, you know, than I ever imagined."
    },
    {
      "id": 716,
      "start": 3515.42,
      "end": 3515.6200000000003,
      "text": "Okay."
    },
    {
      "id": 717,
      "start": 3515.7400000000002,
      "end": 3516.6000000000004,
      "text": "So let's end here."
    },
    {
      "id": 718,
      "start": 3516.96,
      "end": 3518.42,
      "text": "So you found your impact."
    },
    {
      "id": 719,
      "start": 3518.82,
      "end": 3521.7400000000002,
      "text": "I mean, you're working the dream pretty much right now."
    },
    {
      "id": 720,
      "start": 3521.82,
      "end": 3527.78,
      "text": "I mean, think about all the ways that AI can be used for biology just to start."
    },
    {
      "id": 721,
      "start": 3527.78,
      "end": 3532.6000000000004,
      "text": "You also say that this is a dangerous technology."
    },
    {
      "id": 722,
      "start": 3533.3,
      "end": 3550.52,
      "text": "And I'm curious if your desire for impact could be pushing you to accelerate this technology while, you know, potentially devaluing the possibility that it could, that controlling it might not be feasible."
    },
    {
      "id": 723,
      "start": 3550.94,
      "end": 3556.86,
      "text": "So, you know, I think I have more than anyone else in the industry warned about the dangers of the technology, right?"
    },
    {
      "id": 724,
      "start": 3556.86,
      "end": 3572.0,
      "text": "We just spent 10, 20 minutes talking about, you know, the frightening, you know, the large array of, you know, people who run, you know, trillion dollar companies criticizing me for, you know, for talking about the dangers of these technologies."
    },
    {
      "id": 725,
      "start": 3572.0,
      "end": 3572.36,
      "text": "Right."
    },
    {
      "id": 726,
      "start": 3572.42,
      "end": 3574.5,
      "text": "You know, I have U.S. government officials."
    },
    {
      "id": 727,
      "start": 3574.5,
      "end": 3580.86,
      "text": "I have people who run $4 trillion companies criticizing me for talking about the dangers of the technology."
    },
    {
      "id": 728,
      "start": 3581.02,
      "end": 3581.12,
      "text": "Right."
    },
    {
      "id": 729,
      "start": 3581.2,
      "end": 3590.14,
      "text": "Imputing all these bizarre motives that bear no relationship to, you know, to anything I've ever said, not supported in anything I've ever done."
    },
    {
      "id": 730,
      "start": 3590.26,
      "end": 3592.16,
      "text": "And yet I'm going to continue to do it."
    },
    {
      "id": 731,
      "start": 3592.16,
      "end": 3607.3199999999997,
      "text": "I actually think that, you know, as the revenues, as the economic business of AI ramps up and it's ramping up exponentially, you know, if I'm right, in a couple years, it'll be the biggest source of revenue in the world."
    },
    {
      "id": 732,
      "start": 3607.42,
      "end": 3607.54,
      "text": "Right."
    },
    {
      "id": 733,
      "start": 3607.56,
      "end": 3609.24,
      "text": "It'll be the biggest industry in the world."
    },
    {
      "id": 734,
      "start": 3609.24,
      "end": 3611.3999999999996,
      "text": "And people who run companies already think it."
    },
    {
      "id": 735,
      "start": 3611.5,
      "end": 3624.64,
      "text": "So we actually have this terrifying situation where, you know, hundreds of billions to trillions to, I would say, maybe 20 trillion of capitals on the side of Accelerate AI as fast as possible."
    },
    {
      "id": 736,
      "start": 3625.02,
      "end": 3631.3199999999997,
      "text": "We have this, you know, company that's very valuable in absolute terms, but, you know, looks very small compared to that."
    },
    {
      "id": 737,
      "start": 3631.3999999999996,
      "end": 3631.54,
      "text": "Right."
    },
    {
      "id": 738,
      "start": 3631.54,
      "end": 3633.7799999999997,
      "text": "60, 60, 60 billion dollars."
    },
    {
      "id": 739,
      "start": 3634.2599999999998,
      "end": 3653.02,
      "text": "And I keep speaking up, even if, you know, it makes folks and, you know, there have been these articles, you know, some folks in the U.S. government are upset at us, for example, for opposing the moratorium on AI regulation, for being in favor of export controls for chips on China, for talking about the economic impacts of AI."
    },
    {
      "id": 740,
      "start": 3653.48,
      "end": 3657.22,
      "text": "Every time I do that, I get attacked by many of my peers."
    },
    {
      "id": 741,
      "start": 3657.38,
      "end": 3657.46,
      "text": "Right."
    },
    {
      "id": 742,
      "start": 3657.48,
      "end": 3659.36,
      "text": "But you're still assuming that we can control it."
    },
    {
      "id": 743,
      "start": 3659.56,
      "end": 3660.4,
      "text": "That's what I'm pointing out."
    },
    {
      "id": 744,
      "start": 3660.4,
      "end": 3674.78,
      "text": "But I'm just telling you how much effort, how much persistence, how much despite everything that stacked up, despite all the dangers, despite the risk that it has to the company of being willing to speak up, I'm willing to do it."
    },
    {
      "id": 745,
      "start": 3675.34,
      "end": 3683.84,
      "text": "And that's why I'm saying that, look, if I thought that there was no way to control the technology, right?"
    },
    {
      "id": 746,
      "start": 3683.84,
      "end": 3687.34,
      "text": "If I thought, even if I thought this is just a gamble, right?"
    },
    {
      "id": 747,
      "start": 3687.34,
      "end": 3691.48,
      "text": "Some people are like, oh, you think there's a 5 or 10 percent chance that AI could go wrong."
    },
    {
      "id": 748,
      "start": 3691.6400000000003,
      "end": 3692.94,
      "text": "You're just rolling the dice."
    },
    {
      "id": 749,
      "start": 3693.1000000000004,
      "end": 3694.58,
      "text": "That's not the way I think about it."
    },
    {
      "id": 750,
      "start": 3694.78,
      "end": 3696.36,
      "text": "This is a multi-step game."
    },
    {
      "id": 751,
      "start": 3696.6000000000004,
      "end": 3696.8,
      "text": "Right."
    },
    {
      "id": 752,
      "start": 3696.9,
      "end": 3698.02,
      "text": "You take one step."
    },
    {
      "id": 753,
      "start": 3698.02,
      "end": 3700.48,
      "text": "You build the next step of most powerful models."
    },
    {
      "id": 754,
      "start": 3700.7,
      "end": 3703.32,
      "text": "You have a more intensive testing regime."
    },
    {
      "id": 755,
      "start": 3703.56,
      "end": 3707.96,
      "text": "As we get closer and closer to the more powerful models, I'm speaking up more and more."
    },
    {
      "id": 756,
      "start": 3708.12,
      "end": 3714.54,
      "text": "And I'm taking more and more drastic actions because I'm concerned that the risks of AI are getting closer and closer."
    },
    {
      "id": 757,
      "start": 3714.7599999999998,
      "end": 3716.08,
      "text": "We're working to address them."
    },
    {
      "id": 758,
      "start": 3716.28,
      "end": 3717.92,
      "text": "We've made a certain amount of progress."
    },
    {
      "id": 759,
      "start": 3717.92,
      "end": 3735.2400000000002,
      "text": "But when I worry that the progress that we've made on the risks is not fully aligned with the â€“ is not going as fast as we need to go for the speed of the technology, then I speak up louder."
    },
    {
      "id": 760,
      "start": 3735.94,
      "end": 3742.54,
      "text": "And so you're asking, why am I â€“ you started this interview by saying, what's gotten into you?"
    },
    {
      "id": 761,
      "start": 3742.86,
      "end": 3744.16,
      "text": "Why are you talking about this?"
    },
    {
      "id": 762,
      "start": 3744.16,
      "end": 3754.16,
      "text": "It's because the exponential is getting to the point that I worry that we may have a situation that our ability to handle the risks is not keeping up with the speed of the technology."
    },
    {
      "id": 763,
      "start": 3754.52,
      "end": 3755.94,
      "text": "And that's how I'm responding to it."
    },
    {
      "id": 764,
      "start": 3756.18,
      "end": 3763.7999999999997,
      "text": "If I believe that there was no way to control the technology, which I â€“ I see absolutely no evidence for that proposition."
    },
    {
      "id": 765,
      "start": 3764.02,
      "end": 3768.74,
      "text": "We've gotten better at controlling models with every model that we release."
    },
    {
      "id": 766,
      "start": 3768.74,
      "end": 3768.98,
      "text": "Right."
    },
    {
      "id": 767,
      "start": 3768.98,
      "end": 3774.12,
      "text": "All these things go wrong, but like you really â€“ you really have to stress test the models pretty hard."
    },
    {
      "id": 768,
      "start": 3774.44,
      "end": 3777.34,
      "text": "That doesn't mean you can't have emergent bad behavior."
    },
    {
      "id": 769,
      "start": 3777.84,
      "end": 3785.28,
      "text": "And I think, you know, if we got to much more powerful models with only the alignment techniques we have now, then I'd be very concerned."
    },
    {
      "id": 770,
      "start": 3785.66,
      "end": 3788.42,
      "text": "Then I'd be out there saying everyone should stop building these things."
    },
    {
      "id": 771,
      "start": 3788.52,
      "end": 3789.84,
      "text": "Even China should stop building these."
    },
    {
      "id": 772,
      "start": 3789.84,
      "end": 3795.3,
      "text": "I don't think they'd listen to me, which is one reason I think export controls is a better â€“ is a better measure."
    },
    {
      "id": 773,
      "start": 3795.3,
      "end": 3807.1200000000003,
      "text": "But if we got a few years ahead in models and had only the alignment and steering techniques we had today, then, you know, I would definitely be advocating for us to, you know, to slow down a lot."
    },
    {
      "id": 774,
      "start": 3807.38,
      "end": 3817.1000000000004,
      "text": "The reason I'm warning about the risk is so that we don't have to slow down, so that we can invest in safety techniques and can continue the progress â€“ continue the progress of the field."
    },
    {
      "id": 775,
      "start": 3817.1,
      "end": 3819.66,
      "text": "It would be a huge economic effort."
    },
    {
      "id": 776,
      "start": 3819.9,
      "end": 3825.4,
      "text": "Even if one company was willing to slow down the technology, you know, that doesn't stop all the other companies."
    },
    {
      "id": 777,
      "start": 3825.74,
      "end": 3831.92,
      "text": "That doesn't stop our geopolitical adversaries to whom this is an existential fight, fight, fight, fight for survival."
    },
    {
      "id": 778,
      "start": 3831.92,
      "end": 3839.26,
      "text": "So, you know, there's very little, you know, there's very little latitude here, right?"
    },
    {
      "id": 779,
      "start": 3839.3,
      "end": 3847.42,
      "text": "We're stuck between all the benefits of the technology, the race to accelerate it, and the fact that that is a multi-party race."
    },
    {
      "id": 780,
      "start": 3847.58,
      "end": 3855.82,
      "text": "And so I am doing the best thing I can do, which is to invest in safety technology to speed up the progress of safety."
    },
    {
      "id": 781,
      "start": 3855.82,
      "end": 3863.7000000000003,
      "text": "I've written essays on the importance of interpretability, on how important various directions in safety are."
    },
    {
      "id": 782,
      "start": 3863.9,
      "end": 3867.98,
      "text": "We release all of our safety work openly because we think that's the thing that's a public good."
    },
    {
      "id": 783,
      "start": 3868.06,
      "end": 3871.82,
      "text": "That's the thing that everyone needs to share."
    },
    {
      "id": 784,
      "start": 3871.92,
      "end": 3882.38,
      "text": "So if you have a better strategy for balancing the benefits, the inevitability of the technology, and the risks that it face, I am very open to hear it."
    },
    {
      "id": 785,
      "start": 3882.38,
      "end": 3894.44,
      "text": "Because I go to sleep every night thinking about it because I have such an incredible understanding of the stakes in terms of the benefits, in terms of, you know, what it can do, the lives that it can save."
    },
    {
      "id": 786,
      "start": 3894.52,
      "end": 3895.7200000000003,
      "text": "I've seen that personally."
    },
    {
      "id": 787,
      "start": 3895.94,
      "end": 3898.34,
      "text": "I also have seen the risks personally."
    },
    {
      "id": 788,
      "start": 3898.4,
      "end": 3901.1,
      "text": "We've already seen things go wrong with the models."
    },
    {
      "id": 789,
      "start": 3901.2200000000003,
      "end": 3903.02,
      "text": "You know, we have an example of that with Brock."
    },
    {
      "id": 790,
      "start": 3903.02,
      "end": 3913.74,
      "text": "And, you know, people dismiss this, but they're not going to laugh anymore when the models are taking actions, when they're manufacturing, and when they're in charge of, you know, medical, medical, medical interventions, right?"
    },
    {
      "id": 791,
      "start": 3913.96,
      "end": 3917.2,
      "text": "People can laugh at the risks when the models are just talking."
    },
    {
      "id": 792,
      "start": 3917.3,
      "end": 3918.42,
      "text": "But I think it's very serious."
    },
    {
      "id": 793,
      "start": 3918.66,
      "end": 3926.08,
      "text": "And so I think what this situation demands is a very serious understanding of both the risks and the benefits."
    },
    {
      "id": 794,
      "start": 3926.58,
      "end": 3928.12,
      "text": "These are high-stakes decisions."
    },
    {
      "id": 795,
      "start": 3928.12,
      "end": 3932.7799999999997,
      "text": "They need to be made with a seriousness."
    },
    {
      "id": 796,
      "start": 3933.56,
      "end": 3941.56,
      "text": "And I think something that makes me very concerned is that, on one hand, we have a cadre of people who are just doomers."
    },
    {
      "id": 797,
      "start": 3941.62,
      "end": 3942.56,
      "text": "People call me a doomer."
    },
    {
      "id": 798,
      "start": 3942.68,
      "end": 3942.96,
      "text": "I'm not."
    },
    {
      "id": 799,
      "start": 3943.02,
      "end": 3947.58,
      "text": "But there are doomers out there, people who say they know there's no way to build this safely."
    },
    {
      "id": 800,
      "start": 3948.56,
      "end": 3950.68,
      "text": "You know, I've looked at their arguments."
    },
    {
      "id": 801,
      "start": 3951.1,
      "end": 3952.44,
      "text": "They're a bunch of gobbledygook."
    },
    {
      "id": 802,
      "start": 3952.44,
      "end": 3960.42,
      "text": "The idea that these models have dangers associated with them, including dangers to humanity as a whole, that makes sense to me."
    },
    {
      "id": 803,
      "start": 3960.68,
      "end": 3966.42,
      "text": "The idea that we can kind of logically prove that there's no way to make them safe, that seems like nonsense to me."
    },
    {
      "id": 804,
      "start": 3966.7400000000002,
      "end": 3972.48,
      "text": "So I think that is an intellectually and morally unserious way to respond to the situation we're in."
    },
    {
      "id": 805,
      "start": 3972.48,
      "end": 3982.76,
      "text": "I also think it is intellectually and morally unserious for people who are sitting on $20 trillion of capital, who all work together because their incentives are all in the same way."
    },
    {
      "id": 806,
      "start": 3982.84,
      "end": 3989.6,
      "text": "There are dollar signs in all of their eyes, to sit there and say, we shouldn't regulate this technology for 10 years."
    },
    {
      "id": 807,
      "start": 3990.02,
      "end": 3997.48,
      "text": "Anyone who says that we should worry about the safety of these models is someone who just wants to control the technology themselves."
    },
    {
      "id": 808,
      "start": 3997.76,
      "end": 3999.42,
      "text": "That's an outrageous claim."
    },
    {
      "id": 809,
      "start": 3999.52,
      "end": 4001.66,
      "text": "And it's a morally unserious claim."
    },
    {
      "id": 810,
      "start": 4001.66,
      "end": 4005.2999999999997,
      "text": "We've sat here and we've done every possible piece of research."
    },
    {
      "id": 811,
      "start": 4005.66,
      "end": 4008.2999999999997,
      "text": "We speak up when we believe it's appropriate to do so."
    },
    {
      "id": 812,
      "start": 4008.62,
      "end": 4013.02,
      "text": "We've tried to back up, you know, when we make claims about the economic impact of AI."
    },
    {
      "id": 813,
      "start": 4013.16,
      "end": 4014.8599999999997,
      "text": "We have an economic research council."
    },
    {
      "id": 814,
      "start": 4014.8599999999997,
      "end": 4020.12,
      "text": "We have an economic index that we use to track the model in real time."
    },
    {
      "id": 815,
      "start": 4020.2,
      "end": 4026.72,
      "text": "And we're giving grants for people to understand the economic impact of the technology."
    },
    {
      "id": 816,
      "start": 4026.72,
      "end": 4039.3599999999997,
      "text": "I think for people who are far more financially invested in the success of the technology than I am to just, you know, breezily lob ad hominem attacks."
    },
    {
      "id": 817,
      "start": 4039.36,
      "end": 4044.02,
      "text": "You know, I think that is just as intellectually and morally unserious as the doomer's position."
    },
    {
      "id": 818,
      "start": 4044.54,
      "end": 4048.0,
      "text": "I think what we need here is we need more thoughtfulness."
    },
    {
      "id": 819,
      "start": 4048.32,
      "end": 4049.6800000000003,
      "text": "We need more honesty."
    },
    {
      "id": 820,
      "start": 4050.08,
      "end": 4059.32,
      "text": "We need more people willing to go against their interest, willing to not have, you know, breezy Twitter fights, hot takes."
    },
    {
      "id": 821,
      "start": 4059.32,
      "end": 4071.48,
      "text": "We need people to actually invest in understanding the situation, actually do the work, actually put out the research, and actually add some light and some insight to the situation that we're in."
    },
    {
      "id": 822,
      "start": 4071.6400000000003,
      "end": 4073.1000000000004,
      "text": "I am trying to do that."
    },
    {
      "id": 823,
      "start": 4073.3,
      "end": 4076.94,
      "text": "I don't think I'm doing that perfectly as no human can."
    },
    {
      "id": 824,
      "start": 4077.1600000000003,
      "end": 4079.7400000000002,
      "text": "I'm trying to do it as well as I can."
    },
    {
      "id": 825,
      "start": 4079.9,
      "end": 4083.4,
      "text": "It would be very helpful if there were others who would try to do the same thing."
    },
    {
      "id": 826,
      "start": 4083.4,
      "end": 4088.04,
      "text": "Well, Dario, I said this off camera, but I want to make sure to say it on as we're wrapping up."
    },
    {
      "id": 827,
      "start": 4088.6800000000003,
      "end": 4091.62,
      "text": "I appreciate how much Anthropic publishes."
    },
    {
      "id": 828,
      "start": 4092.02,
      "end": 4100.32,
      "text": "We have learned a ton from the experiments, everything from red teaming the models to vending machine Claude, which we didn't have a chance to speak about today."
    },
    {
      "id": 829,
      "start": 4101.0,
      "end": 4104.1,
      "text": "But I think the world is better off just to hear everything going on here."
    },
    {
      "id": 830,
      "start": 4104.5,
      "end": 4107.58,
      "text": "And to that note, thank you for sitting down with me and spending so much time together."
    },
    {
      "id": 831,
      "start": 4107.74,
      "end": 4108.4,
      "text": "Thanks for having me."
    },
    {
      "id": 832,
      "start": 4108.4,
      "end": 4113.54,
      "text": "Thanks, everybody, for listening and watching, and we'll see you next time on Big Technology Podcast."
    }
  ]
}