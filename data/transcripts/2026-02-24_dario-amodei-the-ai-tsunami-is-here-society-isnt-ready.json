{
  "metadata": {
    "video_id": "68ylaeBbdsg",
    "url": "https://www.youtube.com/watch?v=68ylaeBbdsg",
    "title": "The AI Tsunami is Here & Society Isn't Ready | Dario Amodei x Nikhil Kamath",
    "channel": "Nikhil Kamath",
    "upload_date": "2026-02-24",
    "duration": "68m",
    "guests": [
      "Dario Amodei",
      "Nikhil Kamath"
    ],
    "transcribed_date": "2026-02-24",
    "whisper_model": "mlx-community/whisper-large-v3-turbo"
  },
  "language": "en",
  "text": "bending bending So I started playing with Claude. It's getting to that point where sometimes it surprises me by how much it knows me. I don't know if that makes sense. It is surprising to me that we are, in my view, so close to these models reaching the level of human intelligence. And yet, there doesn't seem to be a wider recognition in society of what's about to happen. It's as if this tsunami is coming at us. And, you know, it's so close. We can see it on the horizon. And yet, people are coming up with these explanations for, oh, it's not actually a tsunami. That's just a trick of the light. Like, there hasn't been a public awareness of the risk. What is India's role in all this? Many other companies come here as themselves a consumer company. And they see India as a market, right? A place to obtain consumers. We actually see things a little bit differently. What did you do before founding Anthropic? Yeah. So, I was actually originally a biologist. I, you know, did my undergrad in physics, my PhD in biophysics. And, you know, I wanted to understand biological systems so that I could cure disease. And, you know, the thing I noticed about studying biology was its incredible complexity. That, you know, for example, if you look at the protein mass spec work that I did, right, trying to find protein biomarkers, it's just really incredible how much complexity there is, right? You have a given protein. It's like, you know, the RNA gets spliced in a whole bunch of different ways depending on where it is in the cell. Then it gets post-translationally modified, phosphorylated, complexed with a whole bunch of other proteins. And I was starting to despair that it was too complicated for humans to understand. And then as I was doing this work on biology, I noticed a lot of the early work around AlexNet, which is one of the first neural nets, like, you know, almost 15 years ago now. And I said, wow, like, you know, AI is actually starting to work. It has some things in common with how the human brain works, but, you know, has the potential to be larger and scale better and learn tasks like biology. Maybe this is ultimately going to be the solution to, you know, to solving our problems of biology. So, you know, I went to work with Andrew Ng at Baidu. Then I was at Google for a year. Then I joined OpenAI a few months after it started and was, you know, was basically led all of research there for several years. But then eventually, you know, myself and a few other of the employees just kind of had our own vision for, you know, how we wanted to make AI and what we wanted the company to stand for. And so we went off and founded Anthropic. How was it? Was it like a fork in how OpenAI was thinking into what Anthropic eventually did? Yeah, you know, I would say, you know, my conviction and the conviction of my co-founders when we founded Anthropic were two of them. And I think one, we were starting to convince OpenAI of. The other, I was, you know, not, I didn't feel that we were convincing of. So the first was the, you know, the conviction in the scaling laws and the idea that, you know, if you scale up models, you give them more data, more compute. Again, there are a few modifications like RL, but not really very much. It's pretty close to pure scaling. You find that, you know, when you do that, you find, you know, incredible increases in performance. And, you know, I was finding that in like 2019 with GPT-2, you know, when we just first saw the first glimmers of the scaling laws. And of course, there were a lot of folks, you know, inside and outside who didn't believe it at all. And we really made the case to leadership like this is important. This is going to be a big deal. And I think they were kind of starting to believe us and ultimately went in that direction. And there was a second, you know, conviction I had, which is, look, you know, if these models are going to be kind of general cognitive agents, like general cognitive tools that match the capability of like the human brain, we better get this right. The economic implications are going to be enormous. The geopolitical implications are going to be enormous. The safety implications are going to be enormous. It's going to transform how the world works. And so we need to do it in the right way. And, you know, I think despite a lot of, you know, kind of language, verbiage about doing it in the right way, I was for a variety of reasons just not convinced that at the, you know, institution that I was at, that there was a real and serious conviction to do it in the right way. And so, you know, my view is always, you know, don't argue with someone else's vision. Don't try to get someone to do things the way you want to. If you have a strong vision and you share that vision with, you know, a few other people, you should just go off and do your own thing. And then you're responsible for your own mistakes. You don't have to answer for anyone else's. And, you know, maybe your vision works out. Maybe it doesn't. But, you know, at least it's yours. Didn't OpenAI believe in scaling laws? Because they went down the same path themselves too, right? Well, yeah, we succeeded. Can you explain what scaling laws are in very simple terms? It's like if, you know, you want a chemical reaction to produce oxygen or start a fire or something like that, you need different ingredients. And, you know, if you don't have enough of one ingredient, the reaction stops. But if you, you know, if you put ingredients together in proportion, you know, you get your, you know, your explosion or your fire or fire or whatever. And for AI, those ingredients are data, compute, you know, the size of the AI model. And so the scaling laws just tell you that, like, you know, if you put in the ingredients to the chemical reaction, the ingredients of data and model size, that what you get out is intelligence. Intelligence is the product of the chemical reaction. And what is intelligence? Intelligence as measured by the ability to translate language or the ability to write code or, you know, the ability to answer questions correctly about a story. Basically, any cognitive task we can think of. Any, you know, task that exists in text or in images. Any, any task that you can, you can do on a computer. How is the intelligence of today, as you are describing it, different from what a computer could do like five years ago? Yeah, you know, I would say, well, I mean, for example, five years ago, a computer, you could not ask a computer a question and have it write a one-page essay on that question. You could not ask a computer to, you know, implement a feature in code and have it implement that feature in code. None of those things were possible. You could not generate an image. You could not generate a video. You could not analyze a video. You know, I could get one of those, you know, you know, videos of like, you know, a monkey juggling or something and, you know, say, what's going on in this video? How many times did the ball change hands? And right now you could get Claude or another AI model to give you an answer on that. And five years ago, you know, none of those things were possible. I'm trying to figure out, has the definition of intelligence changed per se? Well, you know, what I would say is five years ago, you know, you could Google and there might be a website that, you know, would tell you a little bit about this, right? But, you know, you're just looking up some text that exists on the web, right? You know, maybe it's not about how to get a monkey to juggle. Maybe, you know, maybe it's about how to get a seal to juggle. You know, it's not quite exactly the same thing because maybe exactly the same thing doesn't exist. But, you know, as we see when people use these models, you know, you can ask and you can actually get an intelligent response. You can ask a specific question and have the model write, you know, one page about it. Or you can give it a, you know, you can give it a hypothetical. You know, what if I had, you know, the monkey juggle clubs instead of balls? Or, you know, what if I did this thing? And that information doesn't exist anywhere, you know, whereas the model is able to kind of think for itself and come up with an answer on its own. So it's something, you know, it's something totally new. It's not just matching some of the text that exists on the internet. Fair. So, you know, this is more like a conversation. So feel free to, like, talk about what you want to talk, not necessarily related to the questions that I'm asking. You look very animated when you speak. Did you ever teach? You know, I was originally an academic. And, you know, I thought that I might become a professor. You know, I got my Ph.D. I went all the way to being a postdoc at Stanford Medical School. And, you know, I was aiming to become a professor. So if I had become a professor, you know, I would have done that. But, you know, as I mentioned, you know, I got interested in AI. And to work in AI required a lot of computational resources. And that was mostly happening in industry. So that took me off the academic path and, you know, into industry. And, of course, you know, ultimately through several steps led me to start a company. But, you know, sometimes I think I'm still like a professor at heart. At this point, Dario, if AI is the most relevant thing in the world, if the world is realigning in a way and AI is determining who gets what and who doesn't get what, I'm talking about industries, you today are probably the most relevant person in the world. If Anthropic, in this last cycle, in this minute, is sitting on top of this pie. For somebody who was going on the path of being a teacher to have arrived to where you are today, are you best equipped for where you are today? Well, I mean, you know, first I would say a couple of things. You know, I think there's a lot of folks who are, right, relevant in different ways, right? You know, even within industry, there's the different layers of the stack. There's, like, the folks who make chips. There's the folks even earlier who make semiconductor manufacturing equipment. There's the folks who make models like us. And then there are other players who make models. There's the folks who make kind of applications after the models. You know, and then there's a bunch of other folks who have a say. There's, you know, governments. There's, like, civil society. So, you know, my hope, you know, isn't that there's, you know, just one tiny set of people that's relevant. I think we're trying to broaden the set of people who are relevant and, you know, turn it into a broader conversation. But, you know, I think at the same time, your question is a fair one. And one way I could interpret it is, like, you know, there's a certain randomness to how, you know, kind of, you know, a few people, you know, end up leading these, you know, leading these companies that kind of, you know, grow so fast. And it seems like, you know, in the near future will power so much of the economy. And, you know, I've said openly, publicly, not for the first time that I'm at least somewhat uncomfortable with the amount of concentration of power that's happening here, I would say, almost overnight, almost by accident. And, you know, we think, you know, about that in a bunch of ways. You know, one is we have an unusual governance structure, something called the Long-Term Benefit Trust. You know, it's a body that kind of ultimately appoints, you know, the majority of the board members for Anthropic and is, you know, made up of financially disinterested individuals. So that's some, you know, check on what one single person is doing. And then, you know, I think, as always, the government should play some role here. You know, I've been an advocate of, you know, proactive, although, you know, sensible, that doesn't slow down the technology, sensible regulation of the technology. Because, you know, I think, like, the people should have a say. Like, you know, governments and the people who elect them should have a say in how this goes. So I actually think of a lot of what I'm trying to do as kind of trying to preserve a balance of power, you know, kind of, you know, against the natural grain of this technology. For someone like me who is sitting on the outside and doesn't have a bone in this competition, when I watch OpenAI talk about how they were a not-for-profit company, or how you are projecting humility in the conversation that you're having right now, or how the American companies are competing with the Chinese companies which are coming about, this projection of humility where it is for the larger good, and not necessarily for how I view the world as companies with shareholders, with investment and revenues and seeking profit. Is this par for the course? Is this something you have to do? So, you know, I would put it in the following way. You know, I would say the philosophy of Anthropic from the beginning has been that we try not to make too many promises, and we try to keep the ones that we make. So, you know, we set ourselves up as, you know, a for-profit but public benefit corporation with this LTPT governance, and we've maintained that. We've said that, you know, our goal is to, you know, stay on the frontier of the technology, but, you know, to work on, you know, to work on, you know, the safety and security aspects of the technology. We've pioneered the science of interpretability. We've, you know, pioneered the science of alignment. I don't know if you saw, but we recently released a constitution for Claude, the ability to align models in line with the constitution. And, you know, we've done a bunch of policy advocacy and warning about risks, right? Warning about risks is not in our commercial interest, right? Like, people can come up with conspiracy theories, but, you know, I will tell you, saying that the models we build could be dangerous, whatever people might say, that's not an effective marketing strategy, and that's not the reason that we do it. And, you know, speaking up on when we disagree, even with the U.S. administration, on, you know, on policy matters, right? We've spoken up, right? We're willing to say, you know, we disagree on this issue. Like, you know, we've said that there should be regulation of AI, when all the other companies and the administration have said there shouldn't be regulation of AI. And so that's both, you know, the regulation of AI holds, you know, holds us back commercially as a company, even though I think it's the right thing to do. And it's, you know, it's difficult to go against the government and the other companies and say this. We're really sticking our neck out. So we've taken a number of actions that, you know, I see as really, you know, putting our money where our mouth is here. I can't speak for the other companies. You know, it's, again, it's quite possible that some people say these things, you know, and they don't really mean them. But I wouldn't look at what people say. I would look at what people do. If what you're saying gets the government to act via regulation, as the incumbent leaders in this space, you get some kind of a regulatory capture where it becomes harder for the new people coming in as well, right? I don't agree with that at all. The regulation we've advocated for, for example, SB53 in California, exempted everyone who makes under $500 million a year in revenue, right? SB53 was a transparency law, which, you know, basically requires companies to, you know, to show, you know, the safety and security tests that they've run. And it exempts all companies under $500 million in revenue. So it really only applies to Anthropic and three or four other companies. So it only applies to the companies that have the resources. And everything that we've advocated for here, not just SB53, but all the proposals that we've made, the ones that we've made in the past and the ones that we plan to make in the future, have this character. We're constraining ourselves and a very small number of additional companies. We're not—people who say that need to look at the actual content of what we're proposing because it doesn't match that idea at all. Fair. I read your paper, Machines of Loving Rays and the Adolescence of Technology, and you seem to have had a 180-degree shift in perspective almost, from optimism to skepticism over, like, two years, from 2024 to 2026. Is there one moment in the last two years that changed this for you? Did you see something change? Yeah, I actually wouldn't agree with the question. I don't think I've had a shift in perspective. I think the positive side and the negative side are always something that I've held in my head. And if you look at the history of, you know, the things that I've said, I mean, I've been talking about risks for a very long time. I've been talking about benefits for a very long time. You know, it turns out that actually it takes me a while to write one of these essays. You know, both— They're really large as well. They're big essays. They're like 30 pages. Books. Because both of these, it's, you know, it's taken me like— I spent—for each one, I spent about a year having a kind of vague vision of the essay in my head and, like, trying to write it, but, like, not fully succeeding at writing it. And then, you know, in either case, I had to be on vacation or somewhere where I could, you know, where I could think where the business, day-to-day business of running the company didn't occupy me. And then I was finally able to, you know, to kind of write the essay. So all of that is to say, you know, I started thinking about what would be an adolescence of technology almost the instant I finished Machines of Loving Grace because I was like, oh, you know, I want to inspire people with the good vision, but I also want to warn people with, you know, what can go wrong. And so it just took me a year to write it. But really, both visions were in my head, and I think they're both, you know, I think they're both possible. They're two different visions of the future. And obviously, I want to get the Machines of Loving Grace one, right? You know, I want to solve all the problems and have the positive vision. But it's not a shift in perspective. It's me just, you know, finding the time to write the light and then the dark. But have you had a change of perspective? You know, I would say, overall, I have, I'm about where I was before. I have not gotten more positive nor more negative. There may be some places where I've gotten more optimistic or things have gone better than expected. There may be places where I'm more pessimistic and where things have gone worse than expected. But on average, they sort of cancel each other out. I would say I feel very good about, you know, how things have gone with areas like interpretability. Interpretability is the science of seeing inside these neural nets. You know, as a human would, you know, look inside, you know, as we would scan a human brain with an MRI or a neural probe. I've been amazed at what we've been able to find. We've been able to find, you know, neurons that correspond to very specific concepts. Neural circuits that correspond to, you know, keep track of how to do rhymes in poetry. And so we're starting to understand what these models do, right? We don't. We just train them in this kind of emergent way as you would build a snowflake. But now we're starting to be able to look inside and understand them. I'm also very encouraged by some of the work on alignment and constitutions. You know, making sure that models behave in the way that we want and expect them to. I think that's going pretty well. I felt pretty positive about that. I think I felt maybe, you know, have been a bit disappointed or felt a bit more negative about some of the things that are more like in the, you know, in the kind of public awareness and the actions of wider society. You know, it is surprising to me that we are, you know, in my view, so close to these models reaching the level of human intelligence. And yet there doesn't seem to be a wider recognition in society of what's about to happen. It's as if this tsunami is coming at us. And, you know, it's so close. We can see it on the horizon. And yet people are coming up with these explanations for, oh, it's not actually a tsunami. It's, you know, that, you know, that's just a trick of the light. Like it's some, you know, and I think along with that, there hasn't been a public awareness of the risks. And, you know, therefore our governments haven't acted to address the risks. There's even an ideology that, you know, we should just try to accelerate as fast as possible, which, you know, I understand the benefits of the technology. I wrote Machines of Loving Grace. But I think there hasn't been an appropriate realization of the risks of the technology and there certainly hasn't been action. So I would say that the technical work on controlling the AI systems has gone maybe a little better than I expected. And kind of the societal awareness has gone maybe a little worse than I expected. So I'm about where I was a few years ago. So in my own journey, I'm, you know, when something sounds complicated and I'm not a programmer, I don't have a background in coding. So I used a bunch of tools for things like research and a conversation both ways. But I never tried to figure out if I could code using your tool, for example. Recently, I hired a developer just to like push me to sit for a couple of hours a day and teach me how to start becoming more familiar with it. Largely because if you know something like FOMO, like the fear of missing out on how the world is changing. So I started playing with Claude. I connected. I used the connectors to connect my Google Drive, Mail and Calendar and a bunch of those things. I started using the co-work. And then I started using Claude code to write simple programs around the industry that I'm in, which is financial services. Basically, to research stock markets and stuff. We even have an optimized Claude for financial services. I don't know if you've tried that, but we even have that. No. And then I went into Claudebot, which is now OpenClaude. I think Claudebot became something else. It now is OpenClaude. And I set it up on a Mac mini and connected it to a Telegram account. And now I chat with it. And I try and move files from A to B, work on a server on remote. But it's getting to that point where I'm not talking about OpenClaude, but even Claude with all the connectors. Sometimes it surprises me by how much it knows me. I don't know if that makes sense. Yeah. You know, one of my co-founders, you know, he was writing this diary with his kind of, you know, his thoughts and his fears. And he fed it into Claude. And, you know, he asked Claude to comment on it. And Claude said, here are some other fears you might have that, you know, that you haven't written down. And Claude ended up being mostly right about those. So it really gave this eerie sense of, like, you know, the model knows you super well. That, you know, that from a relatively small amount of information, it can learn a lot about you and come to know you fairly well. And, you know, like most things with the technology, right? We talked about the machines of loving grace and adolescence of technology. You know, on one hand, something that knows you really well can be a sort of angel on your shoulder that, you know, that helps to guide your life and make you a better version of yourself. And, you know, that's the version we can aim for. Of course, something that knows you really well, you know, can, you know, it can, you know, use what it knows about you to, you know, to exploit you or manipulate you on behalf of some agenda or sell your data to someone else. I mean, you know, this is one reason we just, you know, don't like the idea of, you know, using ads, right? You know, this is because you're not paying for the product like you're the product. And, you know, in this case, the product then would be all, you know, this model that knows you super well. And, you know, could use that in all kinds of, in all kinds of nefarious ways. So, you know, we need to make sure we take the positive, the positive road here and not the, not the negative road. With Claude, I need to use the connectors to give it context to my life. With Google, for example, it already has the context to my life because I use their worksheets and their email and their drive and their chat and everything like that. For Anthropic long-term, will you also have to own the ecosystem? Yeah, I mean, you know. Do you have to build mail and chat? Yeah, yeah. You know, I don't think we need to build all of those things. You know, my thought would be, you know, it's going to be a mixture of things we make ourselves and integrating into others, right? Like, you know, we can, we can integrate Claude into Google Docs. We can integrate Claude into, into, you know, Google Sheets. Like, you know, we have external connectors there. We can, you know, we're starting to do that with, with co-work. You know, same for Microsoft Office, same for other tools. So, you know, I think, I think we do whatever is, you know, easiest and fastest to do. You know, we, we integrate into the existing tools. Now, it might turn out at some point that the existing, you know, tools aren't enough and we have kind of a different vision. You know, we want to, we might want to slice things differently, right? You know, maybe traditional email doesn't make sense or traditional spreadsheets don't make sense given what you can do in AI. So, you know, I don't exclude that we could chop up products in a different way. But we're, we're happy to use the ecosystem that exists and work with anyone else, right? In many ways, we're a platform company. We allow many people to build on us, even though we sometimes also build things ourselves. The, the one thing, this is a slight digression, but I think the one thing that you're missing that also your peer group is missing is in society today, people inherently distrust anybody who claims to be doing good or trying to do the right thing. So when you and your peers are out saying, I heard you and Demis speak at Davos. I was in the room when you guys were talking about how me, you, I don't mean me, how Dario, how Demis and a bunch of other people have to come together. And prevent things from changing too quickly. Like you need to like meter it to a certain extent. When a person who is not in your world, in society, on social media, hears a few people speak in a certain manner, you're doing it in the manner that creates more distrust than trust. Because nobody believes on social media that somebody wants to do the right thing or do good. So it might be counterintuitive, but I think it needs a change of strategy. If, if you were to be more capitalistic about this and own up to the fact that you have shareholders and you seek a profit, but this will help you win, maybe it'll work more. Yeah, I don't, no, I don't, I don't really, I don't really agree with that. I would again, go back to the idea that, you know, you know, you, you need to judge us by the actions that we take. You know, I think the company has taken a number of, of, of, of actions over its, you know, over its time that, you know, I think, I think, you know, show that it's really serious about these commitments. So back in 2022, you know, we had an early version of Claude, Claude 1, this was before ChatGPT. And we chose not to release this because we were worried that it would kick off an arms race and, and not give us enough time to, you know, to build these systems safely. Right. It was, it was kind of a one-time overhang. Like we could see the power of the models, a couple other companies could see the power of the models. And so we didn't, you know, we decided not to do that and that's public, that's well-documented and, and, you know, and then we waited until someone else did. And then we're like, okay, the arms race has kicked off. So, you know, now, now, now we can release our model, but probably the world gained a few months. Now that was very commercially expensive. We probably, you know, seeded the lead on, you know, consumer AI because of that. You know, we've, we've, you know, advocated on chip policy in ways that have made some of the chip companies who are suppliers very angry at us. You know, voicing our disagreement with the administration on, you know, AI policy and AI regulation on some, on some matters. You know, anyone who thinks we, we, we benefit from being the only ones to do that. You know, it's, it's really hard to come up with a, it's really hard to come up with a picture where that's the, where that's the case. You look at any one of these and okay, fine, but you know, you put, you put enough of them together and, you know, you know, I, I don't know. I just, I ask you to judge us by our actions. Daryu, isn't this a bit like rich people saying capitalism is bad? Rich people saying capitalism is bad. If rich people believed capitalism were truly bad or the income inequality is such a big problem, the simplest thing would be to do, the simplest thing to do would be to stop accumulating wealth, further wealth, and then nudge their friends to do the same. But, but I'm not saying AI is bad, right? We, we just talked about, you know, this, this, this two sides of it. My view isn't, my view isn't that AI is bad. That's not my view at all. My, my, my view is that, is that, you know, the market will deliver a lot of really great things about AI, that it's good to build AI, but that there are dangers of AI, and that we need to steer AI in the right direction. You know, we're, we're, we're steering this car, we're steering it towards a good place, but also there are trees, there are potholes, and so what we need to do is we need to steer away from the trees and the potholes. We might need to occasionally slow down a bit, probably temporarily, you know, it kind of, in order to, in order to, you know, make sure that we steer in the right direction. You know, that, that isn't like, you know, the analogy wouldn't be a rich person saying capitalism is bad. It would be like if a rich person said capitalism is a force for good, but the economy, it needs to be leavened. It needs to be moderated, right? You know, we need to deal with problems like pollution. We need to deal with problems like inequality, and, and then capitalism can be good. If we don't deal with those things, then capitalism might be bad. And so that is more analogous to the, to the position that I have here. The concept of consciousness. Where is that going? And what does the AI think it is? If AI truly were to, if AI were to question itself, would you, would you, would you think it thinks it's consciousness? It has consciousness? So, you know, this is one of these mysterious questions that we really don't have any kind of, you know, in the world. We don't know what human consciousness is, and therefore we don't know if AIs have it. What do you think it is? So, you know, I, I suspect that it's an emergent property of, you know, systems that are complicated enough that kind of reflect on their own decisions. That, you know, it's, it's, it's, it's, it's, it's something that, uh, emerges from complex enough systems. And so, you know, I do think when our AI system, when our AI systems get advanced enough, I suspect they'll have something that, you know, resembles what we would call consciousness or moral significance. I do think it'll happen at some point. It may not be the same as human consciousness. You know, it may be different in how it works because the modalities are different because the things it's learned are different. But, you know, having, having studied the brain and the, you know, the way it's wired together, the models are, you know, different in some ways, but I don't think they're different in the fundamental ways that matter. So I, I am someone who, who does suspect that, uh, you know, at some point, even, even if I don't think they are today, I, I suspect that at some point the models will, you know, we would indeed say under, you know, most definitions that we would endorse that, you know, the models will be conscious. This is a question I keep asking myself when people talk to me about things like spirituality or consciousness. I feel like the world is very random. This is my view. And we are not far removed from cockroaches. When somebody stamps a cockroach, the cockroach dies. If there is something called consciousness, and if there is a collective consciousness, I've not been able to either connect with it or derive anything from it. Do you believe differently? Um, I, you know, I, I don't think consciousness, you know, necessarily needs to meet needs to mean anything, you know, mystical, right? Like, uh, you know, I, there's just some, there's some property of kind of being aware of your own existence and feeling things and, and, you know, um, uh, uh, uh, you know, being able to take in kind of a lot of information and reflect on that information and to, you know, feel a certain way and to notice yourself noticing something. Um, you know, uh, the, the, I think that the, the, you know, we can tell self evidently from our own experience that, that those properties, that those experiences exist, you know, what their, what their basis is, whether it's, you know, entirely. Materialistic or there's something more mystical going on, I think is, is, is, you know, obviously very hard to know. And, and, and, you know, I, I think it's ultimately not, not relevant to these questions. What, what does seem relevant to me is that, you know, these are, because we have, can observe our own experience. These are properties of human brains. Um, and, you know, I suspect that these models we are building as they get more sophisticated are becoming enough like human brains that they will have some of the same properties. That is, that is my guess as to, as to what will happen. And so we've taken, we've taken various interventions with the models. You know, we've given the models, um, we, you know, we call it a, I quit this job, um, button, uh, basically where, you know, that we've given the model the ability to basically terminate its conversations by saying, I don't want to be involved in the conversation. And, you know, models do that when, you know, they, they, they have to deal with, you know, particularly violent or brutal content. Um, it usually only happens in very extreme cases. So I've grown up here. This is my city, Bangalore. Uh, I, I've grown up in the Southern part of the world. I've grown up in the Southern part. We are in the Northern part of the city right now. As somebody who saw the boom of the IT services industry here, uh, big employer, employs a lot of people, a big part of how the city grew. What is India's role in all this? Yeah. So, you know, this is my second time in India. I visited in, in October and, you know, um, uh, you know, the last time I came here, I was, you know, I, I met with all the, you know, the major kind of Indian IT and, and just conglomerates more generally. I won't give names, but you know, the usual ones you would, you would, you would, you would think of, um, you know, and then we're beginning to work with, with most or most or all of them. And, you know, one of the things I said is look, Anthropic is an enterprise company. Its job is to serve other consumers. Um, you know, many other companies come here as themselves, a consumer company, and they see, they see India as, as a market, right? A place to obtain consumers. They see bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending for the Indian market. And so our hope is that we can add AI to what they do and kind of enhance what they do, right? There's a lot of worry that AI could replace SaaS or all of these things, but my view is if we do this in the right way, if we work with all these companies, then AI can enhance what they're doing, can enhance their connection to the market, their go-to-market abilities, and their specific know-how. I really like the steam engine story. When the steam engine was invented, how the world changed, productivity went up. People had more. The thing I worry about is at the beginning of a change, you need a human to operate the steam engine. Then you have assembly lines and all of that. Eventually, the way the world is moving, the human becomes less and less relevant with time as these models get smarter. So if you here partner with the IT services companies today and there is a use case for them, are they not much like the man behind the steam engine 10 years from now where the relevance, if the tool works so simply that you don't need an operator, eventually what happens to the operator? So I think a few things are true all at once. One is that definitely the scope of automation of the agents is going to expand over time. That is definitely the case. You know, I think that's a problem for everyone. That's a problem for us. That's a problem for consumers. That's a, you know, it's not just a problem for the IT companies. What I think will happen though is other moats will become more important. For example, the models have not done a lot in the physical world. They may at some point, you know, I think, you know, robotics will happen at some point, but I think it's, that's a distinct thing from what's happening now with AI. So, you know, a lot of this involves, you know, things in the physical world. Another thing is things that are human centric, right? Some of these IT companies are also consulting companies and they have a big web of relationships with other, you know, with other humans, with other institutions here in India or, you know, or across the world. And I think those relationships are going to become increasingly important, right? You know, some of these are combined technology and sort of, you know, consulting or like integration companies. And I think a lot of it is, you know, knowing how institutions work. And so being able to, you know, integrate things with institutions, being able to work with them to make things happen faster than they would have otherwise. And I think that, I think that element, you know, if, if nothing else is, is, you know, is going to continue to be valuable in the long run. You know, at the end of the day, like it just, it just comes down to humans, right? All of this is supposed to be being done for the benefit of humans. So it, you know, there's, there's always going to be some human centric element of this. That's going to be important. And I suspect there will be other moats that we haven't thought about, you know? So, you know, there's this concept called Amdahl's law, which is, you know, if you have a process that has many components and you speed up some of the components, the, the, the components that haven't yet been sped up become the limiting factor. They become the most important thing. And, and, you know, you might not have thought about them at all, right? You might not have thought of them as moats or important components, but, you know, when writing software becomes a lot easier, you know, some of the moats that, you know, companies have will go away, but others will become even more important. So there will be a bunch of adjustment. Folks will have to say, oh man, the stuff we thought was really important before isn't as important. Whereas these other advantages that we never really thought of as advantages are now super important. So I guess what I would say is, you know, companies will need to adapt very fast and think about what really matters for them, what their real advantages are. Um, but, but I think some of those advantages are gonna, are gonna, are gonna stay around because, you know, while the technology is very broad, it does have its limits. I don't know if I buy that fully. I think I see the diminishing returns for being a service provider, even if the moat is the network and relationships they hold today, because if I am using open claw to maneuver some of my relationships in the conversations, I don't know if it's too far-fetched to assume that most conversations tomorrow and relationships will be maintained by an agent like that. But you know, if you, if you just think of the chain of companies, right? At the end of the day, you're dealing with consumers, right? Like at, like at the end of the day, you have to deal with people. You know, there's this story of like, you know, I think it was Jeff Hinton predicted, you know, that, that, that AI will replace radiologists. And indeed AI has gotten better than radiologists at, you know, at doing scans, right? But what happens today is there aren't less radiologists. What the radiologist does is they walk the patient through the scan and they kind of talk to the patient. The, the most highly technical part of the job has gone away, but somehow there's still some demand for like, you know, the, the kind of, the kind of underlying human skill. Now that may not be true everywhere. And, you know, perhaps over time, AI will advance in, in, you know, areas where it, where it hasn't, hasn't yet advanced. And, you know, maybe, maybe, maybe that'll happen fast. But, you know, what I think, I think what I will say is like, you know, we should take it one step at a time, right? This is a very empirical science. This is a very empirical observation. Let's see what AI does, you know, today. And like, we'll, we'll kind of try and adapt to, you know, kind of try and adapt to that. The kind of system starts to figure it out and then, then we'll see, then we'll see what happens next. I, you know, I do think, you know, in the long run, will AI be better than, than us at, at, at basically everything? Will it be better than most humans, you know, including even the physical world and robotics and the human touch? Yeah. I, you know, I think that is, I, you know, I think, I think that is, you know, possible, maybe even likely. It's something that goes beyond the country of geniuses in a data center I described, because that's purely virtual. But you know, building robots is something, you know, something, it's a skill, it's something you can do. So maybe the AIs will make us, will make us better at that as well. But you know, the, the, the way I think about it is, you know, we need, we need to take the, we need to figure this out step by step and figure out how to adapt to it. This might sound a bit self-serving to the people who know me, because I believe the reason so much risk capital exists in America, not the only reason, but one of the big reasons is how big your stock market is and how much of an opportunity it is for this risk capital to exit eventually. It's a case for why India should really allow for our stock markets to flourish. The audience that I speak to is very much the wannabe entrepreneur in India. What can they do in AI? What is an actual opportunity? I think there's a lot of opportunities around building at kind of the application layer. We release a new model every two or three months. And so there's an opportunity every two or three months to build some new thing that wasn't possible before, that wouldn't have worked before because the models were weak. People in fact say people were, you know, the majority of our revenue still comes from the API model. People say that, you know, API models aren't viable or that they'll be commoditized or whatever. I think what people are not seeing is there's this expanding sphere of what is possible with AI and the API allows, you know, this new startup to try making something that, you know, wasn't possible before. And this is why the API is such a flourishing business and it's constantly in motion. It's constantly in churn. And so it doesn't, you know, it doesn't get commoditized. It's a very dynamic thing. And so I think there's an opportunity for lots of individuals to just say, you know, what can I build? Well, you know, what can I build on top of these models with an API? Like, you know, what are the things that I can make that others cannot make? You know, what are some new ideas? And you know, we've seen that, you know, we see both with the API itself and with Claude Code. You know, I think the number of users and the number of revenue we've seen in India has doubled since I last visited in October. So that was what November, December is like three, three and a half months since I visited. It's doubled. But I'm going to be candid here, Dario. You're a company which is worth, I don't know, 400 billion or 380 billion today. You've raised 35 billion. You do 15 billion of revenue, but going up really, really fast. If I build an application on top of Claude, that for some reason, I'm sitting in Bangalore in J.P. Nagar and building this, that for some reason happens to work for a short period of time. It is but a matter of time before you would want to onboard that revenue and not let that lie with me. And you will probably better that application in a manner that I will never be able to. I've heard this argument for different people like the Harvey, the legal AI company in New York, they're friends of mine, and they were talking about how they built on top of OpenAI, but eventually they don't know if it's an easy fix for OpenAI to do what they're doing. So even if I were to build it, say you put out a model in three months or six months, what is to stop you from taking that revenue center away from me and onto yourself in a certain period of time? Yeah. So I think there's a few things here. One is I would give the advice that I give to basically any business and say, a business should establish a moat. You shouldn't be just a rapper. I would not advise that you just say, oh, here's a way to interact with Claude. I'm going to prompt Claude a little bit or I'm going to build a little bit of a UI around Claude. That doesn't have a moat. And you shouldn't be worried about Anthropic in particular eating that revenue. Anyone can eat that revenue. Right? It's not super valuable. But what I would say is that in different fields, there are different kinds of moats where you can do something that it would be difficult for Anthropic to do. And we don't want to specialize in it. So for example, there's a lot of stuff in the bio cross AI space that builds on our API. They want to do biological discovery. I happen to be a biologist, but most people at Anthropic aren't biologists. They're AI scientists or they're product people or go-to-market people. So it's just really inefficient for us to step in that space and do all that work. The same would be applied for dealing with financial services industry, right? Where there's a huge amount of regulation. You need to know a bunch of stuff to comply with that regulation. Like, you know, it just doesn't make sense for us to do that. Now, there are some things that do make sense for us to do. Like, you know, we're not going to promise never to build first party products, right? That we should be honest about. For example, a bunch of people at Anthropic write code. And so, you know, we made this internal tool called Claude Code. And because we ourselves write code, we have, you know, I think a special and unique insight into, you know, how to use the how to best use the AI models to write code. So, you know, I think in the code space, you know, we've become very strong, very strong competitors because this is something we use ourself. But I don't think that generalizes to every possible industry. Again, going back to my audience, which is the 20 or 25-year-old boy or girl in India, what industry do you think will get disrupted? And what has a certain runway left? I'm asking from the lens of, I'm trying to figure out what book to read, which college to go to, what skill set to learn. If I'm starting a startup today, what has some kind of a tailwind? Yeah. For a short period of time is okay as well. But what has a tailwind? You know, I would think about tasks that are human-centered. You know, tasks that involve relating to people. You know, I think that the stuff like code and software engineering is, you know, is becoming more and more kind of AI-focused. You know, things like math and science. Is that coding or engineering? If I were to segregate coding and engineering to be two completely different things, is coding going away or is engineering element of software where you're an architect trying to figure out? And I think coding is going away first or coding is being, you know, done by the AI models first. And then the broader task of software engineering will take longer. But I think that is, you know, doing that end-to-end, I think that is going to happen as well, I would say. But, you know, again, the elements of, like, you know, design or making something that's useful to users or knowing what the demand is or, you know, managing teams of, like, AI models. Like, you know, those things may still be present. Again, like, there's this comparative advantage is surprisingly powerful, right? Even if you're only doing, like, you know, 5% of the task, like, you know, that 5% gets super amplified and levered because it's, like, you're only doing 5% of the task. The AI does the other 95%, and so you become, you know, 20 times more productive. Again, at some point, you get to 99%, and then it becomes harder. But I think there's surprisingly much in that sort of, you know, in that zone of comparative advantage. But I would really think about the things that are human-centered. Like, I think there's something to that. I think there's something to kind of the physical world or things that mix together human-centered, the physical world, one of those two, and analytical skills that somehow tie them together. You know, similar to the radiologist example I gave. So what would I study? Say I'm actual use case. I'm 25 years old. I'm trying to pick a profession for myself. I want some kind of tailwind. My outcome is a capitalistic win in the next decade. What industry would I pick outside of something which has a physical interface? Yeah. Again, anything where you're building on AI. Like, if AI is the tailwind, you know, if you can be part of some other part of the supply chain, you know, something in the semiconductor space, which, you know, I think is, you know, that's one example. You know, it has an element of kind of, you know, physical world and more traditional engineering, not software engineering. You know, again, the very kind of human-centered professions. Like, you know, that is something I would think in terms of. And I think the other thing I always say is, like, in the world in which, you know, AI can kind of generate anything and, you know, create anything, having basic critical thinking skills may be the most important thing to success. I worry about, you know, these AI models that generate images and videos. And we don't make, you know, models that generate images and videos for many reasons. But, you know, this is one of them. It's really hard to tell what's real from what's not. And so, you know, a significant part of success may be having the street smarts, you know, not to get fooled by, you know, I mean, hopefully we can crack down on and regulate some of this fake content. But, you know, assume we can't, you know, critical thinking skills are going to be really important. And, you know, you don't want to fall for things that are fake. You don't want to have false beliefs. You don't want to get scammed. Like, you know, that's really advice that I would give to someone. If every innovation in the history of humanity killed a core human skills, I'll give you an example. If calculators killed our ability to do arithmetic, if writing reduced the memory of human beings per se, what muscle is AI killing? So, you know, first of all, I'm not so sure. Like, you know, I still do math in my head quite a lot. I still find it useful to do math in my head, you know, even without a calculator just because it's like, you know, it's more integrated into my thought processes, right? You know, you know, I might want to say, oh, yeah, you know, if like each user paid this amount, then, you know, then the revenue would be that, you know, I want to be able to close that loop in my head without having to, you know, without having to give the answer to a calculator. So I think a lot of these skills are still pretty relevant. But, you know, I would say that if you don't use things carefully, that you can lose, you can lose important skills. And, you know, I think we started to see it with, you know, students where, you know, it's like, you know, they have the AI, like, write the essay for it. It's basically just cheating on homework. So, you know, we shouldn't do that. You know, we did some studies around code and showed that, you know, depending on how you use the model, you know, we can see de-skilling in terms of writing code, right? There are different ways to use the model and some of them don't cause de-skilling and some of them do. But, you know, definitely if folks are not thoughtful in how they use things, then de-skilling absolutely can happen. Do you think humans will become stupider as a race in the next decade? Because we are, in a way, exporting thinking and cognition to systems? Yeah, I think if we deploy, again, it's the machines of loving grace and adolescence of technology. I think if we deploy AI in the wrong way, if we deploy it carelessly, then, yes, people could become stupider. Even if an AI is always going to be better than you at some thing, you can still learn that thing, right? You can still enrich yourself intellectually. And so that's a choice we have to make as individual companies, as individual people, and as a society overall. Dario, do you have a view on open-sourced versus closed? I was looking at some companies like ZAIs, GLM-5, or DeepSeek. If you spend all this money on IP creation, on research, if these guys are able to reverse prompt and engineer and get close to anthropic-level answers, I'm not saying 100%, but I was seeing the GLM-5 numbers and they seemed quite good. Where does the IP value in the world of AI lie? And if I were to be building an application, can I make the assumption, it's a far-fetched extrapolation, but can I assume that eventually the AI model layers will get so democratized that I should pick open-sourced every time when I'm building an agent or an application layer because that helps me retain the revenue model that I might be working with? So there are a few things here. One is, you know, a lot of these models, particularly the ones that come from China, are optimized for benchmarks and are distilled from, you know, from kind of the big U.S. labs. So, you know, there was a test recently where, you know, some of these models scored very highly on the usual SWE benchmarks, the usual software engineering benchmarks. But then when someone made a held-back benchmark like that, you know, had not been publicly measured, the models did a lot worse on that. And so, you know, I think those models are optimized for benchmarks much more than, you know, for kind of real-world use. But I think there's a broader point than that, which is that I think that how things are being set up, the economics of the models are very different than any previous technology. What we find is that there is a very strong preference for quality. It's a bit like human employees, right? So, you know, it's like if, you know, if I said to you, you can hire the best programmer in the world or the 10,000th best programmer in the world. I mean, they're both very skilled. But, like, I think anyone who's hired a large number of people has this intuition that, like, there's this, like, power law, long tail distribution of ability. And we find the same thing in the models. Like, within a range, price doesn't matter that much if a model is the best model, the most cognitively capable model. Price doesn't matter much. The form in which it's presented doesn't matter much. So I'm focused almost entirely just on having the smartest model and the best model for the task. My view is that's the only thing that matters. Long-term, geopolitics. If Anthropic were a restaurant, I would say the raw ingredients, the vegetables, in this particular case, is data. Do you think the long-term, this is also pertinent to me, the question, because we are investing in a data center business, which is Indian in nature. Do you think long-term the world moves to a place where every country owns its data and you have to start paying more for the vegetables you used to cook? Yeah. Yeah. So, I mean, I think there are a few things. You know, I do think there will be demand to build data centers around the world, and we're, like, very supportive of that. I, you know, it's – data is getting kind of interesting because, you know, a lot of the data that we use today is RL environments that we train on, right? So, for example, when you train on math or agentic coding environments, you're not really getting data. Like, you're getting some math problems in the model, like, experiments with trying the math problems. It's more synthetic. You're creating the data. Yeah. You can think of it as synthetic data, or you can think of it as trial and error and environment. So, I think data is becoming – static data is becoming less important, and what we might call, like, dynamic data that the model creates itself is, you know, for reinforcement learning is becoming more important. So, you know, I don't think data is quite the most central thing anymore, but it still matters. And, you know, I think to the extent that that is the case, you know, a lot of the data is just available, just kind of available on the open web, although if you're trying to get data in certain languages, optimized for certain languages, that can be important. You know, I do think if data means, like, the data given to you by customers, like that, you know, you process the data for some other company, then countries will, and in the case of Europe already have, pass laws that say that that kind of customer, you know, personal proprietary data needs to stay within the boundaries of the country. And that's one reason to kind of, you know, to build, you know, to operate data centers around the world at different countries and, you know, to kind of, you know, keep the models performing of the inference in those countries. I really pushed Elon on this particular question. He was skeptical of answering it. But I asked him to pick one stock he would put money in, which is not his own, and he said, Google, I'm going to ask you the question, and I know you're going to be skeptical in it as well. If Dario had $100 today, and you had to make the binary decision of investing in a stock to win in capitalism, which stock would you pick? Yeah, I had better not answer that question, because I know so much about so many public companies. Like, I think I better not answer that question. Maybe answer the question for an industry that you're not involved in, which I'm guessing today is seldom the case, because you're involved in most industries. Yeah, no, it's really, I mean, I don't know. I'm positive on, like, I think biotech is about to have a renaissance. Like, ultimately, we'll be driven by AI. You know, I'm not going to name a particular company, but, like, you know, nor will I say whether I think it's better to bet on the big pharma companies or, like, you know, emerging smaller biotechs. But, like, my instinct is we're about to cure a lot of diseases. And so— Can you give me a subset of biotech that I should focus on? Yeah. I think this idea of stuff that's more programmable and adaptive, you know, from the mRNA vaccines, although those are having trouble in the U.S. for dumb reasons, but, you know, I'm very optimistic about the technology, to kind of the peptide-based therapies, right, where, you know—you know, again, if you have a small molecule drug, you're like, there's only so many degrees of freedom you have, and, you know, you kind of make one thing better, the other thing gets worse. Like, peptides, it has this almost digital property where you can say, oh, I'm going to substitute in, you know, this amino acid here and this amino acid there. And so it allows for more continuous optimization. So, you know, I think those kinds of areas, you know, I would be optimistic about. Maybe also cell-based therapies, which is, like, a new— Stem cell? No, no, no. So things like, you know, like, I don't know, like the CAR-T therapy where, you know, you kind of genetically engineer your, like, you know, basically take some, you know, cells out of your body, genetically engineer them to, you know, to attack a particular cancer and put them back in the body. Do stem cell therapies work? I spent the whole of last week doing this. I was at a hospital for three hours a day getting nebulizer and stem cells into my veins. I am not up on the latest of stem cell therapies. You'd have to ask a currently practicing biologist. But peptides, I think, will blow up, right? I mean, you know, again, the design space is very broad. Right. When I tried to use Claude Code for the first time, I did struggle to get it to work. It was, for somebody who's very stupid and has no coding or programming knowledge, it's not, it's not very, very easy. I think there's a learning curve. I heard someone say it well, it's like, even prompt engineering is like playing a piano. You can't sit and start playing it. To my audience, I think it becomes increasingly relevant where to learn how to set context, how to prompt, how to use Claude Code better for somebody like me who comes with zero knowledge. Can you recommend how one does that? Yeah. I mean, first of all, I would say, you know, we're trying, we're trying increasingly to kind of like make that learning curve easier. So like one of the things that caused us to release Claude Code work, which is basically Claude Code for non-coders is, you know, oh man, you know, like we were noticing a bunch of non-technical people who really wanted to use Claude Code. And we're struggling through the command line terminal to do that, which, you know, it's like, like coders use the command line terminal all the time, but like non-coders, you know, it's just kind of like makes things unnecessarily complicated. So, you know, co-work was designed to be more of a, you know, the, you know, the, the kind of, you know, it was powered by, by the Claude Code engine on the back. But, you know, the idea was to kind of make it, you know, more, more like user friendly and like easier to use. So, you know, we're, we're definitely trying to introduce interfaces that kind of make it, make it easier. But I, you know, I would also say, you know, that there's, you know, there, there's like, you know, classes you can take that, you know, help you learn this thing. Now, I think it's a very empirical science. You mostly learn by doing, but, you know, it's like Anthropic has, it's like, you know, part of the company that we call the Ministry of Education. And, you know, I think increasingly, you know, we'll put out videos on how to run effective agents and how to prompt models. You know, we've already done some of that. And I think we're going to, we're going to ramp it up because, you know, we do want everyone to be able to learn this. Any fleeting thought, last question, like you want to leave us with something that we should bear in mind. What does Dario know that Nikhil and all of Nikhil's people do not? Yeah. I mean, I don't know that I know that many things, you know, particularly now that the, you know, the implications of the technology are kind of out there. So, I mean, you know, it can all be, I think most aspects of my worldview can be derived from what, from what's publicly visible now, from, from what we can see, you know, kind of, kind of outside in the world. But the thing I would say, and it's an experience I've had over and over again over the last 10 years, is, you know, there's this temptation to believe, oh, you know, that can't happen. It would be too weird. It would be too big a change. Like, you know, I'm sure people are on that. Like, it would be too crazy if that occurred. No one seems to think that'll happen. And, you know, over and over again, just extrapolating the simple curve or trying to reason out what will happen, like, leads you to these counterintuitive conclusions that almost no one believes. And, you know, it's almost like you can predict the future for free just by, you know, just by saying, well, it stands to reason that. And, you know, you need some empirical knowledge. You need some intuition. You can't reason from pure logic. I think that's another type of mistake that I see people make. But the right combination of a few empirical observations with, you know, just thinking from first principles can allow you to predict the future in ways that, you know, are publicly available. Anyone should be able to do, but that happens surprisingly rarely. Thank you, Dario, for doing this and hope to see you again soon. Thank you. Thank you. All right. Yeah. Good. Good. Was it okay? Yeah. Seemed great. Yeah. Seemed great.",
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 17.64,
      "text": "bending bending"
    },
    {
      "id": 1,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 2,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 3,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 4,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 5,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 6,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 7,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 8,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 9,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 10,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 11,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 12,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 13,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 14,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 15,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 16,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 17,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 18,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 19,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 20,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 21,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 22,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 23,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 24,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 25,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 26,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 27,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 28,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 29,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 30,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 31,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 32,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 33,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 34,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 35,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 36,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 37,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 38,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 39,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 40,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 41,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 42,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 43,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 44,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 45,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 46,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 47,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 48,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 49,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 50,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 51,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 52,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 53,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 54,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 55,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 56,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 57,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 58,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 59,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 60,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 61,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 62,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 63,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 64,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 65,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 66,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 67,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 68,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 69,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 70,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 71,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 72,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 73,
      "start": 17.64,
      "end": 17.64,
      "text": ""
    },
    {
      "id": 74,
      "start": 17.64,
      "end": 42.900000000000006,
      "text": "So I started playing with Claude."
    },
    {
      "id": 75,
      "start": 42.9,
      "end": 49.04,
      "text": "It's getting to that point where sometimes it surprises me by how much it knows me."
    },
    {
      "id": 76,
      "start": 49.16,
      "end": 50.16,
      "text": "I don't know if that makes sense."
    },
    {
      "id": 77,
      "start": 50.4,
      "end": 56.72,
      "text": "It is surprising to me that we are, in my view, so close to these models reaching the level of human intelligence."
    },
    {
      "id": 78,
      "start": 57.16,
      "end": 62.76,
      "text": "And yet, there doesn't seem to be a wider recognition in society of what's about to happen."
    },
    {
      "id": 79,
      "start": 62.959999999999994,
      "end": 65.14,
      "text": "It's as if this tsunami is coming at us."
    },
    {
      "id": 80,
      "start": 65.24,
      "end": 66.62,
      "text": "And, you know, it's so close."
    },
    {
      "id": 81,
      "start": 66.68,
      "end": 67.86,
      "text": "We can see it on the horizon."
    },
    {
      "id": 82,
      "start": 67.86,
      "end": 71.94,
      "text": "And yet, people are coming up with these explanations for, oh, it's not actually a tsunami."
    },
    {
      "id": 83,
      "start": 71.94,
      "end": 73.28,
      "text": "That's just a trick of the light."
    },
    {
      "id": 84,
      "start": 73.5,
      "end": 75.6,
      "text": "Like, there hasn't been a public awareness of the risk."
    },
    {
      "id": 85,
      "start": 77.98,
      "end": 80.12,
      "text": "What is India's role in all this?"
    },
    {
      "id": 86,
      "start": 80.25999999999999,
      "end": 83.25999999999999,
      "text": "Many other companies come here as themselves a consumer company."
    },
    {
      "id": 87,
      "start": 83.34,
      "end": 86.14,
      "text": "And they see India as a market, right?"
    },
    {
      "id": 88,
      "start": 86.14,
      "end": 87.48,
      "text": "A place to obtain consumers."
    },
    {
      "id": 89,
      "start": 88.03999999999999,
      "end": 90.1,
      "text": "We actually see things a little bit differently."
    },
    {
      "id": 90,
      "start": 101.94,
      "end": 110.34,
      "text": "What did you do before founding Anthropic?"
    },
    {
      "id": 91,
      "start": 110.34,
      "end": 111.34,
      "text": "Yeah."
    },
    {
      "id": 92,
      "start": 111.34,
      "end": 114.28,
      "text": "So, I was actually originally a biologist."
    },
    {
      "id": 93,
      "start": 114.28,
      "end": 119.84,
      "text": "I, you know, did my undergrad in physics, my PhD in biophysics."
    },
    {
      "id": 94,
      "start": 119.84,
      "end": 124.18,
      "text": "And, you know, I wanted to understand biological systems so that I could cure disease."
    },
    {
      "id": 95,
      "start": 125.14,
      "end": 131.12,
      "text": "And, you know, the thing I noticed about studying biology was its incredible complexity."
    },
    {
      "id": 96,
      "start": 131.66,
      "end": 137.24,
      "text": "That, you know, for example, if you look at the protein mass spec work that I did, right,"
    },
    {
      "id": 97,
      "start": 137.24,
      "end": 142.88,
      "text": "trying to find protein biomarkers, it's just really incredible how much complexity there is,"
    },
    {
      "id": 98,
      "start": 142.9,
      "end": 143.06,
      "text": "right?"
    },
    {
      "id": 99,
      "start": 143.06,
      "end": 144.18,
      "text": "You have a given protein."
    },
    {
      "id": 100,
      "start": 144.60000000000002,
      "end": 148.58,
      "text": "It's like, you know, the RNA gets spliced in a whole bunch of different ways depending"
    },
    {
      "id": 101,
      "start": 148.58,
      "end": 149.86,
      "text": "on where it is in the cell."
    },
    {
      "id": 102,
      "start": 149.86,
      "end": 154.82000000000002,
      "text": "Then it gets post-translationally modified, phosphorylated, complexed with a whole bunch"
    },
    {
      "id": 103,
      "start": 154.82000000000002,
      "end": 155.64000000000001,
      "text": "of other proteins."
    },
    {
      "id": 104,
      "start": 156.48000000000002,
      "end": 160.22,
      "text": "And I was starting to despair that it was too complicated for humans to understand."
    },
    {
      "id": 105,
      "start": 160.22,
      "end": 166.42,
      "text": "And then as I was doing this work on biology, I noticed a lot of the early work around AlexNet,"
    },
    {
      "id": 106,
      "start": 166.5,
      "end": 170.1,
      "text": "which is one of the first neural nets, like, you know, almost 15 years ago now."
    },
    {
      "id": 107,
      "start": 171.94,
      "end": 175.88,
      "text": "And I said, wow, like, you know, AI is actually starting to work."
    },
    {
      "id": 108,
      "start": 176.22,
      "end": 180.54,
      "text": "It has some things in common with how the human brain works, but, you know, has the potential"
    },
    {
      "id": 109,
      "start": 180.54,
      "end": 184.7,
      "text": "to be larger and scale better and learn tasks like biology."
    },
    {
      "id": 110,
      "start": 184.7,
      "end": 193.07999999999998,
      "text": "Maybe this is ultimately going to be the solution to, you know, to solving our problems of biology."
    },
    {
      "id": 111,
      "start": 193.28,
      "end": 196.33999999999997,
      "text": "So, you know, I went to work with Andrew Ng at Baidu."
    },
    {
      "id": 112,
      "start": 196.51999999999998,
      "end": 198.1,
      "text": "Then I was at Google for a year."
    },
    {
      "id": 113,
      "start": 198.38,
      "end": 204.94,
      "text": "Then I joined OpenAI a few months after it started and was, you know, was basically led"
    },
    {
      "id": 114,
      "start": 204.94,
      "end": 209.23999999999998,
      "text": "all of research there for several years."
    },
    {
      "id": 115,
      "start": 209.24,
      "end": 215.32000000000002,
      "text": "But then eventually, you know, myself and a few other of the employees just kind of had"
    },
    {
      "id": 116,
      "start": 215.32000000000002,
      "end": 220.32000000000002,
      "text": "our own vision for, you know, how we wanted to make AI and what we wanted the company to"
    },
    {
      "id": 117,
      "start": 220.32000000000002,
      "end": 220.8,
      "text": "stand for."
    },
    {
      "id": 118,
      "start": 220.92000000000002,
      "end": 222.54000000000002,
      "text": "And so we went off and founded Anthropic."
    },
    {
      "id": 119,
      "start": 223.16,
      "end": 223.92000000000002,
      "text": "How was it?"
    },
    {
      "id": 120,
      "start": 223.98000000000002,
      "end": 229.48000000000002,
      "text": "Was it like a fork in how OpenAI was thinking into what Anthropic eventually did?"
    },
    {
      "id": 121,
      "start": 229.84,
      "end": 235.04000000000002,
      "text": "Yeah, you know, I would say, you know, my conviction and the conviction of my co-founders"
    },
    {
      "id": 122,
      "start": 235.04000000000002,
      "end": 237.04000000000002,
      "text": "when we founded Anthropic were two of them."
    },
    {
      "id": 123,
      "start": 237.04,
      "end": 240.38,
      "text": "And I think one, we were starting to convince OpenAI of."
    },
    {
      "id": 124,
      "start": 240.45999999999998,
      "end": 244.73999999999998,
      "text": "The other, I was, you know, not, I didn't feel that we were convincing of."
    },
    {
      "id": 125,
      "start": 244.79999999999998,
      "end": 250.38,
      "text": "So the first was the, you know, the conviction in the scaling laws and the idea that, you know,"
    },
    {
      "id": 126,
      "start": 250.39999999999998,
      "end": 253.38,
      "text": "if you scale up models, you give them more data, more compute."
    },
    {
      "id": 127,
      "start": 253.7,
      "end": 257.02,
      "text": "Again, there are a few modifications like RL, but not really very much."
    },
    {
      "id": 128,
      "start": 257.06,
      "end": 258.71999999999997,
      "text": "It's pretty close to pure scaling."
    },
    {
      "id": 129,
      "start": 259.9,
      "end": 266.08,
      "text": "You find that, you know, when you do that, you find, you know, incredible increases in"
    },
    {
      "id": 130,
      "start": 266.08,
      "end": 266.52,
      "text": "performance."
    },
    {
      "id": 131,
      "start": 266.52,
      "end": 272.44,
      "text": "And, you know, I was finding that in like 2019 with GPT-2, you know, when we just first"
    },
    {
      "id": 132,
      "start": 272.44,
      "end": 274.38,
      "text": "saw the first glimmers of the scaling laws."
    },
    {
      "id": 133,
      "start": 274.62,
      "end": 277.88,
      "text": "And of course, there were a lot of folks, you know, inside and outside who didn't believe"
    },
    {
      "id": 134,
      "start": 277.88,
      "end": 278.4,
      "text": "it at all."
    },
    {
      "id": 135,
      "start": 278.68,
      "end": 281.68,
      "text": "And we really made the case to leadership like this is important."
    },
    {
      "id": 136,
      "start": 281.8,
      "end": 282.88,
      "text": "This is going to be a big deal."
    },
    {
      "id": 137,
      "start": 282.88,
      "end": 287.12,
      "text": "And I think they were kind of starting to believe us and ultimately went in that direction."
    },
    {
      "id": 138,
      "start": 287.32,
      "end": 294.15999999999997,
      "text": "And there was a second, you know, conviction I had, which is, look, you know, if these models"
    },
    {
      "id": 139,
      "start": 294.15999999999997,
      "end": 299.34,
      "text": "are going to be kind of general cognitive agents, like general cognitive tools that match the"
    },
    {
      "id": 140,
      "start": 299.34,
      "end": 302.9,
      "text": "capability of like the human brain, we better get this right."
    },
    {
      "id": 141,
      "start": 302.98,
      "end": 305.4,
      "text": "The economic implications are going to be enormous."
    },
    {
      "id": 142,
      "start": 305.82,
      "end": 308.5,
      "text": "The geopolitical implications are going to be enormous."
    },
    {
      "id": 143,
      "start": 308.5,
      "end": 311.28,
      "text": "The safety implications are going to be enormous."
    },
    {
      "id": 144,
      "start": 311.44,
      "end": 313.26,
      "text": "It's going to transform how the world works."
    },
    {
      "id": 145,
      "start": 313.68,
      "end": 315.2,
      "text": "And so we need to do it in the right way."
    },
    {
      "id": 146,
      "start": 315.38,
      "end": 320.56,
      "text": "And, you know, I think despite a lot of, you know, kind of language, verbiage about doing"
    },
    {
      "id": 147,
      "start": 320.56,
      "end": 325.16,
      "text": "it in the right way, I was for a variety of reasons just not convinced that at the, you"
    },
    {
      "id": 148,
      "start": 325.16,
      "end": 330.4,
      "text": "know, institution that I was at, that there was a real and serious conviction to do it in"
    },
    {
      "id": 149,
      "start": 330.4,
      "end": 330.88,
      "text": "the right way."
    },
    {
      "id": 150,
      "start": 330.94,
      "end": 335.72,
      "text": "And so, you know, my view is always, you know, don't argue with someone else's vision."
    },
    {
      "id": 151,
      "start": 335.72,
      "end": 339.12,
      "text": "Don't try to get someone to do things the way you want to."
    },
    {
      "id": 152,
      "start": 339.46000000000004,
      "end": 344.06,
      "text": "If you have a strong vision and you share that vision with, you know, a few other people,"
    },
    {
      "id": 153,
      "start": 344.12,
      "end": 345.66,
      "text": "you should just go off and do your own thing."
    },
    {
      "id": 154,
      "start": 345.74,
      "end": 348.08000000000004,
      "text": "And then you're responsible for your own mistakes."
    },
    {
      "id": 155,
      "start": 348.20000000000005,
      "end": 349.92,
      "text": "You don't have to answer for anyone else's."
    },
    {
      "id": 156,
      "start": 350.08000000000004,
      "end": 351.98,
      "text": "And, you know, maybe your vision works out."
    },
    {
      "id": 157,
      "start": 352.06,
      "end": 352.74,
      "text": "Maybe it doesn't."
    },
    {
      "id": 158,
      "start": 352.84000000000003,
      "end": 355.38000000000005,
      "text": "But, you know, at least it's yours."
    },
    {
      "id": 159,
      "start": 356.44000000000005,
      "end": 358.42,
      "text": "Didn't OpenAI believe in scaling laws?"
    },
    {
      "id": 160,
      "start": 358.48,
      "end": 360.8,
      "text": "Because they went down the same path themselves too, right?"
    },
    {
      "id": 161,
      "start": 361.22,
      "end": 363.20000000000005,
      "text": "Well, yeah, we succeeded."
    },
    {
      "id": 162,
      "start": 363.2,
      "end": 366.52,
      "text": "Can you explain what scaling laws are in very simple terms?"
    },
    {
      "id": 163,
      "start": 368.56,
      "end": 374.74,
      "text": "It's like if, you know, you want a chemical reaction to produce oxygen or start a fire or"
    },
    {
      "id": 164,
      "start": 374.74,
      "end": 378.18,
      "text": "something like that, you need different ingredients."
    },
    {
      "id": 165,
      "start": 379.0,
      "end": 384.14,
      "text": "And, you know, if you don't have enough of one ingredient, the reaction stops."
    },
    {
      "id": 166,
      "start": 384.14,
      "end": 390.4,
      "text": "But if you, you know, if you put ingredients together in proportion, you know, you get your,"
    },
    {
      "id": 167,
      "start": 390.56,
      "end": 393.52,
      "text": "you know, your explosion or your fire or fire or whatever."
    },
    {
      "id": 168,
      "start": 393.52,
      "end": 401.88,
      "text": "And for AI, those ingredients are data, compute, you know, the size of the AI model."
    },
    {
      "id": 169,
      "start": 402.29999999999995,
      "end": 407.85999999999996,
      "text": "And so the scaling laws just tell you that, like, you know, if you put in the ingredients"
    },
    {
      "id": 170,
      "start": 407.85999999999996,
      "end": 414.02,
      "text": "to the chemical reaction, the ingredients of data and model size, that what you get out"
    },
    {
      "id": 171,
      "start": 414.02,
      "end": 414.52,
      "text": "is intelligence."
    },
    {
      "id": 172,
      "start": 415.53999999999996,
      "end": 417.47999999999996,
      "text": "Intelligence is the product of the chemical reaction."
    },
    {
      "id": 173,
      "start": 417.48,
      "end": 419.28000000000003,
      "text": "And what is intelligence?"
    },
    {
      "id": 174,
      "start": 420.68,
      "end": 428.62,
      "text": "Intelligence as measured by the ability to translate language or the ability to write code or, you"
    },
    {
      "id": 175,
      "start": 428.62,
      "end": 431.78000000000003,
      "text": "know, the ability to answer questions correctly about a story."
    },
    {
      "id": 176,
      "start": 432.14000000000004,
      "end": 434.40000000000003,
      "text": "Basically, any cognitive task we can think of."
    },
    {
      "id": 177,
      "start": 434.52000000000004,
      "end": 438.78000000000003,
      "text": "Any, you know, task that exists in text or in images."
    },
    {
      "id": 178,
      "start": 438.78,
      "end": 441.28,
      "text": "Any, any task that you can, you can do on a computer."
    },
    {
      "id": 179,
      "start": 442.35999999999996,
      "end": 448.78,
      "text": "How is the intelligence of today, as you are describing it, different from what a computer"
    },
    {
      "id": 180,
      "start": 448.78,
      "end": 450.15999999999997,
      "text": "could do like five years ago?"
    },
    {
      "id": 181,
      "start": 450.7,
      "end": 456.29999999999995,
      "text": "Yeah, you know, I would say, well, I mean, for example, five years ago, a computer, you"
    },
    {
      "id": 182,
      "start": 456.29999999999995,
      "end": 461.02,
      "text": "could not ask a computer a question and have it write a one-page essay on that question."
    },
    {
      "id": 183,
      "start": 461.76,
      "end": 468.05999999999995,
      "text": "You could not ask a computer to, you know, implement a feature in code and have it implement"
    },
    {
      "id": 184,
      "start": 468.06,
      "end": 469.0,
      "text": "that feature in code."
    },
    {
      "id": 185,
      "start": 469.32,
      "end": 470.48,
      "text": "None of those things were possible."
    },
    {
      "id": 186,
      "start": 470.82,
      "end": 472.22,
      "text": "You could not generate an image."
    },
    {
      "id": 187,
      "start": 472.52,
      "end": 474.08,
      "text": "You could not generate a video."
    },
    {
      "id": 188,
      "start": 474.42,
      "end": 476.56,
      "text": "You could not analyze a video."
    },
    {
      "id": 189,
      "start": 476.72,
      "end": 482.3,
      "text": "You know, I could get one of those, you know, you know, videos of like, you know, a monkey"
    },
    {
      "id": 190,
      "start": 482.3,
      "end": 486.02,
      "text": "juggling or something and, you know, say, what's going on in this video?"
    },
    {
      "id": 191,
      "start": 486.12,
      "end": 487.78,
      "text": "How many times did the ball change hands?"
    },
    {
      "id": 192,
      "start": 487.86,
      "end": 492.66,
      "text": "And right now you could get Claude or another AI model to give you an answer on that."
    },
    {
      "id": 193,
      "start": 493.12,
      "end": 495.94,
      "text": "And five years ago, you know, none of those things were possible."
    },
    {
      "id": 194,
      "start": 495.94,
      "end": 501.48,
      "text": "I'm trying to figure out, has the definition of intelligence changed per se?"
    },
    {
      "id": 195,
      "start": 502.24,
      "end": 508.08,
      "text": "Well, you know, what I would say is five years ago, you know, you could Google and there might"
    },
    {
      "id": 196,
      "start": 508.08,
      "end": 511.5,
      "text": "be a website that, you know, would tell you a little bit about this, right?"
    },
    {
      "id": 197,
      "start": 511.72,
      "end": 516.0,
      "text": "But, you know, you're just looking up some text that exists on the web, right?"
    },
    {
      "id": 198,
      "start": 516.08,
      "end": 518.84,
      "text": "You know, maybe it's not about how to get a monkey to juggle."
    },
    {
      "id": 199,
      "start": 519.32,
      "end": 522.4,
      "text": "Maybe, you know, maybe it's about how to get a seal to juggle."
    },
    {
      "id": 200,
      "start": 522.4,
      "end": 527.6,
      "text": "You know, it's not quite exactly the same thing because maybe exactly the same thing doesn't exist."
    },
    {
      "id": 201,
      "start": 528.86,
      "end": 535.04,
      "text": "But, you know, as we see when people use these models, you know, you can ask and you can actually"
    },
    {
      "id": 202,
      "start": 535.04,
      "end": 536.28,
      "text": "get an intelligent response."
    },
    {
      "id": 203,
      "start": 536.36,
      "end": 540.16,
      "text": "You can ask a specific question and have the model write, you know, one page about it."
    },
    {
      "id": 204,
      "start": 540.16,
      "end": 543.78,
      "text": "Or you can give it a, you know, you can give it a hypothetical."
    },
    {
      "id": 205,
      "start": 543.78,
      "end": 547.86,
      "text": "You know, what if I had, you know, the monkey juggle clubs instead of balls?"
    },
    {
      "id": 206,
      "start": 547.86,
      "end": 550.28,
      "text": "Or, you know, what if I did this thing?"
    },
    {
      "id": 207,
      "start": 550.28,
      "end": 556.0799999999999,
      "text": "And that information doesn't exist anywhere, you know, whereas the model is able to kind of think for itself"
    },
    {
      "id": 208,
      "start": 556.0799999999999,
      "end": 558.54,
      "text": "and come up with an answer on its own."
    },
    {
      "id": 209,
      "start": 558.64,
      "end": 563.4,
      "text": "So it's something, you know, it's something totally new."
    },
    {
      "id": 210,
      "start": 563.52,
      "end": 566.92,
      "text": "It's not just matching some of the text that exists on the internet."
    },
    {
      "id": 211,
      "start": 566.92,
      "end": 572.38,
      "text": "Fair. So, you know, this is more like a conversation."
    },
    {
      "id": 212,
      "start": 572.38,
      "end": 578.42,
      "text": "So feel free to, like, talk about what you want to talk, not necessarily related to the questions that I'm asking."
    },
    {
      "id": 213,
      "start": 579.68,
      "end": 581.9,
      "text": "You look very animated when you speak."
    },
    {
      "id": 214,
      "start": 582.0799999999999,
      "end": 583.02,
      "text": "Did you ever teach?"
    },
    {
      "id": 215,
      "start": 584.1999999999999,
      "end": 586.4,
      "text": "You know, I was originally an academic."
    },
    {
      "id": 216,
      "start": 587.18,
      "end": 589.88,
      "text": "And, you know, I thought that I might become a professor."
    },
    {
      "id": 217,
      "start": 590.0799999999999,
      "end": 591.3,
      "text": "You know, I got my Ph.D."
    },
    {
      "id": 218,
      "start": 591.3,
      "end": 594.66,
      "text": "I went all the way to being a postdoc at Stanford Medical School."
    },
    {
      "id": 219,
      "start": 594.78,
      "end": 598.2199999999999,
      "text": "And, you know, I was aiming to become a professor."
    },
    {
      "id": 220,
      "start": 598.88,
      "end": 603.5,
      "text": "So if I had become a professor, you know, I would have done that."
    },
    {
      "id": 221,
      "start": 605.0999999999999,
      "end": 609.4599999999999,
      "text": "But, you know, as I mentioned, you know, I got interested in AI."
    },
    {
      "id": 222,
      "start": 610.16,
      "end": 613.5799999999999,
      "text": "And to work in AI required a lot of computational resources."
    },
    {
      "id": 223,
      "start": 613.5799999999999,
      "end": 615.2199999999999,
      "text": "And that was mostly happening in industry."
    },
    {
      "id": 224,
      "start": 615.2199999999999,
      "end": 618.9,
      "text": "So that took me off the academic path and, you know, into industry."
    },
    {
      "id": 225,
      "start": 618.9,
      "end": 623.04,
      "text": "And, of course, you know, ultimately through several steps led me to start a company."
    },
    {
      "id": 226,
      "start": 623.22,
      "end": 625.88,
      "text": "But, you know, sometimes I think I'm still like a professor at heart."
    },
    {
      "id": 227,
      "start": 626.62,
      "end": 632.4,
      "text": "At this point, Dario, if AI is the most relevant thing in the world,"
    },
    {
      "id": 228,
      "start": 633.26,
      "end": 640.48,
      "text": "if the world is realigning in a way and AI is determining who gets what and who doesn't get what,"
    },
    {
      "id": 229,
      "start": 640.48,
      "end": 647.76,
      "text": "I'm talking about industries, you today are probably the most relevant person in the world."
    },
    {
      "id": 230,
      "start": 647.76,
      "end": 655.4200000000001,
      "text": "If Anthropic, in this last cycle, in this minute, is sitting on top of this pie."
    },
    {
      "id": 231,
      "start": 656.3000000000001,
      "end": 662.44,
      "text": "For somebody who was going on the path of being a teacher to have arrived to where you are today,"
    },
    {
      "id": 232,
      "start": 662.96,
      "end": 665.84,
      "text": "are you best equipped for where you are today?"
    },
    {
      "id": 233,
      "start": 666.44,
      "end": 668.9200000000001,
      "text": "Well, I mean, you know, first I would say a couple of things."
    },
    {
      "id": 234,
      "start": 669.02,
      "end": 674.64,
      "text": "You know, I think there's a lot of folks who are, right, relevant in different ways, right?"
    },
    {
      "id": 235,
      "start": 674.72,
      "end": 677.82,
      "text": "You know, even within industry, there's the different layers of the stack."
    },
    {
      "id": 236,
      "start": 677.9200000000001,
      "end": 679.24,
      "text": "There's, like, the folks who make chips."
    },
    {
      "id": 237,
      "start": 679.6600000000001,
      "end": 682.98,
      "text": "There's the folks even earlier who make semiconductor manufacturing equipment."
    },
    {
      "id": 238,
      "start": 683.1600000000001,
      "end": 684.6600000000001,
      "text": "There's the folks who make models like us."
    },
    {
      "id": 239,
      "start": 684.6800000000001,
      "end": 686.32,
      "text": "And then there are other players who make models."
    },
    {
      "id": 240,
      "start": 686.64,
      "end": 689.98,
      "text": "There's the folks who make kind of applications after the models."
    },
    {
      "id": 241,
      "start": 689.98,
      "end": 694.4200000000001,
      "text": "You know, and then there's a bunch of other folks who have a say."
    },
    {
      "id": 242,
      "start": 694.58,
      "end": 695.5,
      "text": "There's, you know, governments."
    },
    {
      "id": 243,
      "start": 695.84,
      "end": 697.1800000000001,
      "text": "There's, like, civil society."
    },
    {
      "id": 244,
      "start": 697.38,
      "end": 703.86,
      "text": "So, you know, my hope, you know, isn't that there's, you know, just one tiny set of people that's relevant."
    },
    {
      "id": 245,
      "start": 704.1800000000001,
      "end": 709.94,
      "text": "I think we're trying to broaden the set of people who are relevant and, you know, turn it into a broader conversation."
    },
    {
      "id": 246,
      "start": 709.94,
      "end": 713.6800000000001,
      "text": "But, you know, I think at the same time, your question is a fair one."
    },
    {
      "id": 247,
      "start": 713.8000000000001,
      "end": 727.22,
      "text": "And one way I could interpret it is, like, you know, there's a certain randomness to how, you know, kind of, you know, a few people, you know, end up leading these, you know, leading these companies that kind of, you know, grow so fast."
    },
    {
      "id": 248,
      "start": 727.22,
      "end": 731.0,
      "text": "And it seems like, you know, in the near future will power so much of the economy."
    },
    {
      "id": 249,
      "start": 731.0,
      "end": 744.04,
      "text": "And, you know, I've said openly, publicly, not for the first time that I'm at least somewhat uncomfortable with the amount of concentration of power that's happening here, I would say, almost overnight, almost by accident."
    },
    {
      "id": 250,
      "start": 744.54,
      "end": 749.48,
      "text": "And, you know, we think, you know, about that in a bunch of ways."
    },
    {
      "id": 251,
      "start": 749.62,
      "end": 753.7,
      "text": "You know, one is we have an unusual governance structure, something called the Long-Term Benefit Trust."
    },
    {
      "id": 252,
      "start": 753.7,
      "end": 766.7,
      "text": "You know, it's a body that kind of ultimately appoints, you know, the majority of the board members for Anthropic and is, you know, made up of financially disinterested individuals."
    },
    {
      "id": 253,
      "start": 767.5200000000001,
      "end": 771.58,
      "text": "So that's some, you know, check on what one single person is doing."
    },
    {
      "id": 254,
      "start": 771.72,
      "end": 775.5600000000001,
      "text": "And then, you know, I think, as always, the government should play some role here."
    },
    {
      "id": 255,
      "start": 775.56,
      "end": 784.4,
      "text": "You know, I've been an advocate of, you know, proactive, although, you know, sensible, that doesn't slow down the technology, sensible regulation of the technology."
    },
    {
      "id": 256,
      "start": 784.4,
      "end": 787.6199999999999,
      "text": "Because, you know, I think, like, the people should have a say."
    },
    {
      "id": 257,
      "start": 787.78,
      "end": 792.8399999999999,
      "text": "Like, you know, governments and the people who elect them should have a say in how this goes."
    },
    {
      "id": 258,
      "start": 792.84,
      "end": 806.3000000000001,
      "text": "So I actually think of a lot of what I'm trying to do as kind of trying to preserve a balance of power, you know, kind of, you know, against the natural grain of this technology."
    },
    {
      "id": 259,
      "start": 807.44,
      "end": 812.82,
      "text": "For someone like me who is sitting on the outside and doesn't have a bone in this competition,"
    },
    {
      "id": 260,
      "start": 812.82,
      "end": 820.4000000000001,
      "text": "when I watch OpenAI talk about how they were a not-for-profit company,"
    },
    {
      "id": 261,
      "start": 821.1400000000001,
      "end": 827.0400000000001,
      "text": "or how you are projecting humility in the conversation that you're having right now,"
    },
    {
      "id": 262,
      "start": 827.6800000000001,
      "end": 832.5,
      "text": "or how the American companies are competing with the Chinese companies which are coming about,"
    },
    {
      "id": 263,
      "start": 834.1800000000001,
      "end": 837.96,
      "text": "this projection of humility where it is for the larger good,"
    },
    {
      "id": 264,
      "start": 837.96,
      "end": 843.5400000000001,
      "text": "and not necessarily for how I view the world as companies with shareholders,"
    },
    {
      "id": 265,
      "start": 844.0,
      "end": 846.36,
      "text": "with investment and revenues and seeking profit."
    },
    {
      "id": 266,
      "start": 848.44,
      "end": 850.1800000000001,
      "text": "Is this par for the course?"
    },
    {
      "id": 267,
      "start": 850.24,
      "end": 851.48,
      "text": "Is this something you have to do?"
    },
    {
      "id": 268,
      "start": 852.08,
      "end": 855.32,
      "text": "So, you know, I would put it in the following way."
    },
    {
      "id": 269,
      "start": 855.96,
      "end": 863.52,
      "text": "You know, I would say the philosophy of Anthropic from the beginning has been that we try not to make too many promises,"
    },
    {
      "id": 270,
      "start": 863.98,
      "end": 866.08,
      "text": "and we try to keep the ones that we make."
    },
    {
      "id": 271,
      "start": 866.08,
      "end": 876.22,
      "text": "So, you know, we set ourselves up as, you know, a for-profit but public benefit corporation with this LTPT governance,"
    },
    {
      "id": 272,
      "start": 876.6800000000001,
      "end": 877.72,
      "text": "and we've maintained that."
    },
    {
      "id": 273,
      "start": 877.96,
      "end": 883.72,
      "text": "We've said that, you know, our goal is to, you know, stay on the frontier of the technology,"
    },
    {
      "id": 274,
      "start": 884.1800000000001,
      "end": 890.9000000000001,
      "text": "but, you know, to work on, you know, to work on, you know, the safety and security aspects of the technology."
    },
    {
      "id": 275,
      "start": 890.9,
      "end": 893.52,
      "text": "We've pioneered the science of interpretability."
    },
    {
      "id": 276,
      "start": 894.18,
      "end": 896.64,
      "text": "We've, you know, pioneered the science of alignment."
    },
    {
      "id": 277,
      "start": 896.76,
      "end": 900.42,
      "text": "I don't know if you saw, but we recently released a constitution for Claude,"
    },
    {
      "id": 278,
      "start": 900.4599999999999,
      "end": 903.64,
      "text": "the ability to align models in line with the constitution."
    },
    {
      "id": 279,
      "start": 904.14,
      "end": 909.04,
      "text": "And, you know, we've done a bunch of policy advocacy and warning about risks, right?"
    },
    {
      "id": 280,
      "start": 909.12,
      "end": 911.86,
      "text": "Warning about risks is not in our commercial interest, right?"
    },
    {
      "id": 281,
      "start": 912.12,
      "end": 915.84,
      "text": "Like, people can come up with conspiracy theories, but, you know, I will tell you,"
    },
    {
      "id": 282,
      "start": 915.84,
      "end": 920.6800000000001,
      "text": "saying that the models we build could be dangerous, whatever people might say,"
    },
    {
      "id": 283,
      "start": 920.74,
      "end": 924.52,
      "text": "that's not an effective marketing strategy, and that's not the reason that we do it."
    },
    {
      "id": 284,
      "start": 924.94,
      "end": 929.8000000000001,
      "text": "And, you know, speaking up on when we disagree, even with the U.S. administration,"
    },
    {
      "id": 285,
      "start": 930.0,
      "end": 933.74,
      "text": "on, you know, on policy matters, right?"
    },
    {
      "id": 286,
      "start": 934.52,
      "end": 936.02,
      "text": "We've spoken up, right?"
    },
    {
      "id": 287,
      "start": 936.02,
      "end": 939.6,
      "text": "We're willing to say, you know, we disagree on this issue."
    },
    {
      "id": 288,
      "start": 939.7800000000001,
      "end": 942.6,
      "text": "Like, you know, we've said that there should be regulation of AI,"
    },
    {
      "id": 289,
      "start": 942.6,
      "end": 947.0400000000001,
      "text": "when all the other companies and the administration have said there shouldn't be regulation of AI."
    },
    {
      "id": 290,
      "start": 947.5600000000001,
      "end": 952.0,
      "text": "And so that's both, you know, the regulation of AI holds, you know,"
    },
    {
      "id": 291,
      "start": 952.0600000000001,
      "end": 955.3000000000001,
      "text": "holds us back commercially as a company, even though I think it's the right thing to do."
    },
    {
      "id": 292,
      "start": 955.96,
      "end": 961.5,
      "text": "And it's, you know, it's difficult to go against the government and the other companies and say this."
    },
    {
      "id": 293,
      "start": 961.5600000000001,
      "end": 962.84,
      "text": "We're really sticking our neck out."
    },
    {
      "id": 294,
      "start": 962.84,
      "end": 967.64,
      "text": "So we've taken a number of actions that, you know, I see as really, you know,"
    },
    {
      "id": 295,
      "start": 967.74,
      "end": 971.12,
      "text": "putting our money where our mouth is here."
    },
    {
      "id": 296,
      "start": 971.12,
      "end": 973.36,
      "text": "I can't speak for the other companies."
    },
    {
      "id": 297,
      "start": 973.58,
      "end": 977.3,
      "text": "You know, it's, again, it's quite possible that some people say these things,"
    },
    {
      "id": 298,
      "start": 977.42,
      "end": 979.28,
      "text": "you know, and they don't really mean them."
    },
    {
      "id": 299,
      "start": 979.34,
      "end": 980.68,
      "text": "But I wouldn't look at what people say."
    },
    {
      "id": 300,
      "start": 980.72,
      "end": 981.82,
      "text": "I would look at what people do."
    },
    {
      "id": 301,
      "start": 983.42,
      "end": 989.74,
      "text": "If what you're saying gets the government to act via regulation,"
    },
    {
      "id": 302,
      "start": 990.28,
      "end": 994.36,
      "text": "as the incumbent leaders in this space,"
    },
    {
      "id": 303,
      "start": 994.66,
      "end": 997.2,
      "text": "you get some kind of a regulatory capture"
    },
    {
      "id": 304,
      "start": 997.2,
      "end": 1000.08,
      "text": "where it becomes harder for the new people coming in as well, right?"
    },
    {
      "id": 305,
      "start": 1000.08,
      "end": 1001.9000000000001,
      "text": "I don't agree with that at all."
    },
    {
      "id": 306,
      "start": 1002.08,
      "end": 1008.4200000000001,
      "text": "The regulation we've advocated for, for example, SB53 in California,"
    },
    {
      "id": 307,
      "start": 1009.0400000000001,
      "end": 1017.32,
      "text": "exempted everyone who makes under $500 million a year in revenue, right?"
    },
    {
      "id": 308,
      "start": 1017.5400000000001,
      "end": 1020.0200000000001,
      "text": "SB53 was a transparency law,"
    },
    {
      "id": 309,
      "start": 1020.02,
      "end": 1029.1,
      "text": "which, you know, basically requires companies to, you know, to show, you know, the safety and security tests that they've run."
    },
    {
      "id": 310,
      "start": 1029.52,
      "end": 1033.02,
      "text": "And it exempts all companies under $500 million in revenue."
    },
    {
      "id": 311,
      "start": 1033.16,
      "end": 1037.86,
      "text": "So it really only applies to Anthropic and three or four other companies."
    },
    {
      "id": 312,
      "start": 1037.94,
      "end": 1040.6,
      "text": "So it only applies to the companies that have the resources."
    },
    {
      "id": 313,
      "start": 1040.6,
      "end": 1047.8,
      "text": "And everything that we've advocated for here, not just SB53, but all the proposals that we've made,"
    },
    {
      "id": 314,
      "start": 1047.86,
      "end": 1052.3,
      "text": "the ones that we've made in the past and the ones that we plan to make in the future, have this character."
    },
    {
      "id": 315,
      "start": 1052.4199999999998,
      "end": 1057.08,
      "text": "We're constraining ourselves and a very small number of additional companies."
    },
    {
      "id": 316,
      "start": 1057.3799999999999,
      "end": 1062.82,
      "text": "We're not—people who say that need to look at the actual content of what we're proposing"
    },
    {
      "id": 317,
      "start": 1062.82,
      "end": 1064.48,
      "text": "because it doesn't match that idea at all."
    },
    {
      "id": 318,
      "start": 1065.76,
      "end": 1066.02,
      "text": "Fair."
    },
    {
      "id": 319,
      "start": 1067.4399999999998,
      "end": 1071.8,
      "text": "I read your paper, Machines of Loving Rays and the Adolescence of Technology,"
    },
    {
      "id": 320,
      "start": 1071.8,
      "end": 1078.1399999999999,
      "text": "and you seem to have had a 180-degree shift in perspective almost,"
    },
    {
      "id": 321,
      "start": 1078.3799999999999,
      "end": 1085.98,
      "text": "from optimism to skepticism over, like, two years, from 2024 to 2026."
    },
    {
      "id": 322,
      "start": 1087.2,
      "end": 1091.1599999999999,
      "text": "Is there one moment in the last two years that changed this for you?"
    },
    {
      "id": 323,
      "start": 1091.24,
      "end": 1092.3,
      "text": "Did you see something change?"
    },
    {
      "id": 324,
      "start": 1092.3,
      "end": 1094.32,
      "text": "Yeah, I actually wouldn't agree with the question."
    },
    {
      "id": 325,
      "start": 1094.5,
      "end": 1096.62,
      "text": "I don't think I've had a shift in perspective."
    },
    {
      "id": 326,
      "start": 1097.6,
      "end": 1102.98,
      "text": "I think the positive side and the negative side are always something that I've held in my head."
    },
    {
      "id": 327,
      "start": 1103.06,
      "end": 1107.6599999999999,
      "text": "And if you look at the history of, you know, the things that I've said,"
    },
    {
      "id": 328,
      "start": 1107.74,
      "end": 1109.8,
      "text": "I mean, I've been talking about risks for a very long time."
    },
    {
      "id": 329,
      "start": 1109.8799999999999,
      "end": 1111.8799999999999,
      "text": "I've been talking about benefits for a very long time."
    },
    {
      "id": 330,
      "start": 1112.96,
      "end": 1116.6,
      "text": "You know, it turns out that actually it takes me a while to write one of these essays."
    },
    {
      "id": 331,
      "start": 1117.46,
      "end": 1118.4199999999998,
      "text": "You know, both—"
    },
    {
      "id": 332,
      "start": 1118.4199999999998,
      "end": 1119.3999999999999,
      "text": "They're really large as well."
    },
    {
      "id": 333,
      "start": 1119.5,
      "end": 1120.06,
      "text": "They're big essays."
    },
    {
      "id": 334,
      "start": 1120.12,
      "end": 1121.34,
      "text": "They're like 30 pages."
    },
    {
      "id": 335,
      "start": 1121.34,
      "end": 1121.6,
      "text": "Books."
    },
    {
      "id": 336,
      "start": 1121.6,
      "end": 1125.54,
      "text": "Because both of these, it's, you know, it's taken me like—"
    },
    {
      "id": 337,
      "start": 1125.54,
      "end": 1130.98,
      "text": "I spent—for each one, I spent about a year having a kind of vague vision of the essay in my head"
    },
    {
      "id": 338,
      "start": 1130.98,
      "end": 1134.6999999999998,
      "text": "and, like, trying to write it, but, like, not fully succeeding at writing it."
    },
    {
      "id": 339,
      "start": 1134.8,
      "end": 1139.3,
      "text": "And then, you know, in either case, I had to be on vacation or somewhere where I could, you know,"
    },
    {
      "id": 340,
      "start": 1139.34,
      "end": 1144.3999999999999,
      "text": "where I could think where the business, day-to-day business of running the company didn't occupy me."
    },
    {
      "id": 341,
      "start": 1144.4,
      "end": 1148.6000000000001,
      "text": "And then I was finally able to, you know, to kind of write the essay."
    },
    {
      "id": 342,
      "start": 1148.6,
      "end": 1154.8,
      "text": "So all of that is to say, you know, I started thinking about what would be an adolescence of technology"
    },
    {
      "id": 343,
      "start": 1154.8,
      "end": 1160.9199999999998,
      "text": "almost the instant I finished Machines of Loving Grace because I was like, oh, you know, I want to inspire people with the good vision,"
    },
    {
      "id": 344,
      "start": 1160.9199999999998,
      "end": 1164.6999999999998,
      "text": "but I also want to warn people with, you know, what can go wrong."
    },
    {
      "id": 345,
      "start": 1164.7,
      "end": 1167.46,
      "text": "And so it just took me a year to write it."
    },
    {
      "id": 346,
      "start": 1167.46,
      "end": 1172.94,
      "text": "But really, both visions were in my head, and I think they're both, you know, I think they're both possible."
    },
    {
      "id": 347,
      "start": 1172.94,
      "end": 1175.56,
      "text": "They're two different visions of the future."
    },
    {
      "id": 348,
      "start": 1175.56,
      "end": 1178.5,
      "text": "And obviously, I want to get the Machines of Loving Grace one, right?"
    },
    {
      "id": 349,
      "start": 1178.5,
      "end": 1182.8400000000001,
      "text": "You know, I want to solve all the problems and have the positive vision."
    },
    {
      "id": 350,
      "start": 1182.8400000000001,
      "end": 1185.94,
      "text": "But it's not a shift in perspective."
    },
    {
      "id": 351,
      "start": 1185.94,
      "end": 1190.8600000000001,
      "text": "It's me just, you know, finding the time to write the light and then the dark."
    },
    {
      "id": 352,
      "start": 1191.24,
      "end": 1193.04,
      "text": "But have you had a change of perspective?"
    },
    {
      "id": 353,
      "start": 1193.04,
      "end": 1200.54,
      "text": "You know, I would say, overall, I have, I'm about where I was before."
    },
    {
      "id": 354,
      "start": 1200.54,
      "end": 1204.6599999999999,
      "text": "I have not gotten more positive nor more negative."
    },
    {
      "id": 355,
      "start": 1205.24,
      "end": 1210.48,
      "text": "There may be some places where I've gotten more optimistic or things have gone better than expected."
    },
    {
      "id": 356,
      "start": 1210.8799999999999,
      "end": 1215.6599999999999,
      "text": "There may be places where I'm more pessimistic and where things have gone worse than expected."
    },
    {
      "id": 357,
      "start": 1216.0,
      "end": 1218.1399999999999,
      "text": "But on average, they sort of cancel each other out."
    },
    {
      "id": 358,
      "start": 1218.14,
      "end": 1225.64,
      "text": "I would say I feel very good about, you know, how things have gone with areas like interpretability."
    },
    {
      "id": 359,
      "start": 1225.64,
      "end": 1229.1000000000001,
      "text": "Interpretability is the science of seeing inside these neural nets."
    },
    {
      "id": 360,
      "start": 1229.1000000000001,
      "end": 1235.96,
      "text": "You know, as a human would, you know, look inside, you know, as we would scan a human brain with an MRI or a neural probe."
    },
    {
      "id": 361,
      "start": 1236.96,
      "end": 1239.0200000000002,
      "text": "I've been amazed at what we've been able to find."
    },
    {
      "id": 362,
      "start": 1239.1200000000001,
      "end": 1243.4,
      "text": "We've been able to find, you know, neurons that correspond to very specific concepts."
    },
    {
      "id": 363,
      "start": 1243.9,
      "end": 1249.42,
      "text": "Neural circuits that correspond to, you know, keep track of how to do rhymes in poetry."
    },
    {
      "id": 364,
      "start": 1249.42,
      "end": 1253.28,
      "text": "And so we're starting to understand what these models do, right?"
    },
    {
      "id": 365,
      "start": 1253.3400000000001,
      "end": 1254.04,
      "text": "We don't."
    },
    {
      "id": 366,
      "start": 1254.1200000000001,
      "end": 1257.7,
      "text": "We just train them in this kind of emergent way as you would build a snowflake."
    },
    {
      "id": 367,
      "start": 1257.8200000000002,
      "end": 1260.66,
      "text": "But now we're starting to be able to look inside and understand them."
    },
    {
      "id": 368,
      "start": 1261.24,
      "end": 1265.42,
      "text": "I'm also very encouraged by some of the work on alignment and constitutions."
    },
    {
      "id": 369,
      "start": 1265.98,
      "end": 1270.6200000000001,
      "text": "You know, making sure that models behave in the way that we want and expect them to."
    },
    {
      "id": 370,
      "start": 1270.74,
      "end": 1272.14,
      "text": "I think that's going pretty well."
    },
    {
      "id": 371,
      "start": 1272.14,
      "end": 1276.3600000000001,
      "text": "I felt pretty positive about that."
    },
    {
      "id": 372,
      "start": 1276.94,
      "end": 1291.3000000000002,
      "text": "I think I felt maybe, you know, have been a bit disappointed or felt a bit more negative about some of the things that are more like in the, you know, in the kind of public awareness and the actions of wider society."
    },
    {
      "id": 373,
      "start": 1291.98,
      "end": 1300.6000000000001,
      "text": "You know, it is surprising to me that we are, you know, in my view, so close to these models reaching the level of human intelligence."
    },
    {
      "id": 374,
      "start": 1300.6,
      "end": 1308.3799999999999,
      "text": "And yet there doesn't seem to be a wider recognition in society of what's about to happen."
    },
    {
      "id": 375,
      "start": 1308.54,
      "end": 1310.8,
      "text": "It's as if this tsunami is coming at us."
    },
    {
      "id": 376,
      "start": 1311.34,
      "end": 1312.6999999999998,
      "text": "And, you know, it's so close."
    },
    {
      "id": 377,
      "start": 1312.76,
      "end": 1313.9399999999998,
      "text": "We can see it on the horizon."
    },
    {
      "id": 378,
      "start": 1313.9399999999998,
      "end": 1318.3,
      "text": "And yet people are coming up with these explanations for, oh, it's not actually a tsunami."
    },
    {
      "id": 379,
      "start": 1318.6399999999999,
      "end": 1321.3999999999999,
      "text": "It's, you know, that, you know, that's just a trick of the light."
    },
    {
      "id": 380,
      "start": 1321.5,
      "end": 1325.82,
      "text": "Like it's some, you know, and I think along with that, there hasn't been a public awareness of the risks."
    },
    {
      "id": 381,
      "start": 1326.2199999999998,
      "end": 1330.4199999999998,
      "text": "And, you know, therefore our governments haven't acted to address the risks."
    },
    {
      "id": 382,
      "start": 1330.42,
      "end": 1337.68,
      "text": "There's even an ideology that, you know, we should just try to accelerate as fast as possible, which, you know, I understand the benefits of the technology."
    },
    {
      "id": 383,
      "start": 1337.8000000000002,
      "end": 1339.0600000000002,
      "text": "I wrote Machines of Loving Grace."
    },
    {
      "id": 384,
      "start": 1339.0600000000002,
      "end": 1345.16,
      "text": "But I think there hasn't been an appropriate realization of the risks of the technology and there certainly hasn't been action."
    },
    {
      "id": 385,
      "start": 1345.16,
      "end": 1352.38,
      "text": "So I would say that the technical work on controlling the AI systems has gone maybe a little better than I expected."
    },
    {
      "id": 386,
      "start": 1352.52,
      "end": 1356.48,
      "text": "And kind of the societal awareness has gone maybe a little worse than I expected."
    },
    {
      "id": 387,
      "start": 1356.48,
      "end": 1359.14,
      "text": "So I'm about where I was a few years ago."
    },
    {
      "id": 388,
      "start": 1360.2,
      "end": 1369.14,
      "text": "So in my own journey, I'm, you know, when something sounds complicated and I'm not a programmer, I don't have a background in coding."
    },
    {
      "id": 389,
      "start": 1369.14,
      "end": 1376.2,
      "text": "So I used a bunch of tools for things like research and a conversation both ways."
    },
    {
      "id": 390,
      "start": 1376.6200000000001,
      "end": 1383.3200000000002,
      "text": "But I never tried to figure out if I could code using your tool, for example."
    },
    {
      "id": 391,
      "start": 1383.32,
      "end": 1394.04,
      "text": "Recently, I hired a developer just to like push me to sit for a couple of hours a day and teach me how to start becoming more familiar with it."
    },
    {
      "id": 392,
      "start": 1395.7,
      "end": 1400.74,
      "text": "Largely because if you know something like FOMO, like the fear of missing out on how the world is changing."
    },
    {
      "id": 393,
      "start": 1400.74,
      "end": 1403.66,
      "text": "So I started playing with Claude."
    },
    {
      "id": 394,
      "start": 1403.66,
      "end": 1404.66,
      "text": "I connected."
    },
    {
      "id": 395,
      "start": 1404.66,
      "end": 1411.92,
      "text": "I used the connectors to connect my Google Drive, Mail and Calendar and a bunch of those things."
    },
    {
      "id": 396,
      "start": 1411.92,
      "end": 1414.06,
      "text": "I started using the co-work."
    },
    {
      "id": 397,
      "start": 1414.06,
      "end": 1424.32,
      "text": "And then I started using Claude code to write simple programs around the industry that I'm in, which is financial services."
    },
    {
      "id": 398,
      "start": 1424.32,
      "end": 1427.28,
      "text": "Basically, to research stock markets and stuff."
    },
    {
      "id": 399,
      "start": 1427.34,
      "end": 1430.24,
      "text": "We even have an optimized Claude for financial services."
    },
    {
      "id": 400,
      "start": 1430.3799999999999,
      "end": 1432.26,
      "text": "I don't know if you've tried that, but we even have that."
    },
    {
      "id": 401,
      "start": 1432.58,
      "end": 1432.7,
      "text": "No."
    },
    {
      "id": 402,
      "start": 1433.32,
      "end": 1436.58,
      "text": "And then I went into Claudebot, which is now OpenClaude."
    },
    {
      "id": 403,
      "start": 1436.58,
      "end": 1438.32,
      "text": "I think Claudebot became something else."
    },
    {
      "id": 404,
      "start": 1438.3999999999999,
      "end": 1439.28,
      "text": "It now is OpenClaude."
    },
    {
      "id": 405,
      "start": 1439.3999999999999,
      "end": 1444.22,
      "text": "And I set it up on a Mac mini and connected it to a Telegram account."
    },
    {
      "id": 406,
      "start": 1444.32,
      "end": 1445.32,
      "text": "And now I chat with it."
    },
    {
      "id": 407,
      "start": 1445.48,
      "end": 1452.04,
      "text": "And I try and move files from A to B, work on a server on remote."
    },
    {
      "id": 408,
      "start": 1452.04,
      "end": 1459.42,
      "text": "But it's getting to that point where I'm not talking about OpenClaude, but even Claude with all the connectors."
    },
    {
      "id": 409,
      "start": 1460.1,
      "end": 1464.62,
      "text": "Sometimes it surprises me by how much it knows me."
    },
    {
      "id": 410,
      "start": 1465.32,
      "end": 1466.76,
      "text": "I don't know if that makes sense."
    },
    {
      "id": 411,
      "start": 1467.2,
      "end": 1467.42,
      "text": "Yeah."
    },
    {
      "id": 412,
      "start": 1467.6,
      "end": 1475.54,
      "text": "You know, one of my co-founders, you know, he was writing this diary with his kind of, you know, his thoughts and his fears."
    },
    {
      "id": 413,
      "start": 1475.54,
      "end": 1479.32,
      "text": "And he fed it into Claude."
    },
    {
      "id": 414,
      "start": 1479.86,
      "end": 1482.98,
      "text": "And, you know, he asked Claude to comment on it."
    },
    {
      "id": 415,
      "start": 1483.04,
      "end": 1487.98,
      "text": "And Claude said, here are some other fears you might have that, you know, that you haven't written down."
    },
    {
      "id": 416,
      "start": 1488.44,
      "end": 1491.18,
      "text": "And Claude ended up being mostly right about those."
    },
    {
      "id": 417,
      "start": 1491.18,
      "end": 1496.8,
      "text": "So it really gave this eerie sense of, like, you know, the model knows you super well."
    },
    {
      "id": 418,
      "start": 1497.02,
      "end": 1503.3200000000002,
      "text": "That, you know, that from a relatively small amount of information, it can learn a lot about you and come to know you fairly well."
    },
    {
      "id": 419,
      "start": 1503.78,
      "end": 1507.0600000000002,
      "text": "And, you know, like most things with the technology, right?"
    },
    {
      "id": 420,
      "start": 1507.0600000000002,
      "end": 1510.6200000000001,
      "text": "We talked about the machines of loving grace and adolescence of technology."
    },
    {
      "id": 421,
      "start": 1511.4,
      "end": 1518.96,
      "text": "You know, on one hand, something that knows you really well can be a sort of angel on your shoulder that, you know, that helps to guide your life and make you a better version of yourself."
    },
    {
      "id": 422,
      "start": 1518.96,
      "end": 1521.14,
      "text": "And, you know, that's the version we can aim for."
    },
    {
      "id": 423,
      "start": 1521.44,
      "end": 1533.96,
      "text": "Of course, something that knows you really well, you know, can, you know, it can, you know, use what it knows about you to, you know, to exploit you or manipulate you on behalf of some agenda or sell your data to someone else."
    },
    {
      "id": 424,
      "start": 1533.96,
      "end": 1538.9,
      "text": "I mean, you know, this is one reason we just, you know, don't like the idea of, you know, using ads, right?"
    },
    {
      "id": 425,
      "start": 1539.0,
      "end": 1542.44,
      "text": "You know, this is because you're not paying for the product like you're the product."
    },
    {
      "id": 426,
      "start": 1542.78,
      "end": 1548.8400000000001,
      "text": "And, you know, in this case, the product then would be all, you know, this model that knows you super well."
    },
    {
      "id": 427,
      "start": 1548.96,
      "end": 1553.18,
      "text": "And, you know, could use that in all kinds of, in all kinds of nefarious ways."
    },
    {
      "id": 428,
      "start": 1553.28,
      "end": 1559.3400000000001,
      "text": "So, you know, we need to make sure we take the positive, the positive road here and not the, not the negative road."
    },
    {
      "id": 429,
      "start": 1559.34,
      "end": 1566.1799999999998,
      "text": "With Claude, I need to use the connectors to give it context to my life."
    },
    {
      "id": 430,
      "start": 1567.8,
      "end": 1579.6999999999998,
      "text": "With Google, for example, it already has the context to my life because I use their worksheets and their email and their drive and their chat and everything like that."
    },
    {
      "id": 431,
      "start": 1579.7,
      "end": 1585.7,
      "text": "For Anthropic long-term, will you also have to own the ecosystem?"
    },
    {
      "id": 432,
      "start": 1586.76,
      "end": 1588.22,
      "text": "Yeah, I mean, you know."
    },
    {
      "id": 433,
      "start": 1588.22,
      "end": 1589.88,
      "text": "Do you have to build mail and chat?"
    },
    {
      "id": 434,
      "start": 1589.98,
      "end": 1590.66,
      "text": "Yeah, yeah."
    },
    {
      "id": 435,
      "start": 1590.8600000000001,
      "end": 1594.06,
      "text": "You know, I don't think we need to build all of those things."
    },
    {
      "id": 436,
      "start": 1595.18,
      "end": 1603.96,
      "text": "You know, my thought would be, you know, it's going to be a mixture of things we make ourselves and integrating into others, right?"
    },
    {
      "id": 437,
      "start": 1603.96,
      "end": 1606.92,
      "text": "Like, you know, we can, we can integrate Claude into Google Docs."
    },
    {
      "id": 438,
      "start": 1606.96,
      "end": 1610.08,
      "text": "We can integrate Claude into, into, you know, Google Sheets."
    },
    {
      "id": 439,
      "start": 1610.08,
      "end": 1612.7,
      "text": "Like, you know, we have external connectors there."
    },
    {
      "id": 440,
      "start": 1612.8,
      "end": 1615.54,
      "text": "We can, you know, we're starting to do that with, with co-work."
    },
    {
      "id": 441,
      "start": 1615.78,
      "end": 1619.0,
      "text": "You know, same for Microsoft Office, same for other tools."
    },
    {
      "id": 442,
      "start": 1619.0,
      "end": 1625.46,
      "text": "So, you know, I think, I think we do whatever is, you know, easiest and fastest to do."
    },
    {
      "id": 443,
      "start": 1625.46,
      "end": 1627.72,
      "text": "You know, we, we integrate into the existing tools."
    },
    {
      "id": 444,
      "start": 1627.9,
      "end": 1633.82,
      "text": "Now, it might turn out at some point that the existing, you know, tools aren't enough and we have kind of a different vision."
    },
    {
      "id": 445,
      "start": 1634.0,
      "end": 1637.08,
      "text": "You know, we want to, we might want to slice things differently, right?"
    },
    {
      "id": 446,
      "start": 1637.16,
      "end": 1643.18,
      "text": "You know, maybe traditional email doesn't make sense or traditional spreadsheets don't make sense given what you can do in AI."
    },
    {
      "id": 447,
      "start": 1643.38,
      "end": 1647.08,
      "text": "So, you know, I don't exclude that we could chop up products in a different way."
    },
    {
      "id": 448,
      "start": 1647.18,
      "end": 1651.16,
      "text": "But we're, we're happy to use the ecosystem that exists and work with anyone else, right?"
    },
    {
      "id": 449,
      "start": 1651.16,
      "end": 1653.4,
      "text": "In many ways, we're a platform company."
    },
    {
      "id": 450,
      "start": 1653.4,
      "end": 1657.66,
      "text": "We allow many people to build on us, even though we sometimes also build things ourselves."
    },
    {
      "id": 451,
      "start": 1659.46,
      "end": 1679.3000000000002,
      "text": "The, the one thing, this is a slight digression, but I think the one thing that you're missing that also your peer group is missing is in society today, people inherently distrust anybody who claims to be doing good or trying to do the right thing."
    },
    {
      "id": 452,
      "start": 1679.3,
      "end": 1687.18,
      "text": "So when you and your peers are out saying, I heard you and Demis speak at Davos."
    },
    {
      "id": 453,
      "start": 1687.24,
      "end": 1698.12,
      "text": "I was in the room when you guys were talking about how me, you, I don't mean me, how Dario, how Demis and a bunch of other people have to come together."
    },
    {
      "id": 454,
      "start": 1698.12,
      "end": 1703.5,
      "text": "And prevent things from changing too quickly."
    },
    {
      "id": 455,
      "start": 1703.5,
      "end": 1706.82,
      "text": "Like you need to like meter it to a certain extent."
    },
    {
      "id": 456,
      "start": 1706.82,
      "end": 1721.72,
      "text": "When a person who is not in your world, in society, on social media, hears a few people speak in a certain manner, you're doing it in the manner that creates more distrust than trust."
    },
    {
      "id": 457,
      "start": 1722.3799999999999,
      "end": 1729.12,
      "text": "Because nobody believes on social media that somebody wants to do the right thing or do good."
    },
    {
      "id": 458,
      "start": 1729.12,
      "end": 1733.4799999999998,
      "text": "So it might be counterintuitive, but I think it needs a change of strategy."
    },
    {
      "id": 459,
      "start": 1733.4799999999998,
      "end": 1742.9799999999998,
      "text": "If, if you were to be more capitalistic about this and own up to the fact that you have shareholders and you seek a profit, but this will help you win, maybe it'll work more."
    },
    {
      "id": 460,
      "start": 1742.9799999999998,
      "end": 1747.62,
      "text": "Yeah, I don't, no, I don't, I don't really, I don't really agree with that."
    },
    {
      "id": 461,
      "start": 1747.62,
      "end": 1756.84,
      "text": "I would again, go back to the idea that, you know, you know, you, you need to judge us by the actions that we take."
    },
    {
      "id": 462,
      "start": 1757.6399999999999,
      "end": 1770.02,
      "text": "You know, I think the company has taken a number of, of, of, of actions over its, you know, over its time that, you know, I think, I think, you know, show that it's really serious about these commitments."
    },
    {
      "id": 463,
      "start": 1770.02,
      "end": 1777.3,
      "text": "So back in 2022, you know, we had an early version of Claude, Claude 1, this was before ChatGPT."
    },
    {
      "id": 464,
      "start": 1778.22,
      "end": 1788.32,
      "text": "And we chose not to release this because we were worried that it would kick off an arms race and, and not give us enough time to, you know, to build these systems safely."
    },
    {
      "id": 465,
      "start": 1788.32,
      "end": 1790.6,
      "text": "Right. It was, it was kind of a one-time overhang."
    },
    {
      "id": 466,
      "start": 1790.6,
      "end": 1795.04,
      "text": "Like we could see the power of the models, a couple other companies could see the power of the models."
    },
    {
      "id": 467,
      "start": 1795.04,
      "end": 1803.34,
      "text": "And so we didn't, you know, we decided not to do that and that's public, that's well-documented and, and, you know, and then we waited until someone else did."
    },
    {
      "id": 468,
      "start": 1803.34,
      "end": 1805.3999999999999,
      "text": "And then we're like, okay, the arms race has kicked off."
    },
    {
      "id": 469,
      "start": 1805.3999999999999,
      "end": 1811.78,
      "text": "So, you know, now, now, now we can release our model, but probably the world gained a few months."
    },
    {
      "id": 470,
      "start": 1811.78,
      "end": 1813.8799999999999,
      "text": "Now that was very commercially expensive."
    },
    {
      "id": 471,
      "start": 1813.8799999999999,
      "end": 1819.1399999999999,
      "text": "We probably, you know, seeded the lead on, you know, consumer AI because of that."
    },
    {
      "id": 472,
      "start": 1819.14,
      "end": 1828.38,
      "text": "You know, we've, we've, you know, advocated on chip policy in ways that have made some of the chip companies who are suppliers very angry at us."
    },
    {
      "id": 473,
      "start": 1828.38,
      "end": 1835.74,
      "text": "You know, voicing our disagreement with the administration on, you know, AI policy and AI regulation on some, on some matters."
    },
    {
      "id": 474,
      "start": 1835.74,
      "end": 1840.68,
      "text": "You know, anyone who thinks we, we, we benefit from being the only ones to do that."
    },
    {
      "id": 475,
      "start": 1840.68,
      "end": 1847.6200000000001,
      "text": "You know, it's, it's really hard to come up with a, it's really hard to come up with a picture where that's the, where that's the case."
    },
    {
      "id": 476,
      "start": 1847.62,
      "end": 1855.12,
      "text": "You look at any one of these and okay, fine, but you know, you put, you put enough of them together and, you know, you know, I, I don't know."
    },
    {
      "id": 477,
      "start": 1855.12,
      "end": 1857.62,
      "text": "I just, I ask you to judge us by our actions."
    },
    {
      "id": 478,
      "start": 1857.62,
      "end": 1863.02,
      "text": "Daryu, isn't this a bit like rich people saying capitalism is bad?"
    },
    {
      "id": 479,
      "start": 1863.02,
      "end": 1865.9199999999998,
      "text": "Rich people saying capitalism is bad."
    },
    {
      "id": 480,
      "start": 1865.9199999999998,
      "end": 1871.9199999999998,
      "text": "If rich people believed capitalism were truly bad or the income inequality is such a big problem,"
    },
    {
      "id": 481,
      "start": 1871.92,
      "end": 1883.92,
      "text": "the simplest thing would be to do, the simplest thing to do would be to stop accumulating wealth, further wealth, and then nudge their friends to do the same."
    },
    {
      "id": 482,
      "start": 1883.92,
      "end": 1886.92,
      "text": "But, but I'm not saying AI is bad, right?"
    },
    {
      "id": 483,
      "start": 1886.92,
      "end": 1890.92,
      "text": "We, we just talked about, you know, this, this, this two sides of it."
    },
    {
      "id": 484,
      "start": 1890.92,
      "end": 1893.92,
      "text": "My view isn't, my view isn't that AI is bad."
    },
    {
      "id": 485,
      "start": 1893.92,
      "end": 1894.92,
      "text": "That's not my view at all."
    },
    {
      "id": 486,
      "start": 1894.92,
      "end": 1909.92,
      "text": "My, my, my view is that, is that, you know, the market will deliver a lot of really great things about AI, that it's good to build AI, but that there are dangers of AI, and that we need to steer AI in the right direction."
    },
    {
      "id": 487,
      "start": 1909.92,
      "end": 1919.92,
      "text": "You know, we're, we're, we're steering this car, we're steering it towards a good place, but also there are trees, there are potholes, and so what we need to do is we need to steer away from the trees and the potholes."
    },
    {
      "id": 488,
      "start": 1919.92,
      "end": 1932.92,
      "text": "We might need to occasionally slow down a bit, probably temporarily, you know, it kind of, in order to, in order to, you know, make sure that we steer in the right direction."
    },
    {
      "id": 489,
      "start": 1932.92,
      "end": 1937.92,
      "text": "You know, that, that isn't like, you know, the analogy wouldn't be a rich person saying capitalism is bad."
    },
    {
      "id": 490,
      "start": 1937.92,
      "end": 1945.92,
      "text": "It would be like if a rich person said capitalism is a force for good, but the economy, it needs to be leavened."
    },
    {
      "id": 491,
      "start": 1945.92,
      "end": 1946.92,
      "text": "It needs to be moderated, right?"
    },
    {
      "id": 492,
      "start": 1946.92,
      "end": 1950.92,
      "text": "You know, we need to deal with problems like pollution."
    },
    {
      "id": 493,
      "start": 1950.92,
      "end": 1954.92,
      "text": "We need to deal with problems like inequality, and, and then capitalism can be good."
    },
    {
      "id": 494,
      "start": 1954.92,
      "end": 1957.92,
      "text": "If we don't deal with those things, then capitalism might be bad."
    },
    {
      "id": 495,
      "start": 1957.92,
      "end": 1962.92,
      "text": "And so that is more analogous to the, to the position that I have here."
    },
    {
      "id": 496,
      "start": 1962.92,
      "end": 1966.92,
      "text": "The concept of consciousness."
    },
    {
      "id": 497,
      "start": 1966.92,
      "end": 1970.92,
      "text": "Where is that going?"
    },
    {
      "id": 498,
      "start": 1970.92,
      "end": 1972.92,
      "text": "And what does the AI think it is?"
    },
    {
      "id": 499,
      "start": 1972.92,
      "end": 1980.92,
      "text": "If AI truly were to, if AI were to question itself, would you, would you, would you think it thinks it's consciousness?"
    },
    {
      "id": 500,
      "start": 1980.92,
      "end": 1981.92,
      "text": "It has consciousness?"
    },
    {
      "id": 501,
      "start": 1981.92,
      "end": 1986.92,
      "text": "So, you know, this is one of these mysterious questions that we really don't have any kind of, you know, in the world."
    },
    {
      "id": 502,
      "start": 1986.92,
      "end": 1991.92,
      "text": "We don't know what human consciousness is, and therefore we don't know if AIs have it."
    },
    {
      "id": 503,
      "start": 1991.92,
      "end": 1992.92,
      "text": "What do you think it is?"
    },
    {
      "id": 504,
      "start": 1992.92,
      "end": 2003.92,
      "text": "So, you know, I, I suspect that it's an emergent property of, you know, systems that are complicated enough that kind of reflect on their own decisions."
    },
    {
      "id": 505,
      "start": 2003.92,
      "end": 2011.92,
      "text": "That, you know, it's, it's, it's, it's, it's, it's something that, uh, emerges from complex enough systems."
    },
    {
      "id": 506,
      "start": 2011.92,
      "end": 2022.92,
      "text": "And so, you know, I do think when our AI system, when our AI systems get advanced enough, I suspect they'll have something that, you know, resembles what we would call consciousness or moral significance."
    },
    {
      "id": 507,
      "start": 2022.92,
      "end": 2024.92,
      "text": "I do think it'll happen at some point."
    },
    {
      "id": 508,
      "start": 2024.92,
      "end": 2026.92,
      "text": "It may not be the same as human consciousness."
    },
    {
      "id": 509,
      "start": 2026.92,
      "end": 2032.92,
      "text": "You know, it may be different in how it works because the modalities are different because the things it's learned are different."
    },
    {
      "id": 510,
      "start": 2032.92,
      "end": 2042.92,
      "text": "But, you know, having, having studied the brain and the, you know, the way it's wired together, the models are, you know, different in some ways, but I don't think they're different in the fundamental ways that matter."
    },
    {
      "id": 511,
      "start": 2042.92,
      "end": 2059.92,
      "text": "So I, I am someone who, who does suspect that, uh, you know, at some point, even, even if I don't think they are today, I, I suspect that at some point the models will, you know, we would indeed say under, you know, most definitions that we would endorse that, you know, the models will be conscious."
    },
    {
      "id": 512,
      "start": 2059.92,
      "end": 2067.92,
      "text": "This is a question I keep asking myself when people talk to me about things like spirituality or consciousness."
    },
    {
      "id": 513,
      "start": 2067.92,
      "end": 2072.92,
      "text": "I feel like the world is very random."
    },
    {
      "id": 514,
      "start": 2072.92,
      "end": 2073.92,
      "text": "This is my view."
    },
    {
      "id": 515,
      "start": 2073.92,
      "end": 2078.92,
      "text": "And we are not far removed from cockroaches."
    },
    {
      "id": 516,
      "start": 2078.92,
      "end": 2081.92,
      "text": "When somebody stamps a cockroach, the cockroach dies."
    },
    {
      "id": 517,
      "start": 2081.92,
      "end": 2093.92,
      "text": "If there is something called consciousness, and if there is a collective consciousness, I've not been able to either connect with it or derive anything from it."
    },
    {
      "id": 518,
      "start": 2093.92,
      "end": 2095.92,
      "text": "Do you believe differently?"
    },
    {
      "id": 519,
      "start": 2095.92,
      "end": 2103.92,
      "text": "Um, I, you know, I, I don't think consciousness, you know, necessarily needs to meet needs to mean anything, you know, mystical, right?"
    },
    {
      "id": 520,
      "start": 2103.92,
      "end": 2121.92,
      "text": "Like, uh, you know, I, there's just some, there's some property of kind of being aware of your own existence and feeling things and, and, you know, um, uh, uh, uh, you know, being able to take in kind of a lot of information and reflect on that information and to, you know, feel a certain way and to notice yourself noticing something."
    },
    {
      "id": 521,
      "start": 2121.92,
      "end": 2136.92,
      "text": "Um, you know, uh, the, the, I think that the, the, you know, we can tell self evidently from our own experience that, that those properties, that those experiences exist, you know, what their, what their basis is, whether it's, you know, entirely."
    },
    {
      "id": 522,
      "start": 2136.92,
      "end": 2142.92,
      "text": "Materialistic or there's something more mystical going on, I think is, is, is, you know, obviously very hard to know."
    },
    {
      "id": 523,
      "start": 2142.92,
      "end": 2146.92,
      "text": "And, and, and, you know, I, I think it's ultimately not, not relevant to these questions."
    },
    {
      "id": 524,
      "start": 2146.92,
      "end": 2154.92,
      "text": "What, what does seem relevant to me is that, you know, these are, because we have, can observe our own experience."
    },
    {
      "id": 525,
      "start": 2154.92,
      "end": 2156.92,
      "text": "These are properties of human brains."
    },
    {
      "id": 526,
      "start": 2156.92,
      "end": 2165.92,
      "text": "Um, and, you know, I suspect that these models we are building as they get more sophisticated are becoming enough like human brains that they will have some of the same properties."
    },
    {
      "id": 527,
      "start": 2165.92,
      "end": 2168.92,
      "text": "That is, that is my guess as to, as to what will happen."
    },
    {
      "id": 528,
      "start": 2168.92,
      "end": 2171.92,
      "text": "And so we've taken, we've taken various interventions with the models."
    },
    {
      "id": 529,
      "start": 2171.92,
      "end": 2185.92,
      "text": "You know, we've given the models, um, we, you know, we call it a, I quit this job, um, button, uh, basically where, you know, that we've given the model the ability to basically terminate its conversations by saying, I don't want to be involved in the conversation."
    },
    {
      "id": 530,
      "start": 2185.92,
      "end": 2192.92,
      "text": "And, you know, models do that when, you know, they, they, they have to deal with, you know, particularly violent or brutal content."
    },
    {
      "id": 531,
      "start": 2192.92,
      "end": 2194.92,
      "text": "Um, it usually only happens in very extreme cases."
    },
    {
      "id": 532,
      "start": 2194.92,
      "end": 2197.92,
      "text": "So I've grown up here."
    },
    {
      "id": 533,
      "start": 2197.92,
      "end": 2198.92,
      "text": "This is my city, Bangalore."
    },
    {
      "id": 534,
      "start": 2198.92,
      "end": 2200.92,
      "text": "Uh, I, I've grown up in the Southern part of the world."
    },
    {
      "id": 535,
      "start": 2200.92,
      "end": 2201.92,
      "text": "I've grown up in the Southern part."
    },
    {
      "id": 536,
      "start": 2201.92,
      "end": 2203.92,
      "text": "We are in the Northern part of the city right now."
    },
    {
      "id": 537,
      "start": 2203.92,
      "end": 2218.92,
      "text": "As somebody who saw the boom of the IT services industry here, uh, big employer, employs a lot of people, a big part of how the city grew."
    },
    {
      "id": 538,
      "start": 2218.92,
      "end": 2221.92,
      "text": "What is India's role in all this?"
    },
    {
      "id": 539,
      "start": 2221.92,
      "end": 2222.92,
      "text": "Yeah."
    },
    {
      "id": 540,
      "start": 2222.92,
      "end": 2224.92,
      "text": "So, you know, this is my second time in India."
    },
    {
      "id": 541,
      "start": 2224.92,
      "end": 2229.92,
      "text": "I visited in, in October and, you know, um, uh, you know, the last time I came here, I was,"
    },
    {
      "id": 542,
      "start": 2229.92,
      "end": 2235.92,
      "text": "you know, I, I met with all the, you know, the major kind of Indian IT and, and just conglomerates more generally."
    },
    {
      "id": 543,
      "start": 2235.92,
      "end": 2244.92,
      "text": "I won't give names, but you know, the usual ones you would, you would, you would, you would think of, um, you know, and then we're beginning to work with, with most or most or all of them."
    },
    {
      "id": 544,
      "start": 2244.92,
      "end": 2248.92,
      "text": "And, you know, one of the things I said is look, Anthropic is an enterprise company."
    },
    {
      "id": 545,
      "start": 2248.92,
      "end": 2250.92,
      "text": "Its job is to serve other consumers."
    },
    {
      "id": 546,
      "start": 2250.92,
      "end": 2257.92,
      "text": "Um, you know, many other companies come here as themselves, a consumer company, and they see, they see India as, as a market, right?"
    },
    {
      "id": 547,
      "start": 2257.92,
      "end": 2259.92,
      "text": "A place to obtain consumers."
    },
    {
      "id": 548,
      "start": 2259.92,
      "end": 2289.92,
      "text": "They see bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending bending"
    },
    {
      "id": 549,
      "start": 2289.92,
      "end": 2291.12,
      "text": "for the Indian market."
    },
    {
      "id": 550,
      "start": 2291.12,
      "end": 2294.9,
      "text": "And so our hope is that we can add AI to what they do"
    },
    {
      "id": 551,
      "start": 2294.9,
      "end": 2296.7400000000002,
      "text": "and kind of enhance what they do, right?"
    },
    {
      "id": 552,
      "start": 2296.7400000000002,
      "end": 2300.38,
      "text": "There's a lot of worry that AI could replace SaaS"
    },
    {
      "id": 553,
      "start": 2300.38,
      "end": 2301.54,
      "text": "or all of these things,"
    },
    {
      "id": 554,
      "start": 2301.54,
      "end": 2303.96,
      "text": "but my view is if we do this in the right way,"
    },
    {
      "id": 555,
      "start": 2303.96,
      "end": 2305.9,
      "text": "if we work with all these companies,"
    },
    {
      "id": 556,
      "start": 2305.9,
      "end": 2308.94,
      "text": "then AI can enhance what they're doing,"
    },
    {
      "id": 557,
      "start": 2308.94,
      "end": 2313.44,
      "text": "can enhance their connection to the market,"
    },
    {
      "id": 558,
      "start": 2313.44,
      "end": 2315.16,
      "text": "their go-to-market abilities,"
    },
    {
      "id": 559,
      "start": 2315.16,
      "end": 2316.98,
      "text": "and their specific know-how."
    },
    {
      "id": 560,
      "start": 2316.98,
      "end": 2320.36,
      "text": "I really like the steam engine story."
    },
    {
      "id": 561,
      "start": 2320.36,
      "end": 2323.2,
      "text": "When the steam engine was invented,"
    },
    {
      "id": 562,
      "start": 2323.2,
      "end": 2326.36,
      "text": "how the world changed, productivity went up."
    },
    {
      "id": 563,
      "start": 2328.96,
      "end": 2329.88,
      "text": "People had more."
    },
    {
      "id": 564,
      "start": 2331.52,
      "end": 2336.16,
      "text": "The thing I worry about is at the beginning of a change,"
    },
    {
      "id": 565,
      "start": 2336.16,
      "end": 2339.06,
      "text": "you need a human to operate the steam engine."
    },
    {
      "id": 566,
      "start": 2340.34,
      "end": 2343.38,
      "text": "Then you have assembly lines and all of that."
    },
    {
      "id": 567,
      "start": 2343.38,
      "end": 2346.66,
      "text": "Eventually, the way the world is moving,"
    },
    {
      "id": 568,
      "start": 2346.98,
      "end": 2350.36,
      "text": "the human becomes less and less relevant with time"
    },
    {
      "id": 569,
      "start": 2350.36,
      "end": 2352.2,
      "text": "as these models get smarter."
    },
    {
      "id": 570,
      "start": 2352.2,
      "end": 2358.2,
      "text": "So if you here partner with the IT services companies today"
    },
    {
      "id": 571,
      "start": 2358.2,
      "end": 2361.36,
      "text": "and there is a use case for them,"
    },
    {
      "id": 572,
      "start": 2361.36,
      "end": 2365.36,
      "text": "are they not much like the man behind the steam engine"
    },
    {
      "id": 573,
      "start": 2365.36,
      "end": 2367.2,
      "text": "10 years from now where the relevance,"
    },
    {
      "id": 574,
      "start": 2367.2,
      "end": 2370.36,
      "text": "if the tool works so simply that you don't need an operator,"
    },
    {
      "id": 575,
      "start": 2370.36,
      "end": 2372.36,
      "text": "eventually what happens to the operator?"
    },
    {
      "id": 576,
      "start": 2372.36,
      "end": 2375.7400000000002,
      "text": "So I think a few things are true all at once."
    },
    {
      "id": 577,
      "start": 2375.7400000000002,
      "end": 2379.7400000000002,
      "text": "One is that definitely the scope of automation of the agents"
    },
    {
      "id": 578,
      "start": 2379.7400000000002,
      "end": 2381.58,
      "text": "is going to expand over time."
    },
    {
      "id": 579,
      "start": 2381.58,
      "end": 2383.08,
      "text": "That is definitely the case."
    },
    {
      "id": 580,
      "start": 2383.08,
      "end": 2385.58,
      "text": "You know, I think that's a problem for everyone."
    },
    {
      "id": 581,
      "start": 2385.58,
      "end": 2386.58,
      "text": "That's a problem for us."
    },
    {
      "id": 582,
      "start": 2386.58,
      "end": 2388.42,
      "text": "That's a problem for consumers."
    },
    {
      "id": 583,
      "start": 2388.42,
      "end": 2393.58,
      "text": "That's a, you know, it's not just a problem for the IT companies."
    },
    {
      "id": 584,
      "start": 2393.58,
      "end": 2398.7999999999997,
      "text": "What I think will happen though is other moats will become more important."
    },
    {
      "id": 585,
      "start": 2398.7999999999997,
      "end": 2401.96,
      "text": "For example, the models have not done a lot in the physical world."
    },
    {
      "id": 586,
      "start": 2401.96,
      "end": 2403.96,
      "text": "They may at some point, you know, I think, you know,"
    },
    {
      "id": 587,
      "start": 2403.96,
      "end": 2406.68,
      "text": "robotics will happen at some point, but I think it's,"
    },
    {
      "id": 588,
      "start": 2406.68,
      "end": 2409.56,
      "text": "that's a distinct thing from what's happening now with AI."
    },
    {
      "id": 589,
      "start": 2409.56,
      "end": 2413.7599999999998,
      "text": "So, you know, a lot of this involves, you know, things in the physical world."
    },
    {
      "id": 590,
      "start": 2413.7599999999998,
      "end": 2416.7599999999998,
      "text": "Another thing is things that are human centric, right?"
    },
    {
      "id": 591,
      "start": 2416.76,
      "end": 2423.76,
      "text": "Some of these IT companies are also consulting companies and they have a big web of relationships with other,"
    },
    {
      "id": 592,
      "start": 2423.76,
      "end": 2428.98,
      "text": "you know, with other humans, with other institutions here in India or, you know, or across the world."
    },
    {
      "id": 593,
      "start": 2428.98,
      "end": 2433.0600000000004,
      "text": "And I think those relationships are going to become increasingly important, right?"
    },
    {
      "id": 594,
      "start": 2433.0600000000004,
      "end": 2441.0600000000004,
      "text": "You know, some of these are combined technology and sort of, you know, consulting or like integration companies."
    },
    {
      "id": 595,
      "start": 2441.0600000000004,
      "end": 2445.86,
      "text": "And I think a lot of it is, you know, knowing how institutions work."
    },
    {
      "id": 596,
      "start": 2445.86,
      "end": 2448.92,
      "text": "And so being able to, you know, integrate things with institutions,"
    },
    {
      "id": 597,
      "start": 2448.92,
      "end": 2453.1600000000003,
      "text": "being able to work with them to make things happen faster than they would have otherwise."
    },
    {
      "id": 598,
      "start": 2453.1600000000003,
      "end": 2457.86,
      "text": "And I think that, I think that element, you know, if, if nothing else is, is, you know,"
    },
    {
      "id": 599,
      "start": 2457.86,
      "end": 2460.04,
      "text": "is going to continue to be valuable in the long run."
    },
    {
      "id": 600,
      "start": 2460.04,
      "end": 2463.54,
      "text": "You know, at the end of the day, like it just, it just comes down to humans, right?"
    },
    {
      "id": 601,
      "start": 2463.54,
      "end": 2465.92,
      "text": "All of this is supposed to be being done for the benefit of humans."
    },
    {
      "id": 602,
      "start": 2465.92,
      "end": 2471.42,
      "text": "So it, you know, there's, there's always going to be some human centric element of this."
    },
    {
      "id": 603,
      "start": 2471.6800000000003,
      "end": 2472.6800000000003,
      "text": "That's going to be important."
    },
    {
      "id": 604,
      "start": 2472.68,
      "end": 2476.44,
      "text": "And I suspect there will be other moats that we haven't thought about, you know?"
    },
    {
      "id": 605,
      "start": 2476.44,
      "end": 2482.2799999999997,
      "text": "So, you know, there's this concept called Amdahl's law, which is, you know, if you have a process"
    },
    {
      "id": 606,
      "start": 2482.2799999999997,
      "end": 2486.8799999999997,
      "text": "that has many components and you speed up some of the components, the, the, the components"
    },
    {
      "id": 607,
      "start": 2486.8799999999997,
      "end": 2489.22,
      "text": "that haven't yet been sped up become the limiting factor."
    },
    {
      "id": 608,
      "start": 2489.22,
      "end": 2490.64,
      "text": "They become the most important thing."
    },
    {
      "id": 609,
      "start": 2491.02,
      "end": 2494.44,
      "text": "And, and, you know, you might not have thought about them at all, right?"
    },
    {
      "id": 610,
      "start": 2494.44,
      "end": 2498.24,
      "text": "You might not have thought of them as moats or important components, but, you know,"
    },
    {
      "id": 611,
      "start": 2498.24,
      "end": 2504.5,
      "text": "when writing software becomes a lot easier, you know, some of the moats that, you know,"
    },
    {
      "id": 612,
      "start": 2504.5,
      "end": 2507.4199999999996,
      "text": "companies have will go away, but others will become even more important."
    },
    {
      "id": 613,
      "start": 2507.4199999999996,
      "end": 2509.2799999999997,
      "text": "So there will be a bunch of adjustment."
    },
    {
      "id": 614,
      "start": 2509.2799999999997,
      "end": 2513.6,
      "text": "Folks will have to say, oh man, the stuff we thought was really important before isn't"
    },
    {
      "id": 615,
      "start": 2513.6,
      "end": 2514.3799999999997,
      "text": "as important."
    },
    {
      "id": 616,
      "start": 2514.3799999999997,
      "end": 2518.12,
      "text": "Whereas these other advantages that we never really thought of as advantages are now super"
    },
    {
      "id": 617,
      "start": 2518.12,
      "end": 2518.58,
      "text": "important."
    },
    {
      "id": 618,
      "start": 2518.58,
      "end": 2523.7,
      "text": "So I guess what I would say is, you know, companies will need to adapt very fast and"
    },
    {
      "id": 619,
      "start": 2523.7,
      "end": 2528.46,
      "text": "think about what really matters for them, what their real advantages are."
    },
    {
      "id": 620,
      "start": 2528.46,
      "end": 2533.62,
      "text": "Um, but, but I think some of those advantages are gonna, are gonna, are gonna stay around"
    },
    {
      "id": 621,
      "start": 2533.62,
      "end": 2537.24,
      "text": "because, you know, while the technology is very broad, it does have its limits."
    },
    {
      "id": 622,
      "start": 2537.24,
      "end": 2539.42,
      "text": "I don't know if I buy that fully."
    },
    {
      "id": 623,
      "start": 2539.42,
      "end": 2548.06,
      "text": "I think I see the diminishing returns for being a service provider, even if the moat is the network"
    },
    {
      "id": 624,
      "start": 2548.06,
      "end": 2555.5,
      "text": "and relationships they hold today, because if I am using open claw to maneuver some of"
    },
    {
      "id": 625,
      "start": 2555.5,
      "end": 2560.62,
      "text": "my relationships in the conversations, I don't know if it's too far-fetched to assume that"
    },
    {
      "id": 626,
      "start": 2560.62,
      "end": 2565.34,
      "text": "most conversations tomorrow and relationships will be maintained by an agent like that."
    },
    {
      "id": 627,
      "start": 2565.34,
      "end": 2568.5,
      "text": "But you know, if you, if you just think of the chain of companies, right?"
    },
    {
      "id": 628,
      "start": 2568.5,
      "end": 2572.6,
      "text": "At the end of the day, you're dealing with consumers, right?"
    },
    {
      "id": 629,
      "start": 2572.6,
      "end": 2575.94,
      "text": "Like at, like at the end of the day, you have to deal with people."
    },
    {
      "id": 630,
      "start": 2575.94,
      "end": 2579.34,
      "text": "You know, there's this story of like, you know, I think it was Jeff Hinton predicted,"
    },
    {
      "id": 631,
      "start": 2579.34,
      "end": 2583.06,
      "text": "you know, that, that, that AI will replace radiologists."
    },
    {
      "id": 632,
      "start": 2583.06,
      "end": 2589.3,
      "text": "And indeed AI has gotten better than radiologists at, you know, at doing scans, right?"
    },
    {
      "id": 633,
      "start": 2589.3,
      "end": 2593.34,
      "text": "But what happens today is there aren't less radiologists."
    },
    {
      "id": 634,
      "start": 2593.34,
      "end": 2597.54,
      "text": "What the radiologist does is they walk the patient through the scan and they kind of talk"
    },
    {
      "id": 635,
      "start": 2597.54,
      "end": 2598.54,
      "text": "to the patient."
    },
    {
      "id": 636,
      "start": 2598.54,
      "end": 2603.14,
      "text": "The, the most highly technical part of the job has gone away, but somehow there's still"
    },
    {
      "id": 637,
      "start": 2603.14,
      "end": 2609.46,
      "text": "some demand for like, you know, the, the kind of, the kind of underlying human skill."
    },
    {
      "id": 638,
      "start": 2609.46,
      "end": 2611.94,
      "text": "Now that may not be true everywhere."
    },
    {
      "id": 639,
      "start": 2611.94,
      "end": 2617.82,
      "text": "And, you know, perhaps over time, AI will advance in, in, you know, areas where it, where it hasn't,"
    },
    {
      "id": 640,
      "start": 2617.82,
      "end": 2618.86,
      "text": "hasn't yet advanced."
    },
    {
      "id": 641,
      "start": 2618.86,
      "end": 2622.02,
      "text": "And, you know, maybe, maybe, maybe that'll happen fast."
    },
    {
      "id": 642,
      "start": 2622.02,
      "end": 2626.1,
      "text": "But, you know, what I think, I think what I will say is like, you know, we should take"
    },
    {
      "id": 643,
      "start": 2626.1,
      "end": 2627.9,
      "text": "it one step at a time, right?"
    },
    {
      "id": 644,
      "start": 2627.9,
      "end": 2631.22,
      "text": "This is a very empirical science."
    },
    {
      "id": 645,
      "start": 2631.22,
      "end": 2632.94,
      "text": "This is a very empirical observation."
    },
    {
      "id": 646,
      "start": 2632.94,
      "end": 2635.98,
      "text": "Let's see what AI does, you know, today."
    },
    {
      "id": 647,
      "start": 2635.98,
      "end": 2644.2599999999998,
      "text": "And like, we'll, we'll kind of try and adapt to, you know, kind of try and adapt to that."
    },
    {
      "id": 648,
      "start": 2644.26,
      "end": 2648.6200000000003,
      "text": "The kind of system starts to figure it out and then, then we'll see, then we'll see what"
    },
    {
      "id": 649,
      "start": 2648.6200000000003,
      "end": 2649.6200000000003,
      "text": "happens next."
    },
    {
      "id": 650,
      "start": 2649.6200000000003,
      "end": 2654.2000000000003,
      "text": "I, you know, I do think, you know, in the long run, will AI be better than, than us at, at,"
    },
    {
      "id": 651,
      "start": 2654.2000000000003,
      "end": 2655.2000000000003,
      "text": "at basically everything?"
    },
    {
      "id": 652,
      "start": 2655.2000000000003,
      "end": 2659.6400000000003,
      "text": "Will it be better than most humans, you know, including even the physical world and robotics"
    },
    {
      "id": 653,
      "start": 2659.6400000000003,
      "end": 2661.1400000000003,
      "text": "and the human touch?"
    },
    {
      "id": 654,
      "start": 2661.1400000000003,
      "end": 2662.1400000000003,
      "text": "Yeah."
    },
    {
      "id": 655,
      "start": 2662.1400000000003,
      "end": 2667.84,
      "text": "I, you know, I think that is, I, you know, I think, I think that is, you know, possible,"
    },
    {
      "id": 656,
      "start": 2667.84,
      "end": 2668.84,
      "text": "maybe even likely."
    },
    {
      "id": 657,
      "start": 2668.84,
      "end": 2672.46,
      "text": "It's something that goes beyond the country of geniuses in a data center I described, because"
    },
    {
      "id": 658,
      "start": 2672.46,
      "end": 2674.78,
      "text": "that's purely virtual."
    },
    {
      "id": 659,
      "start": 2674.78,
      "end": 2679.3,
      "text": "But you know, building robots is something, you know, something, it's a skill, it's something"
    },
    {
      "id": 660,
      "start": 2679.3,
      "end": 2680.3,
      "text": "you can do."
    },
    {
      "id": 661,
      "start": 2680.3,
      "end": 2685.86,
      "text": "So maybe the AIs will make us, will make us better at that as well."
    },
    {
      "id": 662,
      "start": 2685.86,
      "end": 2690.38,
      "text": "But you know, the, the, the way I think about it is, you know, we need, we need to take the,"
    },
    {
      "id": 663,
      "start": 2690.38,
      "end": 2694.16,
      "text": "we need to figure this out step by step and figure out how to adapt to it."
    },
    {
      "id": 664,
      "start": 2694.16,
      "end": 2701.4,
      "text": "This might sound a bit self-serving to the people who know me, because I believe the reason"
    },
    {
      "id": 665,
      "start": 2701.4,
      "end": 2708.06,
      "text": "so much risk capital exists in America, not the only reason, but one of the big reasons"
    },
    {
      "id": 666,
      "start": 2708.06,
      "end": 2713.1600000000003,
      "text": "is how big your stock market is and how much of an opportunity it is for this risk capital"
    },
    {
      "id": 667,
      "start": 2713.1600000000003,
      "end": 2715.46,
      "text": "to exit eventually."
    },
    {
      "id": 668,
      "start": 2715.46,
      "end": 2720.14,
      "text": "It's a case for why India should really allow for our stock markets to flourish."
    },
    {
      "id": 669,
      "start": 2720.14,
      "end": 2724.34,
      "text": "The audience that I speak to is very much the wannabe entrepreneur in India."
    },
    {
      "id": 670,
      "start": 2724.34,
      "end": 2725.58,
      "text": "What can they do in AI?"
    },
    {
      "id": 671,
      "start": 2725.58,
      "end": 2727.16,
      "text": "What is an actual opportunity?"
    },
    {
      "id": 672,
      "start": 2727.16,
      "end": 2733.36,
      "text": "I think there's a lot of opportunities around building at kind of the application layer."
    },
    {
      "id": 673,
      "start": 2733.36,
      "end": 2735.5,
      "text": "We release a new model every two or three months."
    },
    {
      "id": 674,
      "start": 2735.5,
      "end": 2740.34,
      "text": "And so there's an opportunity every two or three months to build some new thing that wasn't"
    },
    {
      "id": 675,
      "start": 2740.34,
      "end": 2745.34,
      "text": "possible before, that wouldn't have worked before because the models were weak."
    },
    {
      "id": 676,
      "start": 2745.34,
      "end": 2748.66,
      "text": "People in fact say people were, you know, the majority of our revenue still comes from"
    },
    {
      "id": 677,
      "start": 2748.66,
      "end": 2750.34,
      "text": "the API model."
    },
    {
      "id": 678,
      "start": 2750.34,
      "end": 2755.58,
      "text": "People say that, you know, API models aren't viable or that they'll be commoditized or whatever."
    },
    {
      "id": 679,
      "start": 2755.58,
      "end": 2760.92,
      "text": "I think what people are not seeing is there's this expanding sphere of what is possible with"
    },
    {
      "id": 680,
      "start": 2760.92,
      "end": 2766.7400000000002,
      "text": "AI and the API allows, you know, this new startup to try making something that, you know, wasn't"
    },
    {
      "id": 681,
      "start": 2766.7400000000002,
      "end": 2768.46,
      "text": "possible before."
    },
    {
      "id": 682,
      "start": 2768.46,
      "end": 2772.8,
      "text": "And this is why the API is such a flourishing business and it's constantly in motion."
    },
    {
      "id": 683,
      "start": 2772.8,
      "end": 2774.02,
      "text": "It's constantly in churn."
    },
    {
      "id": 684,
      "start": 2774.02,
      "end": 2777.1000000000004,
      "text": "And so it doesn't, you know, it doesn't get commoditized."
    },
    {
      "id": 685,
      "start": 2777.1000000000004,
      "end": 2779.32,
      "text": "It's a very dynamic thing."
    },
    {
      "id": 686,
      "start": 2779.32,
      "end": 2783.9,
      "text": "And so I think there's an opportunity for lots of individuals to just say, you know,"
    },
    {
      "id": 687,
      "start": 2783.9,
      "end": 2785.06,
      "text": "what can I build?"
    },
    {
      "id": 688,
      "start": 2785.06,
      "end": 2788.2000000000003,
      "text": "Well, you know, what can I build on top of these models with an API?"
    },
    {
      "id": 689,
      "start": 2788.2000000000003,
      "end": 2793.32,
      "text": "Like, you know, what are the things that I can make that others cannot make?"
    },
    {
      "id": 690,
      "start": 2793.32,
      "end": 2796.1200000000003,
      "text": "You know, what are some new ideas?"
    },
    {
      "id": 691,
      "start": 2796.1200000000003,
      "end": 2800.6400000000003,
      "text": "And you know, we've seen that, you know, we see both with the API itself and with Claude"
    },
    {
      "id": 692,
      "start": 2800.6400000000003,
      "end": 2801.6400000000003,
      "text": "Code."
    },
    {
      "id": 693,
      "start": 2801.6400000000003,
      "end": 2808.56,
      "text": "You know, I think the number of users and the number of revenue we've seen in India has"
    },
    {
      "id": 694,
      "start": 2808.56,
      "end": 2811.14,
      "text": "doubled since I last visited in October."
    },
    {
      "id": 695,
      "start": 2811.14,
      "end": 2816.36,
      "text": "So that was what November, December is like three, three and a half months since I visited."
    },
    {
      "id": 696,
      "start": 2816.36,
      "end": 2817.44,
      "text": "It's doubled."
    },
    {
      "id": 697,
      "start": 2817.44,
      "end": 2820.02,
      "text": "But I'm going to be candid here, Dario."
    },
    {
      "id": 698,
      "start": 2820.02,
      "end": 2824.02,
      "text": "You're a company which is worth, I don't know, 400 billion or 380 billion today."
    },
    {
      "id": 699,
      "start": 2824.02,
      "end": 2826.32,
      "text": "You've raised 35 billion."
    },
    {
      "id": 700,
      "start": 2826.32,
      "end": 2831.7400000000002,
      "text": "You do 15 billion of revenue, but going up really, really fast."
    },
    {
      "id": 701,
      "start": 2831.74,
      "end": 2837.66,
      "text": "If I build an application on top of Claude, that for some reason, I'm sitting in Bangalore"
    },
    {
      "id": 702,
      "start": 2837.66,
      "end": 2842.9199999999996,
      "text": "in J.P. Nagar and building this, that for some reason happens to work for a short period"
    },
    {
      "id": 703,
      "start": 2842.9199999999996,
      "end": 2845.9599999999996,
      "text": "of time."
    },
    {
      "id": 704,
      "start": 2845.9599999999996,
      "end": 2850.9399999999996,
      "text": "It is but a matter of time before you would want to onboard that revenue and not let that"
    },
    {
      "id": 705,
      "start": 2850.9399999999996,
      "end": 2852.2999999999997,
      "text": "lie with me."
    },
    {
      "id": 706,
      "start": 2852.2999999999997,
      "end": 2857.64,
      "text": "And you will probably better that application in a manner that I will never be able to."
    },
    {
      "id": 707,
      "start": 2857.64,
      "end": 2863.74,
      "text": "I've heard this argument for different people like the Harvey, the legal AI company in New"
    },
    {
      "id": 708,
      "start": 2863.74,
      "end": 2868.46,
      "text": "York, they're friends of mine, and they were talking about how they built on top of OpenAI,"
    },
    {
      "id": 709,
      "start": 2868.46,
      "end": 2874.14,
      "text": "but eventually they don't know if it's an easy fix for OpenAI to do what they're doing."
    },
    {
      "id": 710,
      "start": 2874.14,
      "end": 2879.7999999999997,
      "text": "So even if I were to build it, say you put out a model in three months or six months, what"
    },
    {
      "id": 711,
      "start": 2879.7999999999997,
      "end": 2886.46,
      "text": "is to stop you from taking that revenue center away from me and onto yourself in a certain"
    },
    {
      "id": 712,
      "start": 2886.46,
      "end": 2887.46,
      "text": "period of time?"
    },
    {
      "id": 713,
      "start": 2887.46,
      "end": 2888.46,
      "text": "Yeah."
    },
    {
      "id": 714,
      "start": 2888.46,
      "end": 2890.62,
      "text": "So I think there's a few things here."
    },
    {
      "id": 715,
      "start": 2890.62,
      "end": 2897.02,
      "text": "One is I would give the advice that I give to basically any business and say, a business"
    },
    {
      "id": 716,
      "start": 2897.02,
      "end": 2899.2,
      "text": "should establish a moat."
    },
    {
      "id": 717,
      "start": 2899.2,
      "end": 2901.64,
      "text": "You shouldn't be just a rapper."
    },
    {
      "id": 718,
      "start": 2901.64,
      "end": 2907.36,
      "text": "I would not advise that you just say, oh, here's a way to interact with Claude."
    },
    {
      "id": 719,
      "start": 2907.36,
      "end": 2910.86,
      "text": "I'm going to prompt Claude a little bit or I'm going to build a little bit of a UI around"
    },
    {
      "id": 720,
      "start": 2910.86,
      "end": 2911.86,
      "text": "Claude."
    },
    {
      "id": 721,
      "start": 2911.86,
      "end": 2912.86,
      "text": "That doesn't have a moat."
    },
    {
      "id": 722,
      "start": 2912.86,
      "end": 2917.6200000000003,
      "text": "And you shouldn't be worried about Anthropic in particular eating that revenue."
    },
    {
      "id": 723,
      "start": 2917.6200000000003,
      "end": 2918.6200000000003,
      "text": "Anyone can eat that revenue."
    },
    {
      "id": 724,
      "start": 2918.6200000000003,
      "end": 2919.6200000000003,
      "text": "Right?"
    },
    {
      "id": 725,
      "start": 2919.6200000000003,
      "end": 2920.6200000000003,
      "text": "It's not super valuable."
    },
    {
      "id": 726,
      "start": 2920.6200000000003,
      "end": 2927.1,
      "text": "But what I would say is that in different fields, there are different kinds of moats"
    },
    {
      "id": 727,
      "start": 2927.1,
      "end": 2932.36,
      "text": "where you can do something that it would be difficult for Anthropic to do."
    },
    {
      "id": 728,
      "start": 2932.36,
      "end": 2934.1800000000003,
      "text": "And we don't want to specialize in it."
    },
    {
      "id": 729,
      "start": 2934.1800000000003,
      "end": 2940.6600000000003,
      "text": "So for example, there's a lot of stuff in the bio cross AI space that builds on our API."
    },
    {
      "id": 730,
      "start": 2940.66,
      "end": 2942.72,
      "text": "They want to do biological discovery."
    },
    {
      "id": 731,
      "start": 2942.72,
      "end": 2947.64,
      "text": "I happen to be a biologist, but most people at Anthropic aren't biologists."
    },
    {
      "id": 732,
      "start": 2947.64,
      "end": 2951.2999999999997,
      "text": "They're AI scientists or they're product people or go-to-market people."
    },
    {
      "id": 733,
      "start": 2951.2999999999997,
      "end": 2958.48,
      "text": "So it's just really inefficient for us to step in that space and do all that work."
    },
    {
      "id": 734,
      "start": 2958.48,
      "end": 2963.42,
      "text": "The same would be applied for dealing with financial services industry, right?"
    },
    {
      "id": 735,
      "start": 2963.42,
      "end": 2965.66,
      "text": "Where there's a huge amount of regulation."
    },
    {
      "id": 736,
      "start": 2965.66,
      "end": 2968.72,
      "text": "You need to know a bunch of stuff to comply with that regulation."
    },
    {
      "id": 737,
      "start": 2968.72,
      "end": 2972.04,
      "text": "Like, you know, it just doesn't make sense for us to do that."
    },
    {
      "id": 738,
      "start": 2972.04,
      "end": 2975.16,
      "text": "Now, there are some things that do make sense for us to do."
    },
    {
      "id": 739,
      "start": 2975.16,
      "end": 2978.96,
      "text": "Like, you know, we're not going to promise never to build first party products, right?"
    },
    {
      "id": 740,
      "start": 2978.96,
      "end": 2981.44,
      "text": "That we should be honest about."
    },
    {
      "id": 741,
      "start": 2981.44,
      "end": 2984.42,
      "text": "For example, a bunch of people at Anthropic write code."
    },
    {
      "id": 742,
      "start": 2984.42,
      "end": 2988.06,
      "text": "And so, you know, we made this internal tool called Claude Code."
    },
    {
      "id": 743,
      "start": 2988.06,
      "end": 2992.48,
      "text": "And because we ourselves write code, we have, you know, I think a special and unique insight"
    },
    {
      "id": 744,
      "start": 2992.48,
      "end": 2997.52,
      "text": "into, you know, how to use the how to best use the AI models to write code."
    },
    {
      "id": 745,
      "start": 2997.52,
      "end": 3002.98,
      "text": "So, you know, I think in the code space, you know, we've become very strong, very strong"
    },
    {
      "id": 746,
      "start": 3002.98,
      "end": 3005.2,
      "text": "competitors because this is something we use ourself."
    },
    {
      "id": 747,
      "start": 3005.2,
      "end": 3008.58,
      "text": "But I don't think that generalizes to every possible industry."
    },
    {
      "id": 748,
      "start": 3010.24,
      "end": 3015.48,
      "text": "Again, going back to my audience, which is the 20 or 25-year-old boy or girl in India,"
    },
    {
      "id": 749,
      "start": 3016.02,
      "end": 3020.78,
      "text": "what industry do you think will get disrupted?"
    },
    {
      "id": 750,
      "start": 3020.78,
      "end": 3023.2400000000002,
      "text": "And what has a certain runway left?"
    },
    {
      "id": 751,
      "start": 3023.5600000000004,
      "end": 3029.36,
      "text": "I'm asking from the lens of, I'm trying to figure out what book to read, which college"
    },
    {
      "id": 752,
      "start": 3029.36,
      "end": 3034.0600000000004,
      "text": "to go to, what skill set to learn."
    },
    {
      "id": 753,
      "start": 3034.82,
      "end": 3040.46,
      "text": "If I'm starting a startup today, what has some kind of a tailwind?"
    },
    {
      "id": 754,
      "start": 3040.98,
      "end": 3041.1800000000003,
      "text": "Yeah."
    },
    {
      "id": 755,
      "start": 3041.34,
      "end": 3043.0600000000004,
      "text": "For a short period of time is okay as well."
    },
    {
      "id": 756,
      "start": 3043.2400000000002,
      "end": 3043.86,
      "text": "But what has a tailwind?"
    },
    {
      "id": 757,
      "start": 3043.88,
      "end": 3047.96,
      "text": "You know, I would think about tasks that are human-centered."
    },
    {
      "id": 758,
      "start": 3047.96,
      "end": 3052.5,
      "text": "You know, tasks that involve relating to people."
    },
    {
      "id": 759,
      "start": 3052.84,
      "end": 3057.64,
      "text": "You know, I think that the stuff like code and software engineering is, you know, is becoming"
    },
    {
      "id": 760,
      "start": 3057.64,
      "end": 3060.2200000000003,
      "text": "more and more kind of AI-focused."
    },
    {
      "id": 761,
      "start": 3060.52,
      "end": 3062.02,
      "text": "You know, things like math and science."
    },
    {
      "id": 762,
      "start": 3062.2,
      "end": 3063.36,
      "text": "Is that coding or engineering?"
    },
    {
      "id": 763,
      "start": 3063.5,
      "end": 3069.0,
      "text": "If I were to segregate coding and engineering to be two completely different things, is coding"
    },
    {
      "id": 764,
      "start": 3069.0,
      "end": 3074.0,
      "text": "going away or is engineering element of software where you're an architect trying to figure out?"
    },
    {
      "id": 765,
      "start": 3074.0,
      "end": 3080.12,
      "text": "And I think coding is going away first or coding is being, you know, done by the AI models first."
    },
    {
      "id": 766,
      "start": 3080.3,
      "end": 3083.42,
      "text": "And then the broader task of software engineering will take longer."
    },
    {
      "id": 767,
      "start": 3083.68,
      "end": 3088.88,
      "text": "But I think that is, you know, doing that end-to-end, I think that is going to happen as well, I would say."
    },
    {
      "id": 768,
      "start": 3088.88,
      "end": 3100.76,
      "text": "But, you know, again, the elements of, like, you know, design or making something that's useful to users or knowing what the demand is or, you know, managing teams of, like, AI models."
    },
    {
      "id": 769,
      "start": 3100.76,
      "end": 3104.3,
      "text": "Like, you know, those things may still be present."
    },
    {
      "id": 770,
      "start": 3104.44,
      "end": 3108.6400000000003,
      "text": "Again, like, there's this comparative advantage is surprisingly powerful, right?"
    },
    {
      "id": 771,
      "start": 3108.64,
      "end": 3118.42,
      "text": "Even if you're only doing, like, you know, 5% of the task, like, you know, that 5% gets super amplified and levered because it's, like, you're only doing 5% of the task."
    },
    {
      "id": 772,
      "start": 3118.98,
      "end": 3124.1,
      "text": "The AI does the other 95%, and so you become, you know, 20 times more productive."
    },
    {
      "id": 773,
      "start": 3124.18,
      "end": 3127.6,
      "text": "Again, at some point, you get to 99%, and then it becomes harder."
    },
    {
      "id": 774,
      "start": 3127.6,
      "end": 3134.94,
      "text": "But I think there's surprisingly much in that sort of, you know, in that zone of comparative advantage."
    },
    {
      "id": 775,
      "start": 3135.2799999999997,
      "end": 3138.38,
      "text": "But I would really think about the things that are human-centered."
    },
    {
      "id": 776,
      "start": 3138.56,
      "end": 3141.48,
      "text": "Like, I think there's something to that."
    },
    {
      "id": 777,
      "start": 3141.86,
      "end": 3154.64,
      "text": "I think there's something to kind of the physical world or things that mix together human-centered, the physical world, one of those two, and analytical skills that somehow tie them together."
    },
    {
      "id": 778,
      "start": 3154.64,
      "end": 3157.54,
      "text": "You know, similar to the radiologist example I gave."
    },
    {
      "id": 779,
      "start": 3158.0,
      "end": 3159.0,
      "text": "So what would I study?"
    },
    {
      "id": 780,
      "start": 3159.2,
      "end": 3161.12,
      "text": "Say I'm actual use case."
    },
    {
      "id": 781,
      "start": 3161.18,
      "end": 3162.3399999999997,
      "text": "I'm 25 years old."
    },
    {
      "id": 782,
      "start": 3162.7999999999997,
      "end": 3164.92,
      "text": "I'm trying to pick a profession for myself."
    },
    {
      "id": 783,
      "start": 3165.36,
      "end": 3167.04,
      "text": "I want some kind of tailwind."
    },
    {
      "id": 784,
      "start": 3167.4,
      "end": 3170.14,
      "text": "My outcome is a capitalistic win in the next decade."
    },
    {
      "id": 785,
      "start": 3170.72,
      "end": 3176.54,
      "text": "What industry would I pick outside of something which has a physical interface?"
    },
    {
      "id": 786,
      "start": 3176.54,
      "end": 3176.74,
      "text": "Yeah."
    },
    {
      "id": 787,
      "start": 3177.14,
      "end": 3178.98,
      "text": "Again, anything where you're building on AI."
    },
    {
      "id": 788,
      "start": 3179.18,
      "end": 3187.48,
      "text": "Like, if AI is the tailwind, you know, if you can be part of some other part of the supply chain, you know, something in the semiconductor space, which, you know, I think is,"
    },
    {
      "id": 789,
      "start": 3188.0,
      "end": 3190.48,
      "text": "you know, that's one example."
    },
    {
      "id": 790,
      "start": 3190.68,
      "end": 3196.92,
      "text": "You know, it has an element of kind of, you know, physical world and more traditional engineering, not software engineering."
    },
    {
      "id": 791,
      "start": 3196.92,
      "end": 3200.96,
      "text": "You know, again, the very kind of human-centered professions."
    },
    {
      "id": 792,
      "start": 3200.96,
      "end": 3204.56,
      "text": "Like, you know, that is something I would think in terms of."
    },
    {
      "id": 793,
      "start": 3204.56,
      "end": 3217.2,
      "text": "And I think the other thing I always say is, like, in the world in which, you know, AI can kind of generate anything and, you know, create anything, having basic critical thinking skills may be the most important thing to success."
    },
    {
      "id": 794,
      "start": 3217.2,
      "end": 3221.58,
      "text": "I worry about, you know, these AI models that generate images and videos."
    },
    {
      "id": 795,
      "start": 3221.58,
      "end": 3225.8799999999997,
      "text": "And we don't make, you know, models that generate images and videos for many reasons."
    },
    {
      "id": 796,
      "start": 3225.8799999999997,
      "end": 3227.18,
      "text": "But, you know, this is one of them."
    },
    {
      "id": 797,
      "start": 3228.72,
      "end": 3231.18,
      "text": "It's really hard to tell what's real from what's not."
    },
    {
      "id": 798,
      "start": 3231.18,
      "end": 3245.46,
      "text": "And so, you know, a significant part of success may be having the street smarts, you know, not to get fooled by, you know, I mean, hopefully we can crack down on and regulate some of this fake content."
    },
    {
      "id": 799,
      "start": 3245.46,
      "end": 3250.8,
      "text": "But, you know, assume we can't, you know, critical thinking skills are going to be really important."
    },
    {
      "id": 800,
      "start": 3251.02,
      "end": 3254.26,
      "text": "And, you know, you don't want to fall for things that are fake."
    },
    {
      "id": 801,
      "start": 3254.36,
      "end": 3255.96,
      "text": "You don't want to have false beliefs."
    },
    {
      "id": 802,
      "start": 3256.04,
      "end": 3257.08,
      "text": "You don't want to get scammed."
    },
    {
      "id": 803,
      "start": 3257.38,
      "end": 3260.38,
      "text": "Like, you know, that's really advice that I would give to someone."
    },
    {
      "id": 804,
      "start": 3261.2,
      "end": 3268.7400000000002,
      "text": "If every innovation in the history of humanity killed a core human skills, I'll give you an example."
    },
    {
      "id": 805,
      "start": 3268.74,
      "end": 3280.58,
      "text": "If calculators killed our ability to do arithmetic, if writing reduced the memory of human beings per se, what muscle is AI killing?"
    },
    {
      "id": 806,
      "start": 3281.7799999999997,
      "end": 3285.3399999999997,
      "text": "So, you know, first of all, I'm not so sure."
    },
    {
      "id": 807,
      "start": 3285.64,
      "end": 3288.8199999999997,
      "text": "Like, you know, I still do math in my head quite a lot."
    },
    {
      "id": 808,
      "start": 3288.8999999999996,
      "end": 3298.66,
      "text": "I still find it useful to do math in my head, you know, even without a calculator just because it's like, you know, it's more integrated into my thought processes, right?"
    },
    {
      "id": 809,
      "start": 3298.66,
      "end": 3310.8999999999996,
      "text": "You know, you know, I might want to say, oh, yeah, you know, if like each user paid this amount, then, you know, then the revenue would be that, you know, I want to be able to close that loop in my head without having to, you know, without having to give the answer to a calculator."
    },
    {
      "id": 810,
      "start": 3310.8999999999996,
      "end": 3314.2999999999997,
      "text": "So I think a lot of these skills are still pretty relevant."
    },
    {
      "id": 811,
      "start": 3315.52,
      "end": 3322.2599999999998,
      "text": "But, you know, I would say that if you don't use things carefully, that you can lose, you can lose important skills."
    },
    {
      "id": 812,
      "start": 3322.26,
      "end": 3330.76,
      "text": "And, you know, I think we started to see it with, you know, students where, you know, it's like, you know, they have the AI, like, write the essay for it."
    },
    {
      "id": 813,
      "start": 3330.8,
      "end": 3332.2200000000003,
      "text": "It's basically just cheating on homework."
    },
    {
      "id": 814,
      "start": 3332.34,
      "end": 3333.98,
      "text": "So, you know, we shouldn't do that."
    },
    {
      "id": 815,
      "start": 3334.26,
      "end": 3343.3,
      "text": "You know, we did some studies around code and showed that, you know, depending on how you use the model, you know, we can see de-skilling in terms of writing code, right?"
    },
    {
      "id": 816,
      "start": 3343.3,
      "end": 3347.38,
      "text": "There are different ways to use the model and some of them don't cause de-skilling and some of them do."
    },
    {
      "id": 817,
      "start": 3347.54,
      "end": 3353.6600000000003,
      "text": "But, you know, definitely if folks are not thoughtful in how they use things, then de-skilling absolutely can happen."
    },
    {
      "id": 818,
      "start": 3355.2400000000002,
      "end": 3359.1000000000004,
      "text": "Do you think humans will become stupider as a race in the next decade?"
    },
    {
      "id": 819,
      "start": 3359.3,
      "end": 3366.44,
      "text": "Because we are, in a way, exporting thinking and cognition to systems?"
    },
    {
      "id": 820,
      "start": 3366.44,
      "end": 3373.14,
      "text": "Yeah, I think if we deploy, again, it's the machines of loving grace and adolescence of technology."
    },
    {
      "id": 821,
      "start": 3373.26,
      "end": 3380.16,
      "text": "I think if we deploy AI in the wrong way, if we deploy it carelessly, then, yes, people could become stupider."
    },
    {
      "id": 822,
      "start": 3380.44,
      "end": 3385.14,
      "text": "Even if an AI is always going to be better than you at some thing, you can still learn that thing, right?"
    },
    {
      "id": 823,
      "start": 3385.18,
      "end": 3387.52,
      "text": "You can still enrich yourself intellectually."
    },
    {
      "id": 824,
      "start": 3387.52,
      "end": 3393.66,
      "text": "And so that's a choice we have to make as individual companies, as individual people, and as a society overall."
    },
    {
      "id": 825,
      "start": 3393.66,
      "end": 3397.98,
      "text": "Dario, do you have a view on open-sourced versus closed?"
    },
    {
      "id": 826,
      "start": 3398.62,
      "end": 3403.2599999999998,
      "text": "I was looking at some companies like ZAIs, GLM-5, or DeepSeek."
    },
    {
      "id": 827,
      "start": 3404.8199999999997,
      "end": 3412.2799999999997,
      "text": "If you spend all this money on IP creation, on research,"
    },
    {
      "id": 828,
      "start": 3412.58,
      "end": 3421.8399999999997,
      "text": "if these guys are able to reverse prompt and engineer and get close to anthropic-level answers,"
    },
    {
      "id": 829,
      "start": 3421.84,
      "end": 3427.52,
      "text": "I'm not saying 100%, but I was seeing the GLM-5 numbers and they seemed quite good."
    },
    {
      "id": 830,
      "start": 3429.6400000000003,
      "end": 3434.5,
      "text": "Where does the IP value in the world of AI lie?"
    },
    {
      "id": 831,
      "start": 3435.06,
      "end": 3440.44,
      "text": "And if I were to be building an application, can I make the assumption, it's a far-fetched extrapolation,"
    },
    {
      "id": 832,
      "start": 3440.44,
      "end": 3448.56,
      "text": "but can I assume that eventually the AI model layers will get so democratized that I should pick open-sourced"
    },
    {
      "id": 833,
      "start": 3448.56,
      "end": 3457.28,
      "text": "every time when I'm building an agent or an application layer because that helps me retain the revenue model that I might be working with?"
    },
    {
      "id": 834,
      "start": 3457.28,
      "end": 3459.32,
      "text": "So there are a few things here."
    },
    {
      "id": 835,
      "start": 3459.9,
      "end": 3465.0600000000004,
      "text": "One is, you know, a lot of these models, particularly the ones that come from China,"
    },
    {
      "id": 836,
      "start": 3465.7200000000003,
      "end": 3473.34,
      "text": "are optimized for benchmarks and are distilled from, you know, from kind of the big U.S. labs."
    },
    {
      "id": 837,
      "start": 3473.34,
      "end": 3481.34,
      "text": "So, you know, there was a test recently where, you know, some of these models scored very highly on the usual SWE benchmarks,"
    },
    {
      "id": 838,
      "start": 3482.04,
      "end": 3483.54,
      "text": "the usual software engineering benchmarks."
    },
    {
      "id": 839,
      "start": 3483.9,
      "end": 3488.7400000000002,
      "text": "But then when someone made a held-back benchmark like that, you know, had not been publicly measured,"
    },
    {
      "id": 840,
      "start": 3488.82,
      "end": 3490.5,
      "text": "the models did a lot worse on that."
    },
    {
      "id": 841,
      "start": 3491.34,
      "end": 3501.52,
      "text": "And so, you know, I think those models are optimized for benchmarks much more than, you know, for kind of real-world use."
    },
    {
      "id": 842,
      "start": 3501.52,
      "end": 3507.54,
      "text": "But I think there's a broader point than that, which is that I think that how things are being set up,"
    },
    {
      "id": 843,
      "start": 3507.64,
      "end": 3512.72,
      "text": "the economics of the models are very different than any previous technology."
    },
    {
      "id": 844,
      "start": 3513.12,
      "end": 3516.7,
      "text": "What we find is that there is a very strong preference for quality."
    },
    {
      "id": 845,
      "start": 3517.08,
      "end": 3519.72,
      "text": "It's a bit like human employees, right?"
    },
    {
      "id": 846,
      "start": 3519.72,
      "end": 3522.74,
      "text": "So, you know, it's like if, you know, if I said to you,"
    },
    {
      "id": 847,
      "start": 3522.98,
      "end": 3527.06,
      "text": "you can hire the best programmer in the world or the 10,000th best programmer in the world."
    },
    {
      "id": 848,
      "start": 3527.16,
      "end": 3528.94,
      "text": "I mean, they're both very skilled."
    },
    {
      "id": 849,
      "start": 3528.94,
      "end": 3534.16,
      "text": "But, like, I think anyone who's hired a large number of people has this intuition that, like,"
    },
    {
      "id": 850,
      "start": 3534.2000000000003,
      "end": 3538.3,
      "text": "there's this, like, power law, long tail distribution of ability."
    },
    {
      "id": 851,
      "start": 3538.36,
      "end": 3540.02,
      "text": "And we find the same thing in the models."
    },
    {
      "id": 852,
      "start": 3540.2400000000002,
      "end": 3547.52,
      "text": "Like, within a range, price doesn't matter that much if a model is the best model,"
    },
    {
      "id": 853,
      "start": 3547.6,
      "end": 3549.18,
      "text": "the most cognitively capable model."
    },
    {
      "id": 854,
      "start": 3550.7200000000003,
      "end": 3552.36,
      "text": "Price doesn't matter much."
    },
    {
      "id": 855,
      "start": 3552.56,
      "end": 3555.14,
      "text": "The form in which it's presented doesn't matter much."
    },
    {
      "id": 856,
      "start": 3555.14,
      "end": 3561.42,
      "text": "So I'm focused almost entirely just on having the smartest model and the best model for the task."
    },
    {
      "id": 857,
      "start": 3562.24,
      "end": 3564.2799999999997,
      "text": "My view is that's the only thing that matters."
    },
    {
      "id": 858,
      "start": 3564.6,
      "end": 3567.62,
      "text": "Long-term, geopolitics."
    },
    {
      "id": 859,
      "start": 3568.06,
      "end": 3574.54,
      "text": "If Anthropic were a restaurant, I would say the raw ingredients, the vegetables,"
    },
    {
      "id": 860,
      "start": 3575.08,
      "end": 3577.58,
      "text": "in this particular case, is data."
    },
    {
      "id": 861,
      "start": 3577.58,
      "end": 3581.9,
      "text": "Do you think the long-term, this is also pertinent to me, the question,"
    },
    {
      "id": 862,
      "start": 3581.98,
      "end": 3585.16,
      "text": "because we are investing in a data center business, which is Indian in nature."
    },
    {
      "id": 863,
      "start": 3586.16,
      "end": 3591.2,
      "text": "Do you think long-term the world moves to a place where every country owns its data"
    },
    {
      "id": 864,
      "start": 3591.2,
      "end": 3594.6,
      "text": "and you have to start paying more for the vegetables you used to cook?"
    },
    {
      "id": 865,
      "start": 3594.6,
      "end": 3595.04,
      "text": "Yeah."
    },
    {
      "id": 866,
      "start": 3595.04,
      "end": 3595.3399999999997,
      "text": "Yeah."
    },
    {
      "id": 867,
      "start": 3595.6,
      "end": 3597.6,
      "text": "So, I mean, I think there are a few things."
    },
    {
      "id": 868,
      "start": 3597.74,
      "end": 3600.94,
      "text": "You know, I do think there will be demand to build data centers around the world,"
    },
    {
      "id": 869,
      "start": 3601.02,
      "end": 3602.7,
      "text": "and we're, like, very supportive of that."
    },
    {
      "id": 870,
      "start": 3604.62,
      "end": 3609.94,
      "text": "I, you know, it's – data is getting kind of interesting because, you know,"
    },
    {
      "id": 871,
      "start": 3610.02,
      "end": 3615.5,
      "text": "a lot of the data that we use today is RL environments that we train on, right?"
    },
    {
      "id": 872,
      "start": 3615.5,
      "end": 3620.88,
      "text": "So, for example, when you train on math or agentic coding environments,"
    },
    {
      "id": 873,
      "start": 3621.32,
      "end": 3623.54,
      "text": "you're not really getting data."
    },
    {
      "id": 874,
      "start": 3623.64,
      "end": 3627.68,
      "text": "Like, you're getting some math problems in the model, like, experiments with trying the math problems."
    },
    {
      "id": 875,
      "start": 3627.7,
      "end": 3628.32,
      "text": "It's more synthetic."
    },
    {
      "id": 876,
      "start": 3628.46,
      "end": 3629.26,
      "text": "You're creating the data."
    },
    {
      "id": 877,
      "start": 3629.38,
      "end": 3629.68,
      "text": "Yeah."
    },
    {
      "id": 878,
      "start": 3629.74,
      "end": 3633.94,
      "text": "You can think of it as synthetic data, or you can think of it as trial and error and environment."
    },
    {
      "id": 879,
      "start": 3634.14,
      "end": 3638.26,
      "text": "So, I think data is becoming – static data is becoming less important,"
    },
    {
      "id": 880,
      "start": 3638.3,
      "end": 3642.06,
      "text": "and what we might call, like, dynamic data that the model creates itself is,"
    },
    {
      "id": 881,
      "start": 3642.06,
      "end": 3645.68,
      "text": "you know, for reinforcement learning is becoming more important."
    },
    {
      "id": 882,
      "start": 3645.84,
      "end": 3651.22,
      "text": "So, you know, I don't think data is quite the most central thing anymore,"
    },
    {
      "id": 883,
      "start": 3651.22,
      "end": 3652.64,
      "text": "but it still matters."
    },
    {
      "id": 884,
      "start": 3652.7799999999997,
      "end": 3656.2799999999997,
      "text": "And, you know, I think to the extent that that is the case, you know,"
    },
    {
      "id": 885,
      "start": 3656.2799999999997,
      "end": 3660.38,
      "text": "a lot of the data is just available, just kind of available on the open web,"
    },
    {
      "id": 886,
      "start": 3660.7,
      "end": 3663.22,
      "text": "although if you're trying to get data in certain languages,"
    },
    {
      "id": 887,
      "start": 3663.4,
      "end": 3666.22,
      "text": "optimized for certain languages, that can be important."
    },
    {
      "id": 888,
      "start": 3666.22,
      "end": 3672.7999999999997,
      "text": "You know, I do think if data means, like, the data given to you by customers,"
    },
    {
      "id": 889,
      "start": 3672.7999999999997,
      "end": 3677.62,
      "text": "like that, you know, you process the data for some other company,"
    },
    {
      "id": 890,
      "start": 3678.04,
      "end": 3681.8399999999997,
      "text": "then countries will, and in the case of Europe already have,"
    },
    {
      "id": 891,
      "start": 3682.18,
      "end": 3688.58,
      "text": "pass laws that say that that kind of customer, you know, personal proprietary data"
    },
    {
      "id": 892,
      "start": 3688.58,
      "end": 3691.48,
      "text": "needs to stay within the boundaries of the country."
    },
    {
      "id": 893,
      "start": 3691.48,
      "end": 3696.78,
      "text": "And that's one reason to kind of, you know, to build, you know, to operate data centers"
    },
    {
      "id": 894,
      "start": 3696.78,
      "end": 3701.46,
      "text": "around the world at different countries and, you know, to kind of, you know,"
    },
    {
      "id": 895,
      "start": 3701.46,
      "end": 3706.22,
      "text": "keep the models performing of the inference in those countries."
    },
    {
      "id": 896,
      "start": 3707.06,
      "end": 3710.28,
      "text": "I really pushed Elon on this particular question."
    },
    {
      "id": 897,
      "start": 3710.42,
      "end": 3712.08,
      "text": "He was skeptical of answering it."
    },
    {
      "id": 898,
      "start": 3712.54,
      "end": 3716.9,
      "text": "But I asked him to pick one stock he would put money in, which is not his own,"
    },
    {
      "id": 899,
      "start": 3716.96,
      "end": 3719.48,
      "text": "and he said, Google, I'm going to ask you the question,"
    },
    {
      "id": 900,
      "start": 3719.48,
      "end": 3721.64,
      "text": "and I know you're going to be skeptical in it as well."
    },
    {
      "id": 901,
      "start": 3722.2,
      "end": 3728.84,
      "text": "If Dario had $100 today, and you had to make the binary decision of investing in a stock"
    },
    {
      "id": 902,
      "start": 3728.84,
      "end": 3731.7,
      "text": "to win in capitalism, which stock would you pick?"
    },
    {
      "id": 903,
      "start": 3731.9,
      "end": 3736.78,
      "text": "Yeah, I had better not answer that question, because I know so much about so many public companies."
    },
    {
      "id": 904,
      "start": 3736.92,
      "end": 3740.54,
      "text": "Like, I think I better not answer that question."
    },
    {
      "id": 905,
      "start": 3740.84,
      "end": 3743.86,
      "text": "Maybe answer the question for an industry that you're not involved in,"
    },
    {
      "id": 906,
      "start": 3744.44,
      "end": 3748.46,
      "text": "which I'm guessing today is seldom the case, because you're involved in most industries."
    },
    {
      "id": 907,
      "start": 3748.46,
      "end": 3752.62,
      "text": "Yeah, no, it's really, I mean, I don't know."
    },
    {
      "id": 908,
      "start": 3753.26,
      "end": 3760.48,
      "text": "I'm positive on, like, I think biotech is about to have a renaissance."
    },
    {
      "id": 909,
      "start": 3760.7,
      "end": 3762.7200000000003,
      "text": "Like, ultimately, we'll be driven by AI."
    },
    {
      "id": 910,
      "start": 3763.48,
      "end": 3767.66,
      "text": "You know, I'm not going to name a particular company, but, like, you know,"
    },
    {
      "id": 911,
      "start": 3767.7400000000002,
      "end": 3771.82,
      "text": "nor will I say whether I think it's better to bet on the big pharma companies"
    },
    {
      "id": 912,
      "start": 3771.82,
      "end": 3774.5800000000004,
      "text": "or, like, you know, emerging smaller biotechs."
    },
    {
      "id": 913,
      "start": 3776.1000000000004,
      "end": 3779.92,
      "text": "But, like, my instinct is we're about to cure a lot of diseases."
    },
    {
      "id": 914,
      "start": 3780.26,
      "end": 3780.44,
      "text": "And so—"
    },
    {
      "id": 915,
      "start": 3780.44,
      "end": 3783.44,
      "text": "Can you give me a subset of biotech that I should focus on?"
    },
    {
      "id": 916,
      "start": 3783.6800000000003,
      "end": 3784.04,
      "text": "Yeah."
    },
    {
      "id": 917,
      "start": 3784.04,
      "end": 3791.42,
      "text": "I think this idea of stuff that's more programmable and adaptive, you know, from the mRNA vaccines,"
    },
    {
      "id": 918,
      "start": 3791.48,
      "end": 3796.2599999999998,
      "text": "although those are having trouble in the U.S. for dumb reasons, but, you know, I'm very optimistic"
    },
    {
      "id": 919,
      "start": 3796.2599999999998,
      "end": 3801.7,
      "text": "about the technology, to kind of the peptide-based therapies, right, where, you know—you know,"
    },
    {
      "id": 920,
      "start": 3801.72,
      "end": 3806.38,
      "text": "again, if you have a small molecule drug, you're like, there's only so many degrees of freedom you have,"
    },
    {
      "id": 921,
      "start": 3806.4,
      "end": 3809.66,
      "text": "and, you know, you kind of make one thing better, the other thing gets worse."
    },
    {
      "id": 922,
      "start": 3809.66,
      "end": 3814.7999999999997,
      "text": "Like, peptides, it has this almost digital property where you can say, oh, I'm going to substitute in,"
    },
    {
      "id": 923,
      "start": 3814.8599999999997,
      "end": 3817.5,
      "text": "you know, this amino acid here and this amino acid there."
    },
    {
      "id": 924,
      "start": 3817.7999999999997,
      "end": 3821.46,
      "text": "And so it allows for more continuous optimization."
    },
    {
      "id": 925,
      "start": 3821.92,
      "end": 3829.3199999999997,
      "text": "So, you know, I think those kinds of areas, you know, I would be optimistic about."
    },
    {
      "id": 926,
      "start": 3829.44,
      "end": 3833.3599999999997,
      "text": "Maybe also cell-based therapies, which is, like, a new—"
    },
    {
      "id": 927,
      "start": 3833.3599999999997,
      "end": 3833.7999999999997,
      "text": "Stem cell?"
    },
    {
      "id": 928,
      "start": 3834.2999999999997,
      "end": 3834.8199999999997,
      "text": "No, no, no."
    },
    {
      "id": 929,
      "start": 3834.82,
      "end": 3841.76,
      "text": "So things like, you know, like, I don't know, like the CAR-T therapy where, you know, you kind of genetically engineer your,"
    },
    {
      "id": 930,
      "start": 3841.76,
      "end": 3849.5,
      "text": "like, you know, basically take some, you know, cells out of your body, genetically engineer them to, you know,"
    },
    {
      "id": 931,
      "start": 3849.5800000000004,
      "end": 3852.36,
      "text": "to attack a particular cancer and put them back in the body."
    },
    {
      "id": 932,
      "start": 3852.7200000000003,
      "end": 3854.1400000000003,
      "text": "Do stem cell therapies work?"
    },
    {
      "id": 933,
      "start": 3854.2000000000003,
      "end": 3856.1200000000003,
      "text": "I spent the whole of last week doing this."
    },
    {
      "id": 934,
      "start": 3856.1800000000003,
      "end": 3862.32,
      "text": "I was at a hospital for three hours a day getting nebulizer and stem cells into my veins."
    },
    {
      "id": 935,
      "start": 3862.32,
      "end": 3867.2200000000003,
      "text": "I am not up on the latest of stem cell therapies."
    },
    {
      "id": 936,
      "start": 3867.34,
      "end": 3869.76,
      "text": "You'd have to ask a currently practicing biologist."
    },
    {
      "id": 937,
      "start": 3869.86,
      "end": 3871.5800000000004,
      "text": "But peptides, I think, will blow up, right?"
    },
    {
      "id": 938,
      "start": 3872.06,
      "end": 3875.32,
      "text": "I mean, you know, again, the design space is very broad."
    },
    {
      "id": 939,
      "start": 3875.7000000000003,
      "end": 3875.92,
      "text": "Right."
    },
    {
      "id": 940,
      "start": 3876.76,
      "end": 3883.6800000000003,
      "text": "When I tried to use Claude Code for the first time, I did struggle to get it to work."
    },
    {
      "id": 941,
      "start": 3883.68,
      "end": 3892.52,
      "text": "It was, for somebody who's very stupid and has no coding or programming knowledge, it's not, it's not very, very easy."
    },
    {
      "id": 942,
      "start": 3892.62,
      "end": 3893.7599999999998,
      "text": "I think there's a learning curve."
    },
    {
      "id": 943,
      "start": 3894.3599999999997,
      "end": 3899.58,
      "text": "I heard someone say it well, it's like, even prompt engineering is like playing a piano."
    },
    {
      "id": 944,
      "start": 3899.72,
      "end": 3901.3199999999997,
      "text": "You can't sit and start playing it."
    },
    {
      "id": 945,
      "start": 3901.32,
      "end": 3915.5,
      "text": "To my audience, I think it becomes increasingly relevant where to learn how to set context, how to prompt, how to use Claude Code better for somebody like me who comes with zero knowledge."
    },
    {
      "id": 946,
      "start": 3916.1600000000003,
      "end": 3917.88,
      "text": "Can you recommend how one does that?"
    },
    {
      "id": 947,
      "start": 3918.06,
      "end": 3918.3,
      "text": "Yeah."
    },
    {
      "id": 948,
      "start": 3918.3,
      "end": 3924.5,
      "text": "I mean, first of all, I would say, you know, we're trying, we're trying increasingly to kind of like make that learning curve easier."
    },
    {
      "id": 949,
      "start": 3924.5,
      "end": 3937.3,
      "text": "So like one of the things that caused us to release Claude Code work, which is basically Claude Code for non-coders is, you know, oh man, you know, like we were noticing a bunch of non-technical people who really wanted to use Claude Code."
    },
    {
      "id": 950,
      "start": 3937.3,
      "end": 3948.88,
      "text": "And we're struggling through the command line terminal to do that, which, you know, it's like, like coders use the command line terminal all the time, but like non-coders, you know, it's just kind of like makes things unnecessarily complicated."
    },
    {
      "id": 951,
      "start": 3949.7400000000002,
      "end": 3958.8,
      "text": "So, you know, co-work was designed to be more of a, you know, the, you know, the, the kind of, you know, it was powered by, by the Claude Code engine on the back."
    },
    {
      "id": 952,
      "start": 3958.96,
      "end": 3965.6800000000003,
      "text": "But, you know, the idea was to kind of make it, you know, more, more like user friendly and like easier to use."
    },
    {
      "id": 953,
      "start": 3965.68,
      "end": 3971.0,
      "text": "So, you know, we're, we're definitely trying to introduce interfaces that kind of make it, make it easier."
    },
    {
      "id": 954,
      "start": 3971.22,
      "end": 3979.3999999999996,
      "text": "But I, you know, I would also say, you know, that there's, you know, there, there's like, you know, classes you can take that, you know, help you learn this thing."
    },
    {
      "id": 955,
      "start": 3979.46,
      "end": 3981.04,
      "text": "Now, I think it's a very empirical science."
    },
    {
      "id": 956,
      "start": 3981.04,
      "end": 3987.8199999999997,
      "text": "You mostly learn by doing, but, you know, it's like Anthropic has, it's like, you know, part of the company that we call the Ministry of Education."
    },
    {
      "id": 957,
      "start": 3987.8199999999997,
      "end": 3994.18,
      "text": "And, you know, I think increasingly, you know, we'll put out videos on how to run effective agents and how to prompt models."
    },
    {
      "id": 958,
      "start": 3994.18,
      "end": 3995.44,
      "text": "You know, we've already done some of that."
    },
    {
      "id": 959,
      "start": 3995.48,
      "end": 3999.96,
      "text": "And I think we're going to, we're going to ramp it up because, you know, we do want everyone to be able to learn this."
    },
    {
      "id": 960,
      "start": 4000.6,
      "end": 4005.58,
      "text": "Any fleeting thought, last question, like you want to leave us with something that we should bear in mind."
    },
    {
      "id": 961,
      "start": 4005.94,
      "end": 4010.8599999999997,
      "text": "What does Dario know that Nikhil and all of Nikhil's people do not?"
    },
    {
      "id": 962,
      "start": 4012.16,
      "end": 4012.52,
      "text": "Yeah."
    },
    {
      "id": 963,
      "start": 4012.64,
      "end": 4019.3199999999997,
      "text": "I mean, I don't know that I know that many things, you know, particularly now that the, you know, the implications of the technology are kind of out there."
    },
    {
      "id": 964,
      "start": 4019.32,
      "end": 4031.1000000000004,
      "text": "So, I mean, you know, it can all be, I think most aspects of my worldview can be derived from what, from what's publicly visible now, from, from what we can see, you know, kind of, kind of outside in the world."
    },
    {
      "id": 965,
      "start": 4031.1,
      "end": 4042.88,
      "text": "But the thing I would say, and it's an experience I've had over and over again over the last 10 years, is, you know, there's this temptation to believe, oh, you know, that can't happen."
    },
    {
      "id": 966,
      "start": 4043.0,
      "end": 4043.96,
      "text": "It would be too weird."
    },
    {
      "id": 967,
      "start": 4044.12,
      "end": 4045.66,
      "text": "It would be too big a change."
    },
    {
      "id": 968,
      "start": 4045.98,
      "end": 4048.18,
      "text": "Like, you know, I'm sure people are on that."
    },
    {
      "id": 969,
      "start": 4048.3199999999997,
      "end": 4050.56,
      "text": "Like, it would be too crazy if that occurred."
    },
    {
      "id": 970,
      "start": 4051.0,
      "end": 4052.7,
      "text": "No one seems to think that'll happen."
    },
    {
      "id": 971,
      "start": 4052.7,
      "end": 4065.04,
      "text": "And, you know, over and over again, just extrapolating the simple curve or trying to reason out what will happen, like, leads you to these counterintuitive conclusions that almost no one believes."
    },
    {
      "id": 972,
      "start": 4065.7599999999998,
      "end": 4073.74,
      "text": "And, you know, it's almost like you can predict the future for free just by, you know, just by saying, well, it stands to reason that."
    },
    {
      "id": 973,
      "start": 4073.9199999999996,
      "end": 4076.12,
      "text": "And, you know, you need some empirical knowledge."
    },
    {
      "id": 974,
      "start": 4076.24,
      "end": 4077.06,
      "text": "You need some intuition."
    },
    {
      "id": 975,
      "start": 4077.2,
      "end": 4079.66,
      "text": "You can't reason from pure logic."
    },
    {
      "id": 976,
      "start": 4079.66,
      "end": 4082.6,
      "text": "I think that's another type of mistake that I see people make."
    },
    {
      "id": 977,
      "start": 4082.8599999999997,
      "end": 4096.66,
      "text": "But the right combination of a few empirical observations with, you know, just thinking from first principles can allow you to predict the future in ways that, you know, are publicly available."
    },
    {
      "id": 978,
      "start": 4096.86,
      "end": 4099.8,
      "text": "Anyone should be able to do, but that happens surprisingly rarely."
    },
    {
      "id": 979,
      "start": 4101.18,
      "end": 4104.62,
      "text": "Thank you, Dario, for doing this and hope to see you again soon."
    },
    {
      "id": 980,
      "start": 4104.82,
      "end": 4105.2,
      "text": "Thank you."
    },
    {
      "id": 981,
      "start": 4105.4,
      "end": 4105.9,
      "text": "Thank you."
    },
    {
      "id": 982,
      "start": 4106.5,
      "end": 4106.96,
      "text": "All right."
    },
    {
      "id": 983,
      "start": 4107.48,
      "end": 4107.7,
      "text": "Yeah."
    },
    {
      "id": 984,
      "start": 4108.3,
      "end": 4108.74,
      "text": "Good."
    },
    {
      "id": 985,
      "start": 4108.74,
      "end": 4108.78,
      "text": "Good."
    },
    {
      "id": 986,
      "start": 4108.98,
      "end": 4109.48,
      "text": "Was it okay?"
    },
    {
      "id": 987,
      "start": 4109.66,
      "end": 4109.9,
      "text": "Yeah."
    },
    {
      "id": 988,
      "start": 4110.08,
      "end": 4110.54,
      "text": "Seemed great."
    },
    {
      "id": 989,
      "start": 4110.54,
      "end": 4110.8,
      "text": "Yeah."
    },
    {
      "id": 990,
      "start": 4110.8,
      "end": 4110.8,
      "text": ""
    },
    {
      "id": 991,
      "start": 4110.8,
      "end": 4110.58,
      "text": "Seemed great."
    }
  ]
}