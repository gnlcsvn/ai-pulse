{
  "metadata": {
    "video_id": "n1E9IZfvGMA",
    "url": "https://www.youtube.com/watch?v=n1E9IZfvGMA",
    "title": "Dario Amodei — 'We are near the end of the exponential'",
    "channel": "Dwarkesh Patel",
    "upload_date": "2026-02-13",
    "duration": "142m",
    "transcribed_date": "2026-02-24",
    "whisper_model": "mlx-community/whisper-large-v3-turbo"
  },
  "language": "en",
  "text": "So we talked three years ago. I'm curious, in your view, what has been the biggest update of the last three years? What has been the biggest difference between what it felt like last three years versus now? Yeah, I would say actually the underlying technology, like the exponential of the technology, has gone, broadly speaking, I would say, about as I expected it to go. I mean, there's like plus or minus, you know, a couple. There's plus or minus a year or two here. There's plus or minus a year or two there. I don't know that I would have predicted the specific direction of code. But actually, when I look at the exponential, it is roughly what I expected in terms of the march of the models from like, you know, smart high school student to smart college student to like, you know, beginning to do Ph.D. and professional stuff. And in the case of code, reaching beyond that. So, you know, the frontier is a little bit uneven. It's roughly what I expected. I will tell you, though, what the most surprising thing has been. The most surprising thing has been the lack of public recognition of how close we are to the end of the exponential. To me, it is absolutely wild that, you know, you have people, you know, within the bubble and outside the bubble, you know, but you have people talking about these, you know, just the same tired old hot button political issues. And like, you know, around us, we're like near the end of the exponential. I want to understand what that exponential looks like right now, because the first question I asked you when we recorded three years ago was, you know, what's up with scaling? How does it work? I have a similar question now, but I feel like it's a more complicated question, because at least from the public's point of view. Yes. Three years ago, there were these, you know, well-known public trends where across many orders of magnitude of compute, you could see how the loss improves. And now we have RL scaling and there's no publicly known scaling law for it. It's not even clear what exactly the story is of is this supposed to be teaching the model skills? Is this supposed to be teaching meta learning? What is the scaling hypothesis at this point? Yeah. So I have actually the same hypothesis that I had even all the way back in 2017. So in 2017, I think I talked about it last time, but I wrote a doc called the Big Blob of Compute Hypothesis. And, you know, it wasn't about the scaling of language models in particular. When I wrote it, GPT-1 had just come out, right? So that was, you know, one among many things, right? There was back in those days, there was robotics. People tried to work on reasoning as a separate thing from language models. There was scaling of the kind of RL that happened, you know, kind of happened in AlphaGo. And, you know, that happened at Dota at OpenAI. And, you know, people remember StarCraft at DeepMind, you know, the AlphaStar. So it was written as a more general document. And the specific thing I said was the following that. And, you know, it's very, you know, Rich Sutton put out the bitter lesson a couple of years later. But, you know, the hypothesis is basically the same. So what it says is all the cleverness, all the techniques, all the kind of we need a new method to do something like that doesn't matter very much. There are only a few things that matter. And I think I listed seven of them. One is like how much raw compute you have. The other is the quantity of data that you have. Then the third is kind of the quality and distribution of data, right? It needs to be a broad, broad distribution of data. The fourth is, I think, how long you train for. The fifth is you need an objective function that can scale to the moon. So the pre-training objective function is one such objective function, right? Another objective function is, you know, the kind of RL objective function that says, like, you have a goal. You're going to go out and reach the goal. Within that, of course, there's objective rewards like, you know, like you see in math and coding. And there's more subjective rewards like you see in RL from human feedback or kind of higher order versions of that. And then the sixth and seventh were things around kind of like normalization or conditioning, like, you know, just getting the numerical stability so that kind of the big blob of compute flows in this laminar way instead of running into problems. So that was the hypothesis. And it's a hypothesis I still hold. I don't think I've seen very much that is not in line with that hypothesis. And so the pre-training scaling laws were one example of kind of what we see there. And indeed, those have continued going. Like, you know, you know, I think now it's been widely reported. Like, you know, we feel good about pre-training. Like, pre-training is continuing to give us gains. What has changed is that now we're also seeing the same thing for RL, right? So we're seeing a pre-training phase and then we're seeing like an RL phase on top of that. And with RL, it's actually just the same. Like, you know, even other companies have published like, you know, in some of their releases have published things that say, look, you know, we train the model on math contests, you know, AIME or the kind of other things. And, you know, how well the model does is log linear and how long we've trained it. And we see that as well. And it's not just math contests. It's a wide variety of RL tasks. And so we're seeing the same scaling in RL that we saw for pre-training. You mentioned Richard Sutton and the Bitter Lesson. Yeah. I interviewed him last year. And he is actually very non-LLM pilled. And if I'm, I don't know if this is his perspective, but one way to paraphrase this objection is something like, look, something which possesses the true core of human learning would not require all these billions of dollars of data and compute and these bespoke environments to learn how to use Excel or how to use PowerPoint, how to navigate a web browser. And the fact that we have to build in these skills using these RL environments hints that we're actually lacking this core human learning algorithm. And so we're scaling the wrong thing. And so, yeah, that does raise the question. Why are we doing all this RL scaling if we do think there's something that's going to be human-like in its ability to learn on the fly? Yeah, yeah. So I think this kind of puts together several things that should be kind of thought of differently. Yeah. I think there is a genuine puzzle here, but it may not matter. In fact, I would guess it probably doesn't matter. So let's take the RL out of it for a second because I actually think RL and it's a red herring to say that RL is any different from pre-training in this matter. So if we look at pre-training scaling, it was very interesting. Back in, you know, 2017 when Alec Radford was doing GPT-1. If you look at the models before GPT-1, they were trained on these data sets that didn't represent a wide, you know, distribution of text, right? You had like, you know, these very standard, you know, kind of language modeling benchmarks. And GPT-1 itself was trained on a bunch of – I think it was fan fiction actually. But, you know, it was like literary – you know, it was like literary text, which is a very small fraction of the text that you get. And what we found with that, you know, and in those days it was like a billion words or something. So small data sets and represented a pretty narrow distribution, right? Like a narrow distribution of kind of what you can see in the world. And it didn't generalize well. If you did better on, you know, the – you know, I forgot what it was, some kind of fan fiction corpus, it wouldn't generalize that well to kind of the other – you know, we had all these measures of like, you know, how well does the model do at predicting all of these other kinds of texts. You really didn't see the generalization. It was only when you trained over all the tasks on the internet, when you kind of did a general internet scrape, right, from something like, you know, Common Crawl or scraping links on Reddit, which is what we did for GPT-2. It's only when you do that that you kind of started to get generalization. And I think we're seeing the same thing on RL, that we're starting with first very simple RL tasks like training on math competitions. Then we're kind of moving to, you know, kind of broader training that involves things like code as a task. And now we're moving to do kind of many, many other tasks. And then I think we're going to increasingly get generalization. So that kind of takes out the RL versus the pre-training side of it. But I think there is a puzzle here either way, which is that on pre-training, when we train the model on pre-training, you know, we use like trillions of tokens, right? And humans don't see trillions of words. So there is an actual sample efficiency difference here. There is actually something different that's happening here, which is that the models start from scratch. And, you know, they have to get much more, much more training. But we also see that once they're trained, if we give them a long context length, the only thing blocking a long context length is like inference. But if we give them like a context length of a million, they're very good at learning and adapting within that context length. And so I don't know the full answer to this, but I think there's something going on that pre-training, it's not like the process of humans learning. It's somewhere between the process of humans learning and the process of human evolution. It's like it's somewhere between like we get many of our priors from evolution. Our brain isn't just a blank slate, right? Whole books have been written about. I think the language models, they're much more blank slates. They literally start as like random weights, whereas the human brain starts with all these regions. It's connected to all these inputs and outputs. And so maybe we should think of pre-training and for that matter, RL as well as being something that exists in the middle space between human evolution and, you know, kind of human on the spot learning. And as the in-context learning that the models do as something between long-term human learning and short-term human learning. So, you know, there's this hierarchy of like there's evolution, there's long-term learning, there's short-term learning, and there's just human reaction. And the LOM phases exist along this spectrum, but not necessarily exactly at the same points. There's no analog to some of the human modes of learning that LOMs are kind of falling between the points. Does that make sense? Yes, although some things are still a bit confusing. For example, if the analogy is that this is like evolution, so it's fine that it's not that sample efficient, then like, well, if we're going to get the kind of super sample efficient Asian from in-context learning, why are we bothering to build in, you know, there's RL environment companies which are, it seems like what they're doing is they're teaching it how to use this API, how to use Slack, how to use whatever. It's confusing to me why there's so much emphasis on that if the kind of agent that can just learn on the fly is emerging or is going to soon emerge or has already emerged. Yeah, yeah. So, I mean, I can't speak for the emphasis of anyone else. I can only talk about how we think about it. I think the way we think about it is the goal is not to teach the model every possible skill within RL, just as we don't do that within pre-training, right? Within pre-training, we're not trying to expose the model to, you know, every possible, you know, way that words could be put together, right? You know, it's rather that the model trains on a lot of things and then it reaches generalization across pre-training, right? That was the transition from GPT-1 to GPT-2 that I saw up close, which is like, you know, the model reaches a point, you know? I like had these moments where I was like, oh, yeah, you just give the model like, you just give the model a list of numbers that's like, you know, you know, this is the cost of the house. This is the square feet of the house. And the model completes the pattern and does linear regression. Like, not great, but it does it. But it's never seen that exact thing before. And so, you know, to the extent that we are building these RL environments, the goal is very similar to what is, you know, to what was done five or ten years ago with pre-training. With we're trying to get a whole bunch of data, not because we want to cover a specific document or a specific skill, but because we want to generalize. I mean, I think the framework you're laying down obviously makes sense. Like, we're making progress towards AGI. I think the crux is something like nobody at this point disagrees that we're going to achieve AGI in this century. And the crux is you say we're hitting the end of the exponential and somebody else looks at this and says, oh, yeah, we're making progress. We've been making progress since 2012. And then 2035 will have a human-like agent. And so I want to understand what it is that you're seeing, which makes you think, yeah, obviously we're seeing the kinds of things that evolution did or that within human lifetime learning is like in these models. And why think that it's one year away and not ten years away? I actually think of it as like two – there's kind of two cases to be made here, all right? Two claims you could make, one of which is like stronger and the other of which is weaker. So I think starting with the weaker claim, you know, when I first saw the scaling back in like, you know, 2019, you know, I wasn't sure. You know, this was the whole – this was kind of a 50-50 thing, right? I thought I saw something that was, you know, and my claim was this is much more likely than anyone thinks it is. Like this is wild. No one else would even consider this. Maybe there's a 50% chance this happens. On the basic hypothesis of, you know, as you put it, within 10 years we'll get to, you know, what I call kind of country of geniuses in a data center. I'm at like 90% on that. And it's hard to go much higher than 90% because the world is so unpredictable. Maybe the irreducible uncertainty would be if we were at 95% where you get to things like, I don't know, maybe, you know, multiple companies have, you know, kind of internal turmoil and nothing happens. And then Taiwan gets invaded and like all the fabs get blown up by missiles and, you know, and then – Now you would drink to Staria. Yeah, yeah, yeah. You know, just you could construct a scenario where there's like a 5% chance that it – or, you know, you can construct a 5% world where like things get delayed for 10 years. That's maybe 5%. There's another 5% which is that I'm very confident on tasks that can be verified. So I think with coding, I'm just – except for that irreducible uncertainty, there's just – I mean, I think we'll be there in one or two years. There's no way we will not be there in 10 years in terms of being able to do it end-to-end coding. My one little bit, the one little bit of fundamental uncertainty even on long timescales is this thing about tasks that aren't verifiable like planning a mission to Mars, like, you know, doing some fundamental scientific discovery like CRISPR, like, you know, writing a novel. Hard to verify those tasks. I am almost certain that we have a reliable path to get there. But like if there was a little bit uncertainty, it's there. So on the 10 years, I'm like, you know, 90%, which is about as certain as you can be. Like, I think it's crazy to say that this won't happen by 2035. Like, in some sane world, it would be outside the mainstream. But the emphasis on verification hints to me as a lack of belief that these models are generalized. If you think about humans, we are good at things that both of which we get verifiable reward and things which we don't. You're like, you haven't started. No, no, no. This is why I'm almost sure. We already see substantial generalization from things that verify to things that don't verify. We're already seeing that. But it seems like you were emphasizing this as a spectrum which will split apart which domains we see more progress. And I'm like, but that doesn't seem like how humans get better. The world in which we don't make it or the world in which we don't get there is the world in which we do all the things that are verifiable. And then they like, you know, many of them generalize, but we kind of don't get fully there. We don't fully color in this side of the box. It's not a binary thing. But it also seems to me, even if in the world where generalization is weak when you only say verifiable domains, it's not clear to me in such a world you could automate software engineering. Because software, like, in some sense, you are, quote, unquote, a software engineer. Yeah. But part of being a software engineer for you involves writing these, like, long memos about your grand vision about different things. Well, I don't think that's part of the job of SWE. That's part of the job of the company. But I do think SWE involves, like, design documents and other things like that. Which, by the way, the models are not bad. They're already pretty good at writing comments. And so, again, I'm making, like, much weaker claims here than I believe to, like, you know, to kind of set up a, you know, to distinguish between two things. Like, we're already almost there for software engineering. We are already almost there. By what metric? There's one metric, which is, like, how many lines of code are written by AI. And if you use, if you consider other productivity improvements in the course of the history of software engineering, compilers write all the lines of software. But there's a difference between how many lines are written and how big the productivity improvement is. Oh, yeah. And then, like, we're almost there meaning, like, how big is the productivity improvement, not just how many lines are written. Yeah, yeah. So, I actually agree with you on this. So, I've made this series of predictions on code and software engineering. And I think people have repeatedly kind of misunderstood them. So, let me lay out the spectrum, right? Like, I think it was, you know, like, you know, eight or nine months ago or something, I said, you know, the AI model will be writing 90% of the lines of code in, like, you know, three to six months. Which happened at least at some places, right? Happened at Anthropic, happened with many people downstream using our models. But that's actually a very weak criterion, right? People thought I was saying, like, we won't need 90% of the software engineers. Those things are worlds apart, right? Like, I would put the spectrum as 90% of code is written by the model, 100% of code is written by the model, and that's a big difference in productivity. 90% of the end-to-end SWE tasks, right, including things like compiling, including things like setting up clusters and environments, testing features, writing memos, 90% of the SWE tasks are written by the models. 100% of today's SWE tasks are written by the models. And even when that happens, it doesn't mean software engineers are out of a job. Like, there's, like, new higher-level things they can do where they can manage. And then there's a further down the spectrum, like, you know, there's 90% less demand for SWE, which I think will happen. But, like, this is a spectrum. And, you know, I wrote about it in The Adolescence of Technology where I went through this kind of spectrum with farming. And so I actually totally agree with you on that. It's just these are very different benchmarks from each other, but we're proceeding through them super fast. It seems like in part of your vision, it's, like, going from 90 to 100. First, it's going to happen fast. And, two, that somehow that leads to huge productivity improvements. Whereas when I notice, even in greenfield projects that people start with Cloud Code or something, people report starting a lot of projects. And I'm, like, do we see in the world out there a renaissance of software, all these new features that wouldn't exist otherwise? And at least so far, it doesn't seem like we see that. And so that does make me wonder, even if, like, I never had to intervene on Cloud Code, there is this thing of, like, there's just the world is complicated, jobs are complicated. And closing the loop on self-contained systems, whether it's just writing software or something, how much sort of how much broader gains we would see just from that? And so maybe that makes us this should dilute our estimation of the country of geniuses. Well, I actually, I, like, simultaneously, I simultaneously agree with you, agree that it's a reason why these things don't happen instantly. But at the same time, I think the effect is going to be very fast. So, like, I don't know. You could have these two poles, right? One is, like, you know, AI is, like, you know, it's not going to make progress. It's slow. Like, it's going to take, you know, kind of forever to diffuse within the economy, right? Economic diffusion has become one of these buzzwords that's, like, a reason why we're not going to make AI progress or why AI progress doesn't matter. And, you know, the other axis is, like, we'll get recursive self-improvement, you know, the whole thing, you know, can't you just draw an exponential line on the curve? You know, it's good. We're going to have, you know, Dyson spheres around the sun in, like, you know, so many nanoseconds after, you know, after we get recursive. I mean, I'm completely caricaturing the view here, but, like, you know, there are these two extremes. But what we've seen from the beginning, you know, at least if you look within Anthropic, there's this bizarre 10x per year growth in revenue that we've seen, right? So, you know, in 2023, it was, like, zero to 100 million. 2024, it was 100 million to a billion. 2025, it was a billion to, like, nine or 10 billion. And then... You guys should have just bought, like, a billion dollars with your own products so you could just, like, have a clean 10b. And the first month of this year, like, that exponential is... You would think it would slow down, but it would, like, you know, we added another few billion to, like, you know, we added another few billion to revenue in January. And so, you know, obviously that curve can't go on forever, right? You know, the GDP is only so large. I don't... You know, I would even guess that it bends somewhat this year. But, like, that is, like, a fast curve, right? That's, like, a really fast curve. And I would bet it stays pretty fast even as the scale goes to the entire economy. So, like, I think we should be thinking about this middle world where things are, like, extremely fast but not instant, where they take time because of economic diffusion, because of the need to close the loop, because, you know, it's, like, this fiddly, oh, man, I have to do change management within my enterprise. You know, I have to, like, you know, you know, I, like, I set this up, but, you know, I have to change the security permissions on this in order to make it actually work. Or, you know, I had this, like, old piece of software that, you know, that, like, you know, checks the model before it's compiled and, like, released. And I have to rewrite it. And, yes, the model can do that, but I have to tell the model to do that. And it has to take time to do that. And so I think everything we've seen so far is compatible with the idea that there's one fast exponential that's the capability of the model, and then there's another fast exponential that's downstream of that, which is the diffusion of the model into the economy. Not instant, not slow, much faster than any previous technology, but it has its limits. And this is what we, you know, when I look inside Anthropic, when I look at our customers, fast adoption, but not infinitely fast. Can I try a hot take on you? Yeah. I feel like diffusion is cope that people use to say when it's, like, if the model isn't able to do something, they're, like, oh, but it's, like, a diffusion issue. But then you should use the comparison to humans. You would think that the inherent advantages that AIs have would make diffusion a much easier problem for new AIs getting onboarded than new humans getting onboarded. So an AI can read your entire Slack and your drive in minutes. They can share all the knowledge that the other copies of the same instance have. You don't have this adverse selection problem when you're hiring AIs because you can just hire copies of a vetted AI model. Hiring a human is, like, so much more hassle. And people hire humans all the time, right? We pay humans upwards of $50 trillion in wages because they're useful, even though it's, like, in principle, it would be much easier to integrate AIs into the economy than it is to hire humans. I think, like, the diffusion, I feel like, doesn't really explain. I think diffusion is very real and doesn't have to, you know, doesn't exclusively have to do with limitations on the AI models. Like, again, there are people who use diffusion to, you know, as kind of a buzzword to say this isn't a big deal. I'm not talking about that. I'm not talking about, you know, AI will diffuse at the speed that previous. I think AI will diffuse much faster than previous technologies have, but not infinitely fast. So I'll just give an example of this, right? Like, there's, like, Claude Code. Like, Claude Code is extremely easy to set up. You know, if you're a developer, you can kind of just start using Claude Code. There is no reason why a developer at a large enterprise should not be adopting Claude Code as quickly as, you know, an individual developer or developer at a startup. And we do everything we can to promote it, right? We sell Claude Code to enterprises and big enterprises, like, you know, big financial companies, big pharmaceutical companies, all of them, they're adopting Claude Code much faster than enterprises typically adopt new technology, right? But, again, like, it takes time. Like, any given feature or any given product, like Claude Code or, like, Co-Work will get adopted by the, you know, the individual developers who are on Twitter all the time, by the, like, Series A startups many months faster than, you know, than they will get adopted by, like, you know, a, like, large enterprise that does food sales. There are a number of factors, like, you have to go through legal. You have to provision it for everyone. It has to, you know, like, it has to pass security and compliance. The leaders of the company who are further away from the AI revolution, you know, are forward-looking, but they have to say, oh, it makes sense for us to spend 50 million. This is what this Claude Code thing is. This is why it helps our company. This is why it makes us more productive. And then they have to explain to the people two levels below, and they have to say, okay, we have 3,000 developers. Like, here's how we're going to roll it out to our developers. And we have conversations like this every day. Like, you know, we are doing everything we can to make Anthropics revenue grow 20 or 30x a year instead of 10x a year. You know, and, again, you know, many enterprises are just saying this is so productive. Like, you know, we're going to take shortcuts on our usual procurement process, right? They're moving much faster than, you know, when we tried to sell them just the ordinary API, which many of them use. But Claude Code is a more compelling product. But it's not an infinitely compelling product. And I don't think even AGI or powerful AI or country of geniuses in the data center will be an infinitely compelling product. It will be a compelling product enough maybe to get 3 or 5 or 10x a year growth even when you're in the hundreds of billions of dollars, which is extremely hard to do and has never been done in history before, but not infinitely fast. I buy that it would be a slight slowdown. And maybe this is not your claim, but sometimes people talk about this like, oh, the capabilities are there, but because of diffusion. Otherwise, like, we're basically at AGI and then. I don't believe we're basically at AGI. I think if you had the country of geniuses in a data center, if your company didn't adopt the country of geniuses in a data center. If you had the country of geniuses in a data center, we would know it. Right. We would know it if you had the country of geniuses in a data center. Like, everyone in this room would know it. Everyone in Washington would know it. Like, you know, people in rural parts might not know it. But, like, we would know it. We don't have that now. That's very clear. As Dario was hinting at, to get generalization, you need to train across a wide variety of realistic tasks and environments. For example, with a sales agent, the hardest part isn't teaching it to mash buttons in a specific database in Salesforce. It's training the agent's judgment across ambiguous situations. How do you sort through a database with thousands of leads to figure out which ones are hot? How do you actually reach out? What do you do when you get ghosted? When an AI lab wanted to train a sales agent, Labelbox brought in dozens of Fortune 500 salespeople to build a bunch of different RL environments. They created thousands of scenarios where the sales agent had to engage with a potential customer, which was role played by a second AI. Labelbox made sure that this customer AI had a few different personas. Because when you cold call, you have no idea who's going to be on the other end. You need to be able to deal with a whole range of possibilities. Labelbox's sales experts monitored these conversations turn by turn, tweaking the role-playing agent to ensure it did the kinds of things an actual customer would do. Labelbox could iterate faster than anybody else in the industry. This is super important because RL is an empirical science. It's not a solved problem. Labelbox has a bunch of tools for monitoring agent performance in real time. This lets their experts keep coming up with tasks so that the model stays in the right distribution of difficulty and gets the optimal reward signal during training. Labelbox can do this sort of thing in almost every domain. They've got hedge fund managers, radiologists, even airline pilots. So whatever you're working on, Labelbox can help. Learn more at labelbox.com slash thorkhesh. Coming back to concrete predictions, because I think because there's so many different things to disambiguate, it can be easy to talk past each other when we're talking about capabilities. So, for example, when I interviewed you three years ago, I asked you a prediction about what should we expect three years from now. I think you were right. So you said we should expect systems, which if you talk to them for the course of an hour, it's hard to tell them apart from a generally well-educated human. Yes. I think you were right about that. And I think spiritually I feel unsatisfied because my internal expectation was that such a system could automate large parts of white-collar work. And so it might be more productive to talk about the actual end capabilities. You want such a system. So I will basically tell you where I think we are. But let me ask you in a very specific question so that we can figure out exactly what kinds of capabilities we should expect soon. So maybe I'll ask about it in the context of a job I understand well, not because it's the most relevant job, but just because I can evaluate the claims about it. Take video editors, right? I have video editors, and part of their job involves learning about our audience's preferences, learning about my preferences and tastes and the different trade-offs we have. And just over the course of many months building up this understanding of context. And so the skill and ability they have six months into the job, a model that can pick up that skill on the job, on the fly, when should we expect such an AI system? Yeah. Yeah. So I guess what you're talking about is like, you know, we're doing this interview for three hours. And then like, you know, someone's going to come in, someone's going to edit it. They're going to be like, oh, you know, you know, I don't know, Dario like, you know, scratched his head and, you know, we could edit that out. And, you know, there was this like long discussion that like is less interesting to people. And then, you know, then there's other thing that's like more interesting to people. So, you know, let's kind of make this edit. So, you know, I think the country of geniuses in a data center will be able to do that. The way it will be able to do that is, you know, it will have general control of a computer screen, right? Like, you know, and you'll be able to feed this in and it'll be able to also use the computer screen to like go on the web, look at all your previous interviews. Like look at what people are saying on Twitter in response to your interviews, like talk to you, ask you questions, talk to your staff, look at the history of kind of edits, edits that you did. And from that, like do the job. Yeah. So I think that's dependent on several things. One that's dependent. And I think this is one of the things that's actually blocking deployment, getting to the point on computer use where the models are really masters at using the computer. Right. And, you know, we've seen this climb in benchmarks and benchmarks are always, you know, imperfect measures. But like, you know, OS world is, you know, went from, you know, like 5%, you know, like I think when we first released, you know, computer use like a year and a quarter ago, it was like maybe 15%. I don't remember exactly. But we've climbed from that to like 65% or 70%. And, you know, there may be harder measures as well. But I think computer use has to pass a point of reliability. Can I just ask a follow up on that before we move on to the next point? I often, for years, I've been trying to build different internal LLM tools for myself. And I often I have these text in, text out tasks, which should be dead center in the repertoire of these models. And yet I still hire humans to do them just because it's if it's something like make identify what the best clips would be in this transcript. And maybe they'll do like a seven out of 10 job at them. But there's not this ongoing way I can engage with them to help them get better at the job the way I could with a human employee. And so that missing ability, even if you saw computer use, would still block my ability to like offload an actual job to them. Again, there's there's this gets back to what to kind of to kind of what we were talking about before with learning on the job where it's it's very interesting. You know, I think I think with the coding agents, like I don't think people would say that learning on the job is what is what is, you know, preventing the coding agents from like, you know, doing everything end to end. Like they keep they keep getting better. We have engineers at Anthropic who like don't write any code. And when I look at the productivity to your to your previous question, you know, we have folks who say this this GPU kernel, this chip. I used to write it myself. I just have Claude do it. And so there's this there's this enormous improvement in productivity. And I don't know, like when I see Claude code, like familiarity with the code base or like, you know, or or a feeling that the model hasn't worked at the company for a year. That's not high up on the list of complaints I see. And so I think what I'm saying is we're like we're kind of taking a different path. Don't you think with coding that's because there is an external scaffold of memory which exists instantiated in the code base, which I don't know how many other jobs have coding made fast progress precisely because it has its unique advantage that other economic activity doesn't. But but but when you say that what you're what you're implying is that by reading the code base into the context, I have everything that the human needed to learn on the job. So that would be an example of whether it's written or not, whether it's available or not, a case where everything you needed to know you got from the context window. Right. And that and that what we think of as learning like, oh, man, I started this job. It's going to take me six months to understand the code base. The model just did it in the context. Yeah, I honestly don't know how to think about this because there are people who qualitative report what you're saying. There was a meter study I'm sure you saw last year. Yes. Where they had experienced developers try to close pull request in repositories that they were familiar with. And those developers reported an uplift. They reported that they felt more productive with the use of these models. But in fact, if you look at their output and how much was actually merged back in, there's a 20 percent downlift. They were less productive as a result of these models. And so I'm trying to square the qualitative feeling that people feel with these models versus one, in a macro level, where are all these where is this like renaissance of software? And two, when people do these independent evaluations, why are we not seeing the. Yeah. So. Productive benefits that we would expect. Within Anthropic, this is just really unambiguous. Right. We're under an incredible amount of commercial pressure and make it even hard, harder for ourselves because we have all the safety stuff we do that I think we do more than than other companies. So like the the the pressure to survive economically while also keeping our values is is just incredible. Right. We're trying to keep this 10x revenue curve going. There's like there is zero time for bullshit. There is zero time for feeling like we're productive when we're not like these tools make us a lot more productive. Like why why do you think we're concerned about competitors using the tools? Because we think we're ahead of the competitors and like we don't we don't want to sell. We we we wouldn't be going through all this trouble if this was secretly reducing reducing our productivity. Like we see the end productivity every few months in the form of model launches. Like there's no kidding yourself about this. Like the models make you more productive. One that people feeling like they're more productive is qualitatively predicted by studies like this. But two if I just look at the end output obviously you guys are making fast progress. But the fact you know the the the idea was supposed to be with recursive self-improvement is that you make a better AI. The AI helps you build a better next AI etc etc. And what I see instead if I look at the you open AI deep mind is that people are just shifting around the podium every few months. And maybe you think that stops because you you've won or whatever. But but why are we not seeing the person with the best coding model have this lasting advantage if in fact there are these enormous productivity gains from the last coding model. So no no no I I mean I mean I mean I think it's all like my my model of the situation is there's there's an advantage that's gradually growing like I would say right now the coding models give maybe I don't know a a like 15 maybe 20 percent total factor speed up like that's my view. Um uh and six months ago it was maybe five percent and so and so it didn't matter like five percent doesn't register. It's now just getting to the point where it's like one of several factors that that kind of matters. And and that's gonna that's gonna keep speeding up. And so I think six months ago like you know there were several there were several companies that were at roughly the same point. Because uh you know this this wasn't uh this wasn't a notable factor, but I think it's starting to speed up more and more. I you know I I would I would also say there are multiple companies that you know write models that are used for code. And you know we're not perfectly good at you know preventing some of these other companies from from from using from from from kind of using our models internally. Um so uh you know I think I think everything we're kind of everything we're seeing is consistent with this kind of um this kind of snowball model. Where where you know there's no hard again my my my my my theme in all of this is like all of this is soft takeoff like soft smooth exponentials. Although the exponentials are relatively steep. And so and so we're seeing this snowball gather momentum where it's like 10 percent 20 percent 25 percent you know for 40 percent. And as you go yeah and all's all you have to get all the like things that are preventing you from from closing the loop out of the way. But like this is one of the biggest priorities within anthropic. Um stepping back I think before in the stack we were talking about um well when do we get this on the job learning. And it seems like the coding the point you were making the coding thing is we actually don't need on the job learning. Uh that you can have tremendous productivity improvements. You can have potentially trillions of dollars of revenue for AI companies without this basic human ability. Maybe that's not your claim you should clarify. Um but without this basic human ability to learn on the job. But I just look at like in in most domains of economic activity people say I hired somebody they weren't that useful for the first few months. And then over time they built up the context understanding it's actually hard to define what we're talking about here. But they got something and then now now they're they're power horse and they're so valuable to us. And if AI doesn't develop this ability to learn on the fly I'm not I'm a bit skeptical that we're going to see huge changes to the world. Yeah. So I think I think I think two things here right. There's the state of the technology right now. Um which is again we have these two stages. We have the pre-training and RL stage where you throw you throw a bunch of data and tasks into the models and then they generalize. So it's like learning but it's like learning from more data and and not you know not learning over kind of one human or one models lifetime. So again this is situated between evolution and and and human learning. But once you learn all those skills you have them. And and just like with pre-training just how the models know more you know if if I look at a pre-trained model you know it knows more about the history of samurai in Japan than I do. It knows more about baseball than I do. It knows you know it knows more about you know low pass filters and electronics than you know all all of these things. It's knowledge is way broader than mine. So I think I think even even just that um you know may get us to the point where the models are better at you know kind of better at everything. And then we also have again just with scaling the kind of existing setup. We have the in context learning which I would describe as kind of like human on the job learning. But like a little weaker and a little short term like you look at in context learning the you give the model a bunch of examples it does get it there's real learning that happens in context. And like a million tokens is a lot that's that's you know that can be days of human learning right. You know if you think about the model you know you know kind of read reading that being a million words you know it you know it takes me how long would it take me to read a million. I mean you know like days or weeks at least um uh so you have these two things and and I think these two these two things within the existing paradigm may just be enough to get you the country of geniuses in the data center. I don't know for sure but I think they're going to get you a large fraction of it. There may be gaps but I I certainly think just as things are this I believe is enough to generate trillions of dollars of revenue. That's one that's all one. Two is this idea of continual learning this idea of a single model learning on the job. I think we're working on that too and I think there's a good chance that in the next year or two we also make we also solve that. Um I again I I I you know I think you get most of the way there without it. I think the trillions of dollars of of you know the the I think the trillions of dollars a year market. Maybe all the national security implications and the safety implications that I wrote about in adolescence of technology can happen without it. But I I I also think we and I imagine others are working on it and I think there's a good chance that that you know that we get there within the next year or two. There are a bunch of ideas. I won't go into all of them in detail but um you know one is just make the context longer. There's there's nothing preventing longer context from working. You just have to train at longer context and then learn to to serve them at inference and both of those are engineering problems that we are working on and that I would assume others are working on as well. Yeah so this context line increase it seemed like there was a period from 2020 to 2023 where from gpd3 to gpd4 turbo there was an increase from like 2000 context lines to 128k. I feel like for the next for the two-ish years since then we've been in the same-ish ballpark. Yeah. And when model context lines get much longer than that people report qualitative degradation in the ability of the model to consider that full context. Um so I'm curious what you're internally seeing that makes you think like oh 10 million context 100 million context to get human like six months learning billion billion context. This isn't a research problem this is a this is an engineering and inference problem right if you want to serve long context you have to like store your entire kv cache you have to you know um uh you know it's it's it's it's difficult to store all the memory in the gpus to juggle the memory around. I don't even know the detail you know at this point this is at a level of detail that that that i'm no longer able to follow although you know i knew it in the gpd3 era of like you know these are the weights these are the activations you have to store. Um uh but you know you know these days the whole thing is flipped because we have moe models and and and kind of all of that but um uh and and this degradation you're talking about like again without getting too specific like a question i would ask is like there's two things there's the context length you train at and there's a context length that you serve at. If you train at a small context length and then try to serve at a long context length like maybe you get these degradations it's better than nothing you might still offer it but you get these degradations. And maybe it's harder to train at a long context length yeah so you know there's there's a lot. I i want to at the same time ask about like maybe some rabbit holes of like well wouldn't you expect that if you had to train on longer context length that would mean that um you're able to get sort of like less samples in for the same amount of compute but before maybe it's not worth diving deep on that i want to get an answer to the bigger picture question which is like okay so um i don't feel a preference for a human editor. A preference for a human editor that's been working for me for six months versus an ai that's been working with me for six months. What year do you predict that that will be the case? I my i mean you know my guess for that is you know there's there's a lot of problems that are basically like we can do this when we have the country of geniuses in a data center. Um and so you know my my my my picture for that is you know again if you if you if you if you know if you made me guess it's like one to two years maybe one to three years it's really hard to tell i have a i have a strong view 99 95 percent that like all this will happen in 10 years like that's i think that's just a super safe bet yeah. And then i have a hunch this is more like a 50 50 thing that it's going to be more like one to two maybe more like one to three. So one to three years the country of geniuses um and the slightly less economically valuable task of editing videos. It seems pretty economically valuable let me tell you it's just there are a lot of use cases like that right there are a lot of similar exactly so you're predicting that within one to three years. Um and then generally anthropic has predicted that by late 26 early 27 we will have ai systems that are quote um have the ability to navigate interfaces available to humans doing digital work today intellectual capabilities mashing or exceeding that of noble prize winners. And the ability to interface with the physical world and then you give an interview two months ago with deal book where you're emphasizing your um your company's more responsible compute scaling as compared to your competitors. And i'm trying to square these two views where if you really believe that we're gonna have a country of geniuses you you want as big a data center as you can get there's no reason to slow down the tam of a noble prize winner that is actually can do everything a prize winner can do is like treat it. And so i'm trying to square this conservatism uh which seems rational if you have more moderate timelines with your stated views about ai progress. Yeah so so it actually all fits together and we go back to this fast but not infinitely fast diffusion. So like let's say that we're making progress at this rate um you know the the technology is making progress this fast again i have you know very high conviction that like it's going you know the the you know we're we're gonna get there within within a few years. So a little uncertainty on the technical side but like you know pretty pretty strong confidence that it won't be off by much what i'm less certain about is again the economic diffusion side. Like i really do believe that we could have models that are a country of geniuses 100 country of geniuses in the data center in one to two years. One question is how many years after that do the trillions in you know do the trillions in revenue start rolling in. Um i don't think it's guaranteed that it's going to be immediate um you know i think it could be um one year it could be two years i could even stretch it to five years although i'm like i'm skeptical of that. And so we have this uncertainty which is even if the technology goes as fast as i suspect that it will. We we don't know exactly how fast it's going to drive revenue we we know it's coming but with the way you buy these data centers if you're off by a couple years that can be ruinous. It is just like how i wrote you know in machines of loving grace i said look i think we might get this powerful ai this country geniuses in the data center that description you gave comes from the machines of loving grace. I said we'll get that 2026 maybe 2027 again that is that is my hunch wouldn't be surprised if i'm off by a year or two but like that is my hunch. Let's say that happens that's the starting gun how long does it take to cure all the diseases right that's that's one of the ways that like drives a huge amount of of of of economic value right like you cure you cure every disease you know there's a question of how much of that goes to the pharmaceutical company to the ai company but there's an enormous consumer surplus because everyone you know every as soon as soon as possible. We can get access for everyone which i care about greatly we you know we cure all of these diseases how long does it take you have to do the biological discovery you have to you know you have to you know manufacture the new drug you have to you know go through the regulatory process and we saw this with like vaccines and covid right like it there's just this we got the vaccine out to everyone but it took a year and a half right. Right and and so my question is how long does it take to get the cure for everything which ai is the genius that can in theory invent out to everyone how long from when that ai first exists in the lab to when diseases have actually been cured for everyone. Right and you know we've had a polio vaccine for 50 years we're still trying to eradicate it in the most remote corners of Africa and you know the gates foundation is trying as hard as they can others are trying as hard as they can but you know that's difficult. That's difficult again i you know i don't expect most of the economic diffusion to be as difficult as that right that's like the most difficult case. But but but there's a there's a real dilemma here and and where i've settled on it is it will be it will be it will be faster than anything we've seen in the world but it still has its limits. And and and so then when we go to buying data centers you know you again again the curve i'm looking at is okay we you know we've had a 10x a year increase every year so beginning of this year we're looking at 10 billion in in in annual in you know rate of annualized revenue at the beginning of the year. We have to decide how much compute to buy um and you know it takes a year or two to actually build out the data centers to reserve the data center so basically i'm saying like in uh 2027 how much compute do i get well i could assume um uh that uh the uh revenue will continue growing 10x a year so it'll be uh one uh one uh 100 billion at the end of 2026. And so i could buy a trillion dollars actually would be like five trillion dollars of compute because it would be a trillion dollar a year for for five years right i could buy a trillion dollars of compute that starts at the end of 2027. And if my if my revenue is not a trillion dollars if it's even 800 billion there's no force on earth there's there's no hedge on earth that could stop me from going bankrupt if i if i buy that much compute and and so even though a part of my brain wonders if it's going to keep going 10x i can't buy a trillion dollars a year of compute in in in in in in in in in in in in in in in in 2027. And if i'm just off by a year in that rate of growth or if the the growth rate is 5x a year instead of 10x a year then then you know that you go bankrupt um and and and and so you end up in a world where you know you're supporting hundreds of billions not trillions and you accept you accept some risk that there's so much demand that you can't support the revenue. And you accept still some risk that you know you got it wrong and it's still so and so when i talked about behaving responsibly what i meant actually was not the absolute amount that that actually was not um you know i think it is true we're spending somewhat less than some of the other players it's actually the other things like have we been thoughtful about it or are we yoloing and saying oh we're gonna do 100 billion dollars here 100 billion dollars there i kind of get the impression that you know some of the other companies have not written down the other companies have not written down the spreadsheet that they don't really understand the risks they're taking they're just kind of doing stuff because it sounds cool um uh and and we've thought carefully about it right we're an enterprise business therefore you know we can rely more on revenue it's less fickle than consumer we have better margins which is the buffer between buying too much and buying too little and so i think we bought an amount that allows us to capture pretty strong upside worlds it won't capture the full 10x a year um and things would have to go pretty badly for us to be for us to be in financial trouble so i think we thought carefully and we've made that balance and and that's what i mean when i say that we're being responsible okay so it seems like um it's possible that we're we actually just have different definitions of country of a genius in a data center because when i think of like actual human geniuses an actual country of human geniuses in the data center i'm like i would happily buy five trillion dollars worth of compute to run uh actual culture of human geniuses in the data center so let's say jp morgan or moderna or whatever doesn't want to use them also i've got a country of geniuses they'll start their own company and if like they can't start their own company and they're bottlenecked by clinical trials it is worth stating with clinical trials like most clinical trials fail because the drug doesn't work there's not efficacy right and i make exactly that point in in machines of love and grace i say the clinical trials are going to go much faster than we're used to but not not not instant not infinitely fast and then suppose it takes a year to for the clinical trials to work out so that you're getting revenue from that and can make more drugs okay well you've got a country of geniuses and you're an ai lab and you have you could use uh many more ai researchers um you also think that there's these like self-reinforcing gains from you know smart people working on ai tech so like okay you can have the that's right but you can have the data center working on like ai progress is there more gains from buying like substantially more gains from buying a trillion dollars a year of compute versus 300 billion dollars a year of compute if your competitor is buying a trillion yes there is well no there's some gain but then but again there's this chance that they go bankrupt before uh you know before uh you know before again if you're off by only a year you destroy yourselves that's that that's the balance we're buying a lot we're buying a hell of a lot like we're not we're you know we're buying an amount that's comparable to that that you know the the the the the biggest players in the game are buying um but but if you're asking me why why haven't we signed you know 10 10 trillion of compute starting and starting in mid 2027 first of all it can't be produced there isn't that much in the world um uh but but second um what if the country of geniuses comes but it comes in mid 2028 instead of mid 2027 you go bankrupt so if your projection is one to three years it seems like you should have won 10 trillion dollars to compute by um 2029 2020 and maybe 2020 latest like i mean you know you like are you like it seems like even in your the longest version of the timelines you state the compute you are ramping up to build doesn't seem what what what what makes you think that well you as you said you want the 10 trillion the human wages let's say are um on the order of 50 trillion a year if you look at so so i won't i won't talk about anthropic in particular but if you talk about the industry like um the amount of compute the industry hit you know the the the amount of compute the industry's building this year is probably in the you know i don't know very low tens of you know call it 10 15 gigawatts next year i you know it goes up by roughly 3x a year so like next year's 30 or 40 gigawatts and um 2028 might be 100 20 29 might be like three 300 gigawatts and like each gigawatt costs like um maybe 10 i mean i'm doing the math in my head but each gigawatt it costs maybe 10 billion dollars you know or order 10 to 15 billion dollars a year so you know you kind of you you know you put that all together and you're getting about about what you described you're getting multiple trillions a year by 2028 or 2029 so you're you're getting exactly that you're getting you're getting exactly what you predict um that's for the industry that that's for the industry that's right so suppose anthropics compute keeps 3xing a year and then by like 27 you have uh or 27 28 you have 10 gigawatts and like multiply that by as you say um 10 billion so then it's like 100 billion a year but then you're saying the tam by 2028 29 i don't want to give exact numbers for anthropic but but these numbers are too small these numbers are too small okay interesting i'm really proud that the puzzles i've worked on with jane street have resulted in them hiring a bunch of people from my audience well they're still hiring and they just send me another puzzle for this one they spent about 20 000 gpu hours trading backdoors into three different language models each one has a hidden prompt that elicits completely different behavior you just have to find the trigger this is particularly cool because finding backdoors is actually an open question in frontier ai research anthropic actually released a couple of papers about sleeper agents and they showed that you can build a simple classifier on the residual stream to detect when a backdoor is about to fire but they already knew what the trigger were because they built them here you don't and it's not feasible to check the activations for all possible trigger phrases unlike the other puzzles they made for this podcast jane street isn't even sure this one is solvable but they've set aside fifty thousand dollars for the best attempts and write-ups the puzzle's live at janestreet.com slash thwarkesh and they're accepting submissions until april 1st all right back to daria you've told investors that you plan to be profitable starting in 28 and this is the year where we're like potentially getting the country of geniuses the data center and you know this is like gonna now unlock all this uh progress and uh medicine and uh health and etc etc and new technologies wouldn't this be particularly exactly the time where you'd like want to reinvest in the business and build bigger countries so they can make more discoveries so i mean profit profitability is this kind of like weird thing in this field i i like like i don't think i don't think in this field profitability is actually a measure of uh um you know kind of spending down versus investing in the business like let's let's just let's just take a model of this i actually think profitability happens when you underestimated the amount of demand you were going to get and loss happens when you overestimated the amount of demand you were going to get um because you're buying the data centers ahead of time so think about it this way um ideally you would like and again these are stylized facts these numbers are not exact for i'm just trying to make a toy model here let's say half of your compute is for training and half of your compute is for inference um and you know the inference has some gross margin that's like more than 50 percent um and so what that means is that if you were in steady state you build a data center if you knew exactly exactly exactly the demand you were getting you would um uh uh uh uh you know you you would you would you would you would you would get a certain amount of revenue say i don't know uh uh let's say you pay a hundred billion dollars a year for compute and on 50 billion dollars a year you support 150 billion dollars on of of of of of revenue and the other 50 billion the other 50 billion are used for training um so basically you're profitable you make fit you make fit you make 50 billion dollars of profit those are the economics of the industry today or sorry not today but like that's where we're where we're projecting forward in a year or two the only thing that makes that not the case is if you get less demand than 50 billion um then you have more than 50 percent of your your data center for research and you're not profitable so you you know you train stronger models but you're like not profitable um if you uh get more demand than you thought then your research gets squeezed um but uh you know you're you're kind of able to support more inference and you're more profitable so it's uh maybe i'm not explaining it well but but the thing i'm trying to say is you decide the amount of compute first and then you have some target desire of inference versus versus training but that gets determined by demand it doesn't get determined by what i'm hearing is the reason you're predicting profit is that you are systematically underestimate under investing in compute right because if you actually like i'm saying i'm saying it's hard to predict so so these things about 2028 and when it will happen that's our that's our attempt to do the best we can with investors all of this stuff is really uncertain because of the cone of uncertainty like we could be profitable in 2026 if the if the revenue grows fast enough and then and then um uh you know if we if we overestimate or underestimate the next year that could swing wildly like i i i what i'm trying to get is you have a model in your head of like the the business invest invest invest invest get scale and and and and kind of then becomes profitable there's a single point at which things turn around i don't think the economics of this industry work that way i see so if i'm understanding correctly you're saying because of the discrepancy between the amount of compute we should have gotten and the amount of compute we got we we were like sort of forced to make profit but that that doesn't mean we're going to continue making profit we're going to like reinvest the money because well now ai has made so much progress and we want the bigger country of geniuses and so then back into uh revenue is high but losses are also high if we if we predict if every year we predict exactly what the demand is going to be will be profitable every year because grow because spending spending 50 of your compute on on on 50 of your compute on research roughly um plus a gross margin that's higher than 50 and and correct demand prediction leads to profit that's the prof that's that's the profitable business model that i think is kind of like there but like obscured by these like building ahead and prediction errors i guess you're treating the 50 as a uh as a sort of like you know just like a given constant whereas you in fact if you if the ai progress is fast and you can increase the progress by scaling up more you just have more than 50 percent and not make profit here's what i'll say you might want to scale up it more you might want to scale it up more but but but you know remember the log returns to scale right if if 70 percent would get you a a a very little bit of a smaller model through a factor of of 1.4x right like that extra 20 billion dollars is is is you know that each each dollar there is worth much less to you because it because because the log linear setup and so you might find that it's better to invest that that that it's better to invest that 20 billion dollars in you know in in serving inference or in hiring engineers who are who are who are kind of better who are who are kind of better who are kind of better at what they're doing so the reason i said 50 that's not that's not exactly our target it's not exactly going to be 50 it'll probably vary vary over time what i'm saying is the the the the like log linear return what it leads to is you spend of order one fraction of the business right like not five percent not 95 and then it then it that you know then then you get diminishing returns because of the because of the law everyone's strange that i'm like convincing dario to like believe in ai progress or something but like uh you okay you don't invest in research because it has diminishing returns but you invest in the other things you mentioned again again we're talking about diminishing returns after you're spending 50 billion a year right like this is a point i'm sure you would make but like diminishing returns on a genius is could be quite high and more generally like what is profit in the market economy profit is basically saying the other companies in the market can like do more things with this money that i yeah i mean put aside anthropic i'm just trying to like because i you know i don't want to give information about anthropic is why i'm giving these stylized numbers but like let's just derive the equilibrium of the industry right i think the so so so why doesn't everyone spend 100 of their um uh you know 100 of their compute on training and not serve any customers right it's because if they didn't get any revenue they couldn't raise money they couldn't do compute deals they couldn't buy more compute the next year so there's going to be an equilibrium where every every company spends less than 100 on on on on training and certainly less than 100 on inference it should be clear why you don't just serve the current models and and you know and and and and never train another model because then you don't have any demand because you'll because you'll fall behind so there's some equilibrium it's it's not going to be 10 it's not going to be 90 let's just say as a stylized fact it's 50 that's what i'm getting at and and and i think we're going to be in a position where that equilibrium of how much you spend on training is less than the gross margins that that you're at you that that that you're able to get on compute and so the the the underlying economics are profitable the problem is you have this this hellish demand prediction problem when you're when you're buying the next year of compute and you might guess under and be very profitable but have no compute for research or you might guess over and you know you're you're you're um uh you you are not profitable and you have all the compute compute for research in the world does that make sense just as a dynamic model of the industry maybe stepping back i'm like uh i i'm not saying i i think the country of genius is going to come in two years and therefore you should buy this compute um to me what you're saying the end conclusion you're arriving it makes a lot of sense but uh that's because like oh it seems like country geniuses is hard and there's a long way to go and so the stepping back the thing i'm trying to get at is more like it seems like your worldview is compatible with somebody who says uh we're like 10 years away from a world in which like we're generating trillions of dollars that's just that's just not my view yeah that is that is not my view like i i so so so i'll like i'll like make another prediction it is hard for me to see that that there won't be trillions of dollars in revenue before 2030. um like uh i can i can construct a plausible world it takes maybe three years so that would you know that would be the end of what i think it's plausible like in 2028 we get the the real country of geniuses in the data center you know the revenue's been been go you know the revenue's been going into the maybe is in the low hundreds of billions by by by by 2028 and and and then the country of geniuses accelerates it to trillions you know and and we're basically we're basically on the slow end of diffusion it takes two years to get to the trillions that that would that that would be the world where it takes until that would be the world where it takes until 2030. i i i suspect even composing the technical exponential and diffusion exponential will get there before 2030. so you laid out a model where anthropic makes profit because it seems like fundamentally we're in a compute constrained world and so it's like eventually we keep growing compute no i think i think the way the profit comes is again and and you know let's let's just abstract the whole industry here like we have a you know let's just imagine we're we're in like an economics textbook we have a small number of firms each can invest a limited amount in you know or or like each can invest some fraction fraction in r d they have some marginal cost to serve the margins on that the profit margin the gross profit margins on that marginal cost are like very high because because because inference is efficient there's some competition but the models are also differentiated there's some there's some um you know companies will compete to push their research budgets up but like because there's a small number of players you know we have the what is it called the neck and the corno equilibrium i think is what the what the uh small number of firm equal equilibrium is it the point is it doesn't equilibrate to perfect competition with with with with with with with zero margins if there's like three firms if there's three firms in the economy all are kind of independently behaving behaving rationally it doesn't equilibrate to zero um help me understand that because right now we do have three leading firms and they're not making profit um and so what what uh yeah what what is changing yeah so the the again the gross margins right now are very positive what's happening what what's happening is a combination of two things one is we're still in the exponential scale up phase of compute yeah um so what basically what that means is we're training like a model gets trained yeah it costs you know let's say a model got trained that costs uh a billion dollars last year um and then uh this year it produced uh four billion dollars of revenue and cost one billion dollars to to uh to to to inference from um so you know again i'm using stylized number here but you know that would be 75 percent you know gross gross gross margins and you know this this 25 tax so that model as a whole makes two billion dollars um but at the same time we're spending 10 billion dollars to train the next model because there's an exponential scale up and so the company loses money each model makes money but the company loses money the equilibrium i'm talking about is an equilibrium where we have the country of geniuses we have the country of geniuses in the data center but that that um model training scale up has equilibrated more maybe maybe it's still it's still going up we're still trying to predict the demand but it's more it's more um leveled out um i'm curious a couple things there so um let's start with the current world um in the current world you're right that as you said before if you treat each individual model as a company it's profitable but of course a big part of the production function of being a frontier lab is training the next model right so if you didn't do that then you'd make profit for two months that's right and you wouldn't have margins because you wouldn't have the best model and then so yeah you can make profits right at some point that reaches the biggest scale that it can reach and then and then in equilibrium we have algorithmic improvements but we're spending roughly the same amount to train the next model as as we spent to train the current model um so this equilibrium relies i mean at some point at some at some point you run out of money in the economy uh a fixed lump of labor follows the economy is going to grow right that's one of your predictions well we're gonna have yes but this is this is this but this is another example of the theme i was talking about which is that the economy will grow much faster with ai than i think it ever has before but it's not like right now the computer is growing 3x a year yeah i don't believe the economy is going to grow 300 a year like i said this in machines of loving grace like i think we may get 10 or 20 per year growth in the economy but we're not going to get 300 growth in the economy so i think i think in the end you know if compute becomes the majority of what the economy produces it's it's gonna it's gonna be capped by that so let's okay now let's assume a model where compute stays capped yeah um the world where frontier labs are making money is one where they continue to make um fast progress because fundamentally your margin is limited by how good the alternative is and so you are able to make money because you have a frontier model um if you didn't have a frontier model you wouldn't be making money um and and so this this model requires there never to be a steady state like forever and ever you keep making no i don't i don't think that's true i mean i i feel i feel like we're we're like we're taught we're you know we're they feel like this is an economics uh like uh you know this is like an economics class you know the tyler cowen quote we never stop talking about economics we never we never stop talking about economics so no but but there there are there are worlds in which um you know they're the so i i don't think this field's gonna be i don't think this field's gonna be a monopoly all my lawyers never want me to say the word monopoly um but i don't think this field's gonna be a monopoly but but you do get you get industries in which there are a small number of players not one but a small number of players and ordinarily like the the way you get monopolies like facebook or or meta i always call them facebook but um uh uh is is these kind of net is these kind of these kind of network effects the way you get industries in which there are small number of players are very high costs of entry right um so you know uh cloud is like this i think cloud is a good example of this you have three maybe four players within cloud i think i think that's the same for ai three maybe four um uh and the reason is that it's it's so expensive it requires so much expertise and so much capital to like run a cloud company right and so you have to put up all this capital and then in addition to putting up all this capital you have to get all this other stuff that like you know requires a lot of skill to you know to make it happen and so it's like if you go to someone and you're like i want to disrupt this industry here's 100 billion dollars you're like okay i'm putting 100 billion dollars and also betting that you can do all these other things that these people have been doing only to decrease the profit and then and then the effect of your entering is the profit margins go down so you know we have equilibria like this all the time in the economy where we have a few we have a few players profits are not astronomical margins are not astronomical but they're they're not zero right um uh and and you know i think i think that's what we see on cloud cloud is very undifferentiated models are more differentiated than cloud right like everyone knows claude is claude claude is good at different things than gpt is good at is then then gemini is good at and it's not just claude's good at coding gpt's good at you know math and reasoning you know um uh it's more subtle than that like models are good at different types of coding models have different styles like i think i think these things are actually you know quite different from each other and so i would expect more differentiation than you see in in um cloud now there there actually is a uh counter there there is one counter argument um and that counter argument is that if all of that the process of producing models um becomes uh if ai models can do that themselves then that could spread throughout the economy but that is not an argument for commoditizing ai models in general that's kind of an argument for commoditizing the whole economy at once um i don't know what what quite happens in that world where basically anyone can do anything anyone can build anything and there's like no mode around anything at all i mean i don't know maybe we want that world like like maybe that's the maybe that's the end state here like maybe maybe um you know when what maybe when when kind of ai models can do you know when when when am i was can do everything if we've solved all the safety and security problems like you know that's one of the one of the one of the mechanisms for for uh you know um uh uh you know just just kind of the economy flattening itself again but but that's kind of like post like far post country geniuses in a data center um maybe a finer way to put that uh potential point is one um it seems like ai research is especially loaded on raw intellectual power which will be especially abundant in a world with agi and two if you just look at the world today there's very few technologies that seem to be diffusing as fast as um as ai algorithmic progress and so that does hint that this industry is sort of structurally diffusive so i think coding is going fast but i think ai research is a superset of coding and there are aspects of it that are not going fast um uh but i but i do think again once we get coding once we get ai models going fast then you know ai you know that will speed up the ability of ai models to kind of to kind of do everything else so i think while coding is going fast now i think once the ai models are building the next ai models and building everything else the kind of whole the whole economy will sort of kind of go at the same pace i am i am worried geographically though i'm a little worried that like just proximity to ai having heard about ai um uh that that that may be one differentiator and so when i said the like you know 10 or 20 growth rate a worry i have is that the growth rate could be like 50 in silicon valley and you know parts of the world that are kind of socially connected to silicon valley and you know not that much faster than its current pace elsewhere and i think that'd be a pretty messed up world so i one of the things i think about a lot is how to prevent that yeah do you think that once we have uh this country of geniuses at a center that robotics is sort of quickly solved afterwards because it seems like a big problem with robotics is that um a human can learn how to teleoperate current hardware but current ai models can't at least not not in a way that's super productive and so if we have this ability to learn like a human should it solve robotics immediately as well i don't think it's dependent on learning like a human it could happen in different ways again we could have trained the model on many different video games which are like robotic controls or many different simulated robotics environments or just you know train them to control computer screens and they learn to generalize so it will happen it it's not necessarily dependent on human-like learning human-like learning is one way it could happen if the model's like oh i pick up a robot i don't know how to use it i learn that that could happen because we discovered uh discovering continual learning that could also happen because we train the model on a bunch of environments and then generalized or it could happen because the model learns that in the context length it it doesn't actually matter which way if we go back to the discussion we had like like an hour ago that type of thing can happen in that type of thing can happen in several different ways yeah um uh uh but but i do think when for for whatever reason the models have those skills then uh robotics will be revolutionized both the design of robots because the models will be much better than humans at that um and also the the ability to kind of control robots so we'll get better at the physical building the physical hardware building the physical robots and we'll also get better at controlling it now you know does that mean the robotics industry will also be generating trillions of dollars of revenue my answer there is yes but there will be the same extremely fast but not infinitely fast diffusion so will robotics be be revolutionized yeah maybe tack on another year or two that's that's my that's the way i think about these things uh there's a general skepticism about extremely fast progress like here's my be which is like it sounds like you are going to solve continual learning one or another within the matter of years but just as people weren't talking about continual learning a couple years ago and then we realized oh why aren't these models as useful as they could be right now even though they are clearly passing the turing test and are experts in so many different domains maybe it's this thing and then we solve this thing and we realize actually there's another um another thing that human intelligence can do and that's a basis of human labor that these models can't do and then so why not think there will be more things like this so i think that like we're you know we've like found the pieces of human intelligence well well to be clear i mean i think continual learning as i said before might not be a barrier at all right like like you know i think i think we maybe just get there by pre-training generalization and and and and and and and and rl generalization like i i think there just might not be um there basically might not be such a thing at all in fact i would point to the history in in ml of people coming up with things that are barriers that end up kind of dissolving within the big blob of compute right that you know people talked about you know you know how do you have you know how do how do your models keep track of nouns and verbs and you know how do they you know they can understand semantic syntactically but they can't understand semantically you know it's only statistical correlations you can understand a paragraph you can understand a word there's reasoning you can't do reasoning but then suddenly it turns out you can do code and math very well at all so i i think they're actually there's there's actually a stronger history of some of these things seeming like a big deal and then and then kind of and then kind of dissolving some of them are real i mean the need for data is real maybe continual continual learn continual learning is a real thing but again i would ground us in something like code like i think we may get to the point in like a year or two where the models can just do sweet and end like that's a whole task that's a whole sphere of human activity that that we're just saying models can do it now um when you say end to end do you mean um setting technical direction understanding the context of the problem yes et cetera yes yes i mean all of that interesting i mean that that is i feel like agi complete um which maybe is internally consistent but um it's not like saying 90 of code or 100 of code it's like no no no the other parts of the job no no no i gave this i gave the spectrum 90 of code 100 of code 90 of end-to-end suite 100 of end-to-end suite new tasks are created for sweez eventually those get done as well but it's a long spectrum there but we're traversing the spectrum very quickly yeah um i do think it's funny that i i've seen a couple of podcasts you've done where um the host will be like ah but work has wrote the session about the continual learning thing and it always makes me crack up because you're like you know you've been an ai researcher for like 10 years and i'm sure there's like some uh feeling of like okay so a podcaster wrote an essay and like every interview i get asked about it you know the the truth of the truth of the matter is that we're all trying to figure this out together yeah right there there are some ways in which i'm able to see things that others aren't these days that probably has more to do with like i can see a bunch of stuff within anthropic and have to make a bunch of decisions than i have any great research insight that that others don't right i you know i'm running a 2500 person company like it's it's actually pretty hard for me to have have concrete research insight you know much harder than you know than it would have been you know 10 years ago or or you know or even two or three years ago um as we go towards a world of a full drop in remote worker replacement does a api pricing model still make the most sense and if not what is the correct way to price agi or serve agi yeah i mean i think there's going to be a bunch of different business models here sort of all at once that are going to be that are going to be experimented with um i i i actually do think that the the api um model is is more durable than many people think um one way i think about it is if the technology is kind of advancing quickly if it's advancing exponentially what that means is there's there's always kind of like a surface area of of kind of new use cases that have been developed in in the last uh in the last three months and any kind of product surface you put in place is always at risk of sort of becoming irrelevant right any given product surface probably makes sense for our you know a range of capabilities of the model right the the chatbot is already running into limitations of you know making it smarter doesn't really help the average consumer that much but i don't think that's a limitation of ai models i don't think that's evidence that you know the models are the models are good enough and they're they're you know them getting better doesn't matter to the economy it doesn't matter to that particular product um and and so i think the value of the api is the api always offers an opportunity you know very close to the bare metal to build on what the latest thing is um and so that you know there's there's there's kind of always going to be this you know this this kind of front of new startups and new ideas that weren't possible a few months ago and are possible because the model is advancing and and so i i actually i i kind of actually predict that we are it's going to exist alongside other models but we're always going to have the api business model because there's there's always going to be a need for a thousand different people to try experimenting with the model in different way and a hundred of them become startups and ten of them become big successful startups and you know two or three really end up being the the way that people use the model of a of a given generation so i basically think it's always going to exist at the same time i'm sure there's going to be other models as well like not every token that's output by the model is worth the same amount think about you know how how what is the value of the tokens that are like you know that the model outputs when someone you know call you know someone you know calls them up and says my mac isn't working or something you know the models like restart it right yeah um and like you know someone hasn't heard that before but like you know the model said that like 10 million times right um you know that that maybe that's worth like a dollar or a few cents or something um whereas if uh the model you know the model goes to you know one of the one of the pharmaceutical companies and it says oh you know this molecule you're developing you should take the aromatic ring from that end of the molecule and put it on that end of the molecule um and and you know if you do that wonderful things will happen um uh like like those tokens could be worth you know tens of millions of dollars right um uh so so i think we're definitely going to see business models that that recognize that you know at some point we're going to see you know pay for results or you you know in some in some form or we may see forms of compensation that are like labor um uh you know that that kind of work by the hour um i i i i you know i don't know i think i think i think because it's a new industry a lot of things are going to be tried and i you know i don't know what will turn out to be the right thing um what i find uh i i take your point that people will have to try things to figure out what is the best way to use this blob of intelligence but what i find striking is clawed code so i don't think in the history of startups there has been a single application that has been as hotly competed in as coding agents and um and and and the cloud code is a category leader here and that seems surprising to me like it doesn't seem intrinsically like anthropic had to build this i wonder if you have an accounting of why it had to be anthropic or why how anthropic ended up building an application in addition to the model underlying it yeah so it actually happened in a pretty simple way which is we had our own um you know we had our coding models which were good at coding and and you know around the beginning of 2025 i said i i think the time has come where you can have non-trivial acceleration of your own research um if you're an ai company by using these models and of course you know we you need an interface you need a harness to use them and so i encourage people internally and i didn't say this is one thing that you know that you have to use i i just said people should experiment with this and then you know this thing i you know i think it might have been originally called claude cli and then the name eventually got changed to claude code internally um was the thing that kind of everyone was using and it was seeing fast internal adoption and i looked at it and i said probably we should launch this externally right um uh you know it's it's seen such fast adoption within anthropic like you know like you know coding is a lot of what we do and and so you know we have a we have a audience of many many hundreds of people that's in some ways at least representative of the external audience so it looks like we already have product market fit let's launch this thing um and and then we launched it and and and i think you know just just the fact that we ourselves are kind of developing the model and we ourselves know what we most need to use the model i think it's it's kind of creating this feedback loop i see in the sense that you let's say a developer at anthropic is like ah it it'd be better if it was better at this x thing and then you bake that into the next model that you build that that's that's one version of it but but then there's just the ordinary product iteration of like you know we have a bunch of we have a bunch of coders within anthropic like we um you know they like use claude code every day and so we get fast feedback that was more important in the early days now of course there are millions of people using it um and so we get a bunch of external feedback as well but it's you know it's just great to be able to get you know kind of kind of uh um fast fast internal feedback you know i think this is the reason why we launched a coding model and you know didn't launch a pharmaceutical company right it's you know you know my background's in in my background's in in like biology but like we don't have any of the resources that are needed to launch a pharmaceutical company so there's been a ton of hype around open claw and i want to check it out for myself i've got a date coming up this weekend and i don't have anything planned yet so i gave open claw a mercury debit card i set a couple hundred dollar limit and i said surprise me okay so here's the mac mini it's on and besides having access to my mercury it's totally quarantined and i actually felt quite comfortable giving you an access to a debit card because mercury makes it super easy to set up guard rails i was able to customize permissions cap the spend and restrict the category of purchases i wanted to make sure the debit card worked so i asked open claw to just make a test transaction and decided to donate a couple bucks to wikipedia besides that i have no idea what's going to happen i will report back on the next episode about how it goes in the meantime if you want a personal banking solution that can accommodate all the different ways that people use their money even experimental ones like this one visit mercury.com personal mercury is a fintech company not an fdic insured bank banking services provided through choice financial group and column na members fdic you know she thinks we're getting coffee and walking around the neighborhood let me ask you about now um making ai go well um it seems like whatever vision we have about how ai goes well has to be compatible with two things one is the ability to build and run ais is diffusing extremely rapidly and two is that the population of ais the amount we have in their intelligence will also increase very rapidly and that means that lots of people will be able to build huge populations of misaligned ais or uh ais which are just like companies which are trying to increase their uh footprint or have weird psyches like sydney bing but now they're superhuman what is a vision for a world in which we have an equilibrium that is compatible with lots of different ais some of which are misaligned running around yeah yeah so i think you know in the adolescence of technology i was kind of you know skeptical of like the balance of power but i i think i was particularly skeptical of or the thing i was specifically skeptical of is you have like three or four of these companies like kind of all building models that are kind of dry you know sort of sort of um uh uh like derived from the like derived from the same thing and uh you know that that these would check each other or or even that kind of you know any number of them would would would uh would check each other like we might live in a offense dominant world where you know like one person or one ai model is like smart enough to do something that like causes damage for everything else um i think in the i mean in the short run we have a limited number of players now so we can start by within the limited number of players we uh you know we kind of you know we we need to put in place the you know the safeguards we need to make sure everyone does the right alignment work we need to make sure everyone has bio classifiers like you know those are those are kind of the immediate things we need to do i agree that you know that that doesn't solve the problem in the long run particularly if the ability of ai models to make other ai models proliferates then you know the the whole thing can kind of um you know it can become harder to solve you know i think i think in the long run we need some architecture of governance right some are some architecture of governance that preserves human freedom but but kind of also allows us to like you know govern the the very large number of kind of um you know uh uh uh human systems ai systems hybrid hybrid human human um you know hybrid hybrid human ai like you know companies or or like or like or like economic units so you know we're gonna need to think about like you know how do we how do we protect the world against you know bioterrorism how do we protect the world against like you know against like against like mirror life like you know probably probably we're gonna need to you know need some kind of like ai monitoring system that like mona you know kind of monitors for for all these things but then we need to build this in a way that like you know preserve civil liberties and like our constitutional rights so i think just just as is as is anything else like it's it's like a new security landscape with a new set of you know a new set of tools and a new set of vulnerabilities and i i think my worry is if we had a hundred years for this to happen all very slowly we'd get used to it you know like we've gotten used to like you know the presence of you know the presence of explosives in society or like the you know the presence of various um you know like new weapons or the you know the pre the presence of video cameras um we would get used to it over over over over over a hundred and we develop governance mechanisms we'd make our mistakes my my worry is just that this is happening all so fast and so i think maybe we need to do our thinking faster about how to make these governance mechanisms work yeah it seems like in a offense dominant world over the course of the next century so the idea is the ai is making the progress that would happen over the next century happen in some period of five to ten years but we would still need the same mechanisms or balance of power would be similarly intractable even if humans were the only game in town um and so i guess we have the advice of ai we it it fundamentally doesn't seem like a totally different ball game here if checks and balances were going to work they would work with humans as well if they aren't going to work they wouldn't work with the eyes as well um and so maybe this just dooms human checks and balances as well but yeah again again i think there's some way to i think there's some way to make this happen like it you know it just it just you know the governments of the world may have to work together to make it happen like you know we may have to you may have to talk to ais about kind of you know building societal structures in such a way that like these these defenses are possible i i don't know i mean this is so this is you know i don't want to say so far ahead in time but like so far ahead in technological ability that may happen over a short period of time that it's hard for us to anticipate it in advance um speaking of governments getting involved on december 26 the tennessee legislature introduced a bill which uh said quote um it would be an offense for a person to knowingly train artificial intelligence to provide emotional support including through open-ended conversations with the user and of course one of the things that claude attempts to do is be a thoughtful um a thoughtful friend thoughtful knowledgeable friend and in general it seems like we're going to have this patchwork of state laws a lot of the benefits that normal people could experience as a result of ai are going to be curtailed especially when we get into the kinds of things you discuss in machines of love and grace biological freedom mental health improvements etc etc it seems easy to imagine worlds in which these can whack them all the way by different laws um whereas bills like this don't seem to address the actual existential threats that you're concerned about so i'm curious about to understand in the context of things like this your anthropics position against the federal moratorium on state ai laws yes so i don't know there's there's many different things going on at once right i think i think that that i think that particular law is is dumb like you know i think it was it was clearly made by legislators who just probably had little idea what ai models could do and not do they're like ai models serving as that just sounds scary like i don't want i don't want that to happen so you know we're we're not we're not in favor of that right but but but you know that that wasn't the thing that was being voted on the thing that was being voted on is we're going to ban all state regulation of ai for 10 years with no apparent plan to to do any federal regulation of ai which would take congress to pass which is a very high bar um so you know the idea that we'd ban states from doing anything for 10 years and people said they had a plan for federal government but you know there was no actual there was no proposal on the table there was no actual attempt um given the serious dangers that i lay out in adolescence of technology around things like the you know kind of biological weapons and bioterrorism autonomy risk and the timelines we've been talking about like 10 years is an eternity like that's that's a that's a i i think that's a crazy thing to do so if if that's the choice if that's what you force us to choose then then we're gonna we're gonna choose not to have that moratorium and you know i i think the the the benefits of that position exceed the costs but it's it's not a perfect position if that's the choice now i think the thing that we should do the thing that i would support is the federal government should step in not saying states you can't regulate but here's what we're going to do and and states you can't differ from this right like i think preemption is fine in the sense of saying that federal government says here's our standard this applies to everyone states can't do something different that would be something i would support if it would be done in the right way what um but but this idea of states you can't do anything and we're not doing anything either that that struck that struck us as you know very much not making sense and i think will not age well it's already starting to not age well with with all the um backlash that that you've seen now in terms of in terms of what we would want i mean you know the things we've talked about are are starting with transparency standards um uh uh you know in order to monitor some of these autonomy risks and bioterrorism risks as the risks become more serious um as we as we get more evidence for them then i think we could be more aggressive in some targeted ways and say hey ai bioterrorism is really a threat let's let's pass a law that kind of forces people to have classifiers and i could even imagine it depends it depends how serious the threat it ends up being we don't know for sure then we need to pursue this in an intellectually honest way where we say ahead of time the risk has not emerged yet but i could certainly imagine with the pace that things are going that you know i could imagine a world where later this year we say hey this this ai bioterrorism stuff is really serious we should do something about it we should put it in a federal we should you know put it in a federal standard and if the federal government won't act we should put it in a state standard i could totally see that i i'm concerned about a world where if you just consider the the pace of progress you're expecting the life cycle of of legislation you know the the benefits are as you say because of diffusion lag the benefits are slow enough that i really do think this patchwork of on the current trajectory this patchwork of state laws would prohibit i mean having an emotional chatbot friend is something that freaks people out then just imagine the kinds of actual benefits from ai we want normal people to be able to experience from improvements in health and health span and improvements in mental health and so forth whereas at the same time uh it seems like you think the dangers are already on the horizon and i just don't see that much um it seems like would be especially injurious to the benefits of ai uh as compared to the the dangers of ai so that that's maybe the where the cost benefit makes less sense to me so so so there's a few things here right i mean people talk about there being thousands of these state laws first of all the vast mass majority of them do not pass um and you know the the the you know the world works a certain way in theory but like just because the law has been passed doesn't mean it's really enforced right the people the people you know implementing it may be like oh my god this is stupid it would mean shutting off like you know everything that's ever been built and everything that's ever been built in tennessee so you know very often laws are interpreted in like you know a way that makes them that that makes them not as dangerous or not as harmful on on the same side of course you have to worry if you're passing a law to stop a bad thing you had this you had this problem as well yeah um uh look my my look i mean my basic view is you know if if if you know we could decide you know what laws were passed and how things were done which you know we're only one small input input into that you know i would deregulate a lot of the stuff around the health benefits of ai um i think you know i i don't worry as much about the like the the the kind of chatbot laws i actually worry more about the drug approval process where i think ai models are going to greatly accelerate um the rate at which we discover drugs and just the the pipeline will get jammed up like the pipeline will not be prepared to like process all all the stuff that's going through it so um you know i i i think i think reform of the regulatory process to buy us more towards we have a lot of things coming where the safety and the efficacy is actually going to be really crisp and clear like i mean a beautiful thing really really crisp and clear and like really really effective but you know and and maybe we don't need all this all this um uh uh like um all this superstructure around it that was designed around an era of drugs that barely work and often have serious side effects um but at the same time i think we should be ramping up quite significantly the um uh you know this this kind of safety and security legislation and you know like i've said um you know starting with transparency is is my view of trying not to hamper the industry right trying to find the right balance i'm worried about it some people criticize my essay for saying that's too slow the dangers of ai will come too soon if we do that well basically i kind of think like the last six months and maybe the next few months are going to be about transparency and then if these if these risks emerge when we're more certain of them which i think we might be as soon as as later this year then i think we need to act very fast in the areas that we've actually seen the risk like i think the only way to do this is to be nimble now the legislative process is normally not nimble but we we need to emphasize to everyone involved the urgency of this that's why i'm sending this message of urgency right that's why i wrote adolescence of technology i wanted policy makers to read it i wanted economists to read it i want national security professionals to read it you know i want decision makers to read it so that they have some hope of acting faster than they would have otherwise is there anything you can do or advocate that would make it more certain that the benefits of ai are um are better instantiated where i feel like you have worked with legislatures to be like okay we're going to prevent bioterrorism here away we're going to increase conspiracy we're going to increase whistleblower protection and i just think by default the actual ben like the things we're looking forward to here it just seems very easy they seem very fragile to uh different kinds of moral panics or political economy problems yeah i don't actually so so i don't actually agree that much in the developed world i feel like you know in the developed world like markets function pretty well and when there's when there's like a lot of money to be made on something and it's clearly the best available alternative it's actually hard for the regulatory system to stop it you know we're we're seeing that in ai itself right i you know like a thing i've been trying to fight for is export controls on chips to china right and like that's in the national security interests of the u.s like you know that's like square within the you know the the policy beliefs of you know every almost everyone in congress of both parties but and you know i think the case is very clear the counter arguments against it are i'll politely call them fishy um uh and yet it doesn't happen and we sell the chips because there's there's so much money there's so much money riding on it um and you know the the that money wants to be made and and in that case in my opinion that's a bad thing um and but but it also it also applies when when it's a good thing and and so i i don't think that if we're talking about drugs and benefits of the technology i i i am not as worried about those benefits being hampered in the developed world i am a little worried about them going too slow and i as i said i do think we should work to speed the approval process in the fda i do think we should fight against these chatbot bills that you're describing right described individually i'm against them i think they're stupid um but i actually think the bigger worry is a developing world um where we don't have functioning markets where um you know we often can't build on the technology that that we've had i worry more that those folks will get left behind and i worry that even if the cures are developed you know maybe there's someone in rural mississippi who who doesn't get it as well right that's a that's a that's a kind of smaller version of the thing the concern we have in the in the developing world and so the things we've been doing are you know you know we work with you know we work with you know philanthropists right you know we work with folks um who you know who you know deliver you know medicine and health interventions to you know to to developing world the sub-saharan africa you know india latin america you know uh you know other other developing parts of the world that's the thing i think that won't happen on its own you mentioned expert controls yeah why can't us and china both have a country of geniuses why why can't you know why won't it happen or why no like why shouldn't it happen why shouldn't it happen um you know i think i think if this does happen um you know then then we kind of have a well we could have a few situations if we have like an offense dominant situation we could have a situation like nuclear weapons but like more dangerous right where it's like um you know kind of kind of either side could could easily destroy everything um we could also have a world where it's kind of it's unstable like the nuclear equilibrium is stable right because it's you know it's like deterrence but let's say there were uncertainty about like if the two ais fought which ai would win um that could create instability right you often have conflict when the two sides have a different assessment of their likelihood of winning right if one side is like oh yeah there's a 90 chance i'll win and the other side's like there's a 90 chance i'll win then then then a fight is much more likely um they can't both be right but they can both think that but this is like a fully general argument against the diffusion of ai technology which it may which is that's the implication of this world let me let let me just go on because i think we will get diffusion eventually the other concern i have is that people the governments will oppress their own people with ai and and and so um you know i'm i'm just i'm worried about some world where you have a country that's already uh you know kind of uh uh uh you know uh uh you know there's there's a government that kind of kind of already um you know is is kind of kind of building a you know a tech high-tech authoritarian state um and to be clear this is about the government this is not about the people like people we need to find a way for people everywhere to benefit um my worry here is about governments um so yeah my you know my my worry is if the world gets carved up into two pieces one of those two pieces could be authoritarian or totalitarian in a way that's very difficult to displace um now will will governments eventually get powerful ai and and you know there's risk of authoritarianism yes will governments eventually get powerful ai and there's risk of um uh you know of of kind of bad bad bad equilibria yes i think both things but the initial conditions matter right you know at some point we're neat we're going to need to set up the rules of the road i'm not saying that one country either the united states or a coalition of democracies which i think would be a better setup although it requires more international cooperation than we currently seem to want to make um but you know i don't i don't think a coalition of democracies or or certainly one country should just say these are the rules of the road there's going to be some negotiation right the world is going to have to grapple with this and what i would like is that the the the you know the democratic nations of the world those with you know who are close whose governments have represent closer to pro-human values are are holding the stronger hand then have have have more leverage when the rules of the road are set and and so i'm very concerned about that initial condition um i was really listening to an interview from three years ago and one of the ways it aged poorly is that i kept asking questions assuming there's going to be some key fulcrum moment two to three years from now when in fact being that far out it just seems like progress continues ai improves ai is more diffused and people use it for more things it seems like you're imagining a world in the future where the countries get together and here's the rules of the road and here's the leverage we have here's the leverage you have when it seems like on current trajectory everybody will have more ai um some of that ai will be used by authoritarian countries some of that within the authoritarian countries will be raised by private actors versus state actors it's not clear who will benefit more it's always unpredictable to tell in advance you know it seems like the internet privileged authoritarian countries more than you would have expected um and maybe the ai will be the opposite way around um so i i want to better understand what you're imagining here yeah yeah so so just to be precise about it i think the exponential of the underlying technology will continue as it has before right the models get smarter and smarter even when they get to country of geniuses in a data center you know i i think you can continue to make the model smarter there's a question of like getting diminishing returns on their value in the world right how much does it matter after you've already solved human biology or you know you know at some point you can do harder math you can do more abstruse math problems but nothing after that matters but putting that aside i do think the the exponential will continue but there will be certain distinguished points on the exponential and companies individuals countries will reach those points at different times um and and so you know there's there's you know could there be some you know you know i talk about is a nuclear deterrent still in adolescence of technology is a nuclear deterrent still stable uh in the world of of a i don't know but that's that's an example of like one thing we've taken for granted that like the technology could reach such a level that it's no longer like you know we can no longer be certain of it at least um uh you know think of think of others you know they're they're they're you know they're they're kind of points where if you if you reach a certain point you maybe you have offensive cyber dominance and like every every computer system is transparent to you after that um unless the other side has it has a kind of equivalent defense so i don't know what the critical moment is or if there's a single critical moment but i think there will be either a critical moment a small number of critical moments or some critical window where it's like ai is ai confers some large advantage from the perspective of national security and one country or coalition has reached it before others that that you know that that you know i'm not advocating that they're just like okay we're in charge now that's not that's not how that's not how i think about it you know that there's always the the other side is catching up there's extreme actions you're not willing to take and and it's not right to take you know to take complete um to take complete control anyway but but at the point that that happens i think people are going to understand that the world has changed and there there's going to be some negotiation implicit or implicit about what what is the what is the post ai world order look like and and i think my interest is in you know making that negotiation be one in which you know classical liberal democracy has you know has a strong hand well i would understand what that better means because you say in the essay quote autocracy is simply not a form of government that people can accept in the post powerful ai age and that sounds like you're saying the ccp as an institution cannot exist after we get agi um and that seems like a like a very strong demand and it seems to imply a world where the leading lab or the leading country will be able to and by that language should get to determine how the world is governed or what kinds of governments are allowed and not allowed yeah so when when i um i believe that paragraph was i think i said something like you could take it even further and say x so i wasn't i wasn't necessarily endorsing that that that i wasn't necessarily endorsing that view i you know i was saying like here's if first you know here's a weaker thing that i believe but you know i think i you know i think i said you know we have to worry a lot about authoritarians and you know we should try and you know kind of kind of check them and limit their power like you could take this kind of further much more interventionist view that says like authoritarian countries with ai are these you know the the you know these kind of self-fulfilling cycles that you can't that are very hard to displace and so you just need to get rid of them from from the beginning that that has exactly all the problems you say which is you know you know if you were to make a commitment to overthrowing every authoritarian country i mean then they would take a bunch of actions now that like you know that that could could lead to instability so that that may or you know that that just that just may not be possible but the point i was making that i do endorse is that it is it is quite possible that you know today you know the view or at least my view or the view in most the western world is is democracy is a better form of government than authoritarianism but it's not like if a country's authoritarian we don't react the way we reacted if they committed a genocide or something right and and i'm i guess what i'm saying is i'm a little worried that in the age of agi authoritarianism will have a different meaning it will be a graver thing um and and we have to decide one way or another how how how to deal with that and the interventionist view is one possible view i was exploring such views um you know uh it may end up being the right view it may end up being too extreme to be the right view but i do have hope and one piece of hope i have is there there is we have seen that as new technologies are invented forms of government become obsolete i i mentioned this in adolescence of technology where i said you know like feudalism was basically you know like a form of government right and and then when when we invented industrialization feudalism was no longer sustainable no longer made sense why is that hope why couldn't that imply that democracy is no longer going to be well a competitive system it could right it could go it could go either way right but but i actually so i these problems with authoritarianism right that the problems of authoritarianism get deeper i just i wonder if that's an indicator of other problems that authoritarianism will have right in other words people become because authoritarianism becomes worse people are more afraid of authoritarianism they work harder to stop it it's it's more of a kid like you have to think in terms of total equilibrium right um i just wonder if it will motivate new ways of thinking about with with with the new technology how to preserve and protect freedom and and uh even more optimistically will it lead to a collective reckoning and you know a a a kind of a more emphatic realization of how important some of the things we take as individual rights are right a more emphatic realization that we just we really can't give these away there's there we've seen there's no other way to live that actually works um i i i i am actually i am actually hopeful that i i guess one way to say it it sounds too idealistic but i actually believe it could be the case is that is that dictatorships become morally obsolete they become morally unworkable forms of government um and that and that and that the the the the the crisis that that creates is is is sufficient to force us to find another way um i think there is genuinely a tough question here which i'm not sure how you resolve and we've had to come out one way or another on it through history right so with china in the 70s and 80s we decided even though it's an authoritarian system we will engage with it i think in retrospect that was the right call because in a state authoritarian system but a billion plus people are much wealthier and better off than they would have otherwise been um and it's not clear that it would have stopped being an authoritarian country otherwise you can just look at north korea uh as an example of that right and i don't know if that takes that much that much intelligence to remain an authoritarian country that continues to coalesce its own power as you can just imagine a north korea with an ai that's much worse than everybody else's but still enough to keep power and and and so in general it seems like should we just have this attitude of the benefits of ai will in the form of all these empowerments of humanity and health and so forth will be big and and historically we have decided it's good to spread the benefits of technology widely even with even to people whose governments are authoritarian and i think i guess it is a tough question how to think about it with ai but um historically we have said yes this is a positive sum world and it's still worth diffusing technology yeah so so there are a number of choices we have i you know i think framing this as a kind of government to government decision and you know in in national security terms that's like one lens but there are a lot of other lenses like you could imagine a world where you know we produce all these cures to diseases and like the you know the the cures to diseases are fine to sell to authoritarian countries the data centers just aren't right the chips and the data centers just aren't um and and the ai industry itself um uh you know like like another possibility is and and i think folks should think about this like you know could there be developments we can make either that naturally happened as a result of ai or that we could make happen by building technology on ai could we create an equilibrium where where it becomes infeasible for authoritarian countries to deny their people kind of private use of the benefits of the technology um uh you know are there are there are there are there equilibria where we can kind of give everyone an authoritarian country their own ai model that kind of you know you know like defends themselves from surveillance and there isn't a way for the authoritarian country to like crack crack down on this while while retaining power i don't know that that sounds to me like if that went far enough it would be it would be a reason why authoritarian countries would disintegrate from the inside um but but maybe there's a middle world where like there's an equilibrium where if they want to hold on to power the authoritarians can't deny kind of individualized access access to the technology but i actually do have a hope for the for the um for the for the more radical version which is you know is it possible that the technology might inherently have properties or that by building on it in certain ways we could create properties um that that that have this kind of dissolving effect on authoritarian structures now we we hoped originally right we think about back to the beginning of the obama administration we thought originally that that you know social media and and the internet would have that property turns out not to but but i i don't know what what if we could uh what if we could try again with with the knowledge of how many things could go wrong and that this is a different technology i don't know that it would work but it's worth a try yeah i i think it's just it's very unpredictable like there's first principles reasons why authoritarianism it's all it's all very unpredictable i i don't think i mean we got it we we just got to we kind of we got to recognize the problem and then we got to come up with 10 things we can try and we got to try those and then assess whether they're working or which ones are working if any and then try new ones if the old ones aren't but i guess what that nets out to today is you say we will not sell data centers or sorry chips and then the ability to make chips to china and so in some sense you are denying there would be some benefits to that's right the chinese economy chinese people etc because we're doing that and then there'd also be benefits to the american economy because it's a positive sum world we could trade they could have their country data centers doing one thing we could have ours doing another and already we you're saying it's not worth that positive sum uh stipend to empower those countries what i would say is that you know we are we are about to be in a world where growth and economic value will come very easily if right if we're able to build these powerful ai models growth and economic value will come very easily what will not come easily is distribution of benefits distribution of wealth political freedom um you know these are the things that are going to be hard to achieve and so when i think about policy i think i think that the technology in the market will deliver all the fundamental benefits you know almost almost faster than we can take them um uh and and that these questions about about distribution and political freedom and rights are are are the ones that that will actually matter and that policy should focus on okay so speaking of distribution as you're mentioning we have developing countries and um in many cases catch-up growth has been weaker than we would have hoped for but when catch-up growth does happen it's fundamentally because they have underutilized labor and we can bring the capital and know-how from developed countries to these countries and then they can grow quite rapidly yes obviously in a world where labor is no longer the constraining factor this mechanism no longer works it's just the hope basically to rely on philanthropy from the people who immediately get wealthy from ai or from the countries that get wealthy from ai what is the i mean i mean philanthropy should obviously play some role as it has the the you know as it has as in the past but i think growth is always growth is always better and stronger if we can make it endogenous yeah so you know what are the relevant industries in like in like in like in like in like an ai driven world look there's lots of stuff you know like there's you know i said i said we shouldn't build data centers in china but there's no reason we shouldn't build data centers in africa right um in fact i think it'd be great to build data centers in africa um you know as long as they're not owned by china we should we should build we should build data centers in africa i think that's a that's that's i think that's a great thing to do um you know we should also build you know there's no reason we can't build you know a pharmaceutical industry that's like ai driven like you know the the if ai is accelerating accelerating drug discovery then you know there will be a bunch of biotech startups like let's make sure some of those happen in the developing world and certainly during the transition i mean we can talk about the point where humans have no role but but humans will have still have some role in starting up these companies and supervising supervising the ai models so let's make sure some of those humans are humans in the developing world so that fast growth can happen there as well you guys recently announced quad is going to have a constitution that's aligned to a set of values and not necessarily just the end user and there's a world you can imagine where if it is aligned to the end user it preserves the balance of power we have in the world today because everybody gets to have their own ai that's advocating for them and so the ratio of bad actors to good actors stays constant it seems to work out for our world today um why is it better not to do that but to have a specific set of values that the ai should carry forward uh yeah so i'm not sure i'd quite draw the distinction in that way um there there may be two relevant distinctions here which are i think you're talking about a mix of the two like one is should we give the model a set of instructions about do this and versus don't do this and the other you know versus should we give the model a set of principles for you know for kind of how to act um and and and there it's it's you know it's it you know it's it's just it's kind of purely a practical and empirical thing that we've observed that by teaching the model principles getting it to learn from principles its behavior is more consistent it's easier to cover edge cases and the model is more likely to do what people want it to do in other words if you know if you're like you know don't tell people how to hot wire a car don't speak in korean don't you know you know just you know if you give it a list of rules it doesn't really understand the rules and it's kind of hard to generalize from them um you know if it's just kind of a like you know list of do's and don'ts whereas if you give it principles and then you know it has some hard guard rails like don't make biological weapons but overall you're trying to understand what it should be aiming to do how it should be aiming to operate so just from a practical perspective that turns out to be just a more effective way to train the model that's one piece of it so that you know it's the kind of rules versus principles trade-off then there's another thing you're talking about which is kind of like the courage ability versus um like you know i would say kind of intrinsic motivation trade-off which is like how much should the model be a kind of i don't know like a a skin suit or something where you know you know you know you just kind of you know it just kind of directly follows the instructions that are given to it by whoever is giving it those instructions um versus how much should the model have an inherent set of values and and go off and do things on its own um and and and and and there i i would actually say everything about the model is actually closer to the direction of of like you know it should mostly do what people want it should mostly follow these we're not trying to build something that like you know goes off and runs the world on its own we're actually pretty far on the corrigible side now now what we do say is there are certain things that the model won't do right that it's like you know that that that i think we say it in various ways in the constitution that under normal circumstances if someone asks the model to do a task you should do that task that that should be the default um but if you've asked it to do something dangerous or if you've you know if you've um asked it to um you know uh uh to kind of harm someone else um then the model is unwilling to do that so i i actually think of it as like a mostly a mostly corrigible model that has some limits but those limits are based on principles yeah i mean then the fundamental question is how are those principles determined and this is not a special question for anthropic this would be a question for any company but um uh because you have been the ones to actually write down the principles i get to ask you this question normally a constitution is like you write it down it's set in stone and there's a process of updating it and changing it and so forth in this case it seems like a document that people in anthropic write that can be changed at any time that guides the behavior of systems are going to be the basis of a lot of economic activity what is the how do you think about how those principles should be set yes um so i think there's there's two there's maybe three three kind of sizes of loop here right three ways to iterate one is you can iterate we iterate within anthropic we train the model we're not happy with it and we kind of change the constitution and i think that's good to do um and you know putting out publicly you know making updates to the constitution every once in a while saying here's a new constitution right i think that's good to do because people can comment on it the second level of loop is different companies will have different constitutions um and you know i think it's useful for like anthropic puts out a constitution and you know the gemini model puts out a constitution and you know other companies put out a constitution and then they can kind of look at them compare outside observers can critique and say this this i like this one this thing from this constitution and this thing for that constitution and and then kind of that that creates some kind of you know soft incentive and feedback for all the companies to like take the best of each elements and improve then i think there's a third loop which is you know society beyond the ai companies and beyond just those who kind of you know who who comment on the constitutions without hard power and and there you know we've done some experiments like you know a couple years ago we did an experiment with i think it was called the collective intelligence project to like um you know to to basically pull people and ask them what should be in our ai constitution um uh and and you know i think at the time we incorporated some of those changes and so you could imagine with the new approach we've taken to the constitution doing something like that it's a little harder because it's like that was actually an easier approach to take when the constitution was like a list of do's and don'ts um at the level of principles it has to have a certain amount of coherence um but but you could you could still imagine getting views from a wide variety of people and i think you could also imagine and this is like a crazy idea but hey you know this whole interview is about crazy ideas right so um uh you know you could even imagine systems of of kind of representative government having having input right like you know i wouldn't i wouldn't do this today because the legislative process is so slow like this is exactly why i think we should be careful about the legislative process and ai regulation but there's no reason you couldn't in principle say like you know all ai you know all ai models have to have a constitution that starts with like these things and then like you can append you can append other things after it but like there has to be this special section that like takes precedence i wouldn't do that that's too rigid that that sounds um you know that that that that sounds kind of overly prescriptive in a way that i think overly aggressive legislation is but like that is the thing you could you know like like that is that that is the thing you could try to do is is there some much less heavy-handed version of that maybe i really like control loop too um where obviously this is not how constitutions of actual governments do or should work where there's not this vague sense in which the supreme court will feel out how people are feeling and what are the vibes and then update the of the constitution accordingly so there's yeah with actual governments there's a more procedural process yeah exactly but you actually have a vision of competition between constitutions which is actually very reminiscent of how um some libertarian charter cities people used to talk about what an archipelago of different kinds of governments would look like and then there would be selection among them of who could operate the most effectively yeah in which place people would be the happiest and in a sense you're actually yeah there's this vision i'm i'm kind of recreating that yeah yeah like the utopia of archipelago you know again you know i think i think that vision has has you know if things to recommend it and things that things that things that will kind of kind of go wrong with it you know i think i think it's a i think it's an interesting in some ways compelling vision but also things will go wrong with it that you hadn't that you hadn't imagined so you know i i like loop two as well but i i i feel like the whole thing has got to be some some mix of loops one two and three and it's it's a matter of the proportions right i i think that's got to be the the answer um when somebody eventually writes the equivalent of the making of the atomic bomb for this era what is the thing that will be hardest to glean from the historical record that they're most likely to miss i think a few things one is at every moment of this exponential the extent to which the world outside it didn't understand it this is this is a bias that's often present in history where anything that actually happened looks inevitable in retrospect and and so you know i i think when people when people look back it will be hard for them to put themselves in the place of people who were actually making a bet on this thing to happen that that wasn't inevitable that we had these arguments like the arguments that you know that i make for scaling or that continual learning will be solved um uh uh you know that that you know some of us internally in our heads put a high probability on this happening but but it's like there's there's a world outside us that's not that's not acting on that it's not kind of not acting on that at all um uh and and and i think i think the the weirdness of it um i i i think unfortunately like the insularity of it like you know if if we're one year or two years away from it happening like the average person on the street has no idea and that's one of the things i'm trying to change like with the memos with talking to policy makers but like i don't know i think i i think that's just a that's just like a crazy that's just like a crazy thing yeah um finally i would say and and this probably applies to almost all historical moments of crisis um how absolutely fast it was happening how everything was happening all at once and so decisions that you might think you know were kind of carefully calculated well actually you have to make that decision and then you have to make 30 other decisions on the on the same day because it's all happening so fast and and you don't even know which decisions are going to turn out to be consequential so you know one of my one of my i guess worries although it's also an insight into into you know into kind of what's happening is that you know some very critical decision will be will be some decision that you know someone just comes into my office and is like dario you have two minutes like you know should we should we do you know should we do thing thing a or thing b on this like you know someone gives me this random you know half page half page memo and is like should we should we do a or b and i'm like i don't know i have to eat lunch let's do b and and you know that ends up being the most consequential thing ever so final question uh it seems like you have there's not tech ceos who are usually writing 50 page memos every few months and it seems like you have managed to build a rule for yourself and a company around you which is compatible with this more intellectual type role as ceo and i want to understand how you construct that and how like how does that work to be you just go away for a couple weeks and then you tell your company this is the memo like here's what we're doing it's also reported you write a bunch of these internally yeah so i mean for this particular one you know i wrote it over winter break um uh so there was the time you know and i was having a hard time finding the time to actually find it to actually write it but i actually think about this in a broader way um i actually think it relates to the culture of the company so i probably spend a third maybe 40 of my time making sure the culture of anthropic is good as anthropic has gotten larger it's it's gotten harder to just you know get involved in like you know directly involved in like the training of the models the launch of the models the building of the products like it's 2500 people it's like you know there's just you know i have certain instincts but like there's only you know i i it's very difficult to get in to get to get involved in every single detail you know i like i i try as much as possible but one thing that's very leveraged is making sure anthropic is a good place to work people like working there everyone thinks of themselves as team members everyone works together instead of against each other and you know we've seen as some of the other ai companies have grown without naming any names you know we're starting to see decoherence and people fighting each other and you know i would argue there was even a lot of that from the beginning but but you know that it's it's gotten worse but i i think we've done an extraordinarily good job even if not perfect of holding the company together making everyone feel the mission that we're sincere about the mission and that you know everyone has faith that everyone else there is working for the right reason that we're a team that people aren't trying to get ahead at each other's expense or backstab each other which again i think happens a lot at some of the other places um and and how do you make that the case i mean it's a lot of things you know it's me it's it's it's daniela who you know runs the company day to day it's the co-founders it's the other people we hire it's the environment we try to create but i think an important thing in the culture is i some and just you know the the you know the other leaders as well but especially me have to articulate what the company is about why it's doing what it's doing what its strategy is what its values are what its mission is and what it stands for and um you know when you get to 2500 people you can't do that person by person you have to write or you have to speak to the whole company this is why i get up in front of the whole company every two weeks and speak for an hour it's actually i mean i wouldn't say i write essays internally i do two things one i write this thing called dvq dario vision quest um uh i wasn't the one who named it that that's the name it received and it's one of these names that i kind of i tried to fight it because it made it sound like i was like going off and smoking peyote or something um uh but but the name just stuck um so i get up in front of the company every two weeks i have like a three or four page document and i just kind of talk through like three or four different topics about what's going on internally the you know the the models we're producing the products the outside industry the world as a whole as it relates to ai and geopolitically in general you know just some mix of that and i just go through very very honestly i just go through and i just i just say you know this this is what i'm thinking this is what anthropic leadership is thinking and then i answer questions and and that direct connection i think has a lot of value that is hard to achieve when you're passing things down the chain you know six six levels deep um uh and you know a large fraction of the company comes comes to attend either either in person or um either in person or virtually and it you know it really means that you can communicate a lot and then the other thing i do is i just you know i have a channel in slack where i just write a bunch of things and comment a lot um and often that's in response to you know just things i'm seeing at the company or questions people ask or like you know we do internal surveys and there are things people are concerned about and so i'll write them up and i'm like i'm you know i'm i'm i'm just i'm very honest about these things you know i just i just say them very directly and the point is to get a reputation of telling the company the truth about what's happening to call things what they are to acknowledge problems to avoid the sort of corpo speak the kind of defensive communication that often is necessary in public because you know the world is very large and full of people who are you know interpreting things in bad faith um but you know if you have a company of people who you trust and we try to hire people that we trust then then you know you can you can you can you know you can you can really just be entirely unfiltered um and uh you know i think i think that's an enormous strength of the company it makes it a better place to work it makes people more you know more of the sum of their parts and increases likelihood that we accomplish the mission because everyone is on the same page about the mission and everyone is debating and discussing how best to accomplish the mission well in lieu of an external dario vision quest we have this interview this this interview is a little like that uh this isn't fun dario thanks for doing it yeah thank you dhwarkash hey everybody i hope you enjoyed that episode if you did the most helpful thing you can do is just share it with other people who you think might enjoy it it's also helpful if you leave a rating or a comment on whatever platform you're listening on if you're interested in sponsoring the podcast you can reach out at dhwarkash.com advertise otherwise i'll see you on the next one",
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 1.84,
      "text": "So we talked three years ago."
    },
    {
      "id": 1,
      "start": 2.14,
      "end": 5.24,
      "text": "I'm curious, in your view, what has been the biggest update of the last three years?"
    },
    {
      "id": 2,
      "start": 5.34,
      "end": 8.1,
      "text": "What has been the biggest difference between what it felt like last three years versus now?"
    },
    {
      "id": 3,
      "start": 8.28,
      "end": 13.82,
      "text": "Yeah, I would say actually the underlying technology, like the exponential of the technology,"
    },
    {
      "id": 4,
      "start": 14.44,
      "end": 19.6,
      "text": "has gone, broadly speaking, I would say, about as I expected it to go."
    },
    {
      "id": 5,
      "start": 19.68,
      "end": 22.12,
      "text": "I mean, there's like plus or minus, you know, a couple."
    },
    {
      "id": 6,
      "start": 22.56,
      "end": 24.36,
      "text": "There's plus or minus a year or two here."
    },
    {
      "id": 7,
      "start": 24.46,
      "end": 25.98,
      "text": "There's plus or minus a year or two there."
    },
    {
      "id": 8,
      "start": 25.98,
      "end": 29.68,
      "text": "I don't know that I would have predicted the specific direction of code."
    },
    {
      "id": 9,
      "start": 30.380000000000003,
      "end": 39.04,
      "text": "But actually, when I look at the exponential, it is roughly what I expected in terms of the march of the models from like, you know,"
    },
    {
      "id": 10,
      "start": 39.1,
      "end": 44.620000000000005,
      "text": "smart high school student to smart college student to like, you know, beginning to do Ph.D. and professional stuff."
    },
    {
      "id": 11,
      "start": 44.68,
      "end": 47.019999999999996,
      "text": "And in the case of code, reaching beyond that."
    },
    {
      "id": 12,
      "start": 47.08,
      "end": 48.86,
      "text": "So, you know, the frontier is a little bit uneven."
    },
    {
      "id": 13,
      "start": 49.1,
      "end": 50.58,
      "text": "It's roughly what I expected."
    },
    {
      "id": 14,
      "start": 50.980000000000004,
      "end": 53.6,
      "text": "I will tell you, though, what the most surprising thing has been."
    },
    {
      "id": 15,
      "start": 53.6,
      "end": 61.6,
      "text": "The most surprising thing has been the lack of public recognition of how close we are to the end of the exponential."
    },
    {
      "id": 16,
      "start": 61.94,
      "end": 69.6,
      "text": "To me, it is absolutely wild that, you know, you have people, you know, within the bubble and outside the bubble, you know,"
    },
    {
      "id": 17,
      "start": 69.64,
      "end": 76.44,
      "text": "but you have people talking about these, you know, just the same tired old hot button political issues."
    },
    {
      "id": 18,
      "start": 76.44,
      "end": 81.16,
      "text": "And like, you know, around us, we're like near the end of the exponential."
    },
    {
      "id": 19,
      "start": 81.7,
      "end": 89.5,
      "text": "I want to understand what that exponential looks like right now, because the first question I asked you when we recorded three years ago was, you know, what's up with scaling?"
    },
    {
      "id": 20,
      "start": 89.67999999999999,
      "end": 90.4,
      "text": "How does it work?"
    },
    {
      "id": 21,
      "start": 90.4,
      "end": 96.56,
      "text": "I have a similar question now, but I feel like it's a more complicated question, because at least from the public's point of view."
    },
    {
      "id": 22,
      "start": 96.9,
      "end": 97.2,
      "text": "Yes."
    },
    {
      "id": 23,
      "start": 97.30000000000001,
      "end": 104.34,
      "text": "Three years ago, there were these, you know, well-known public trends where across many orders of magnitude of compute, you could see how the loss improves."
    },
    {
      "id": 24,
      "start": 104.58000000000001,
      "end": 109.36000000000001,
      "text": "And now we have RL scaling and there's no publicly known scaling law for it."
    },
    {
      "id": 25,
      "start": 109.62,
      "end": 113.98,
      "text": "It's not even clear what exactly the story is of is this supposed to be teaching the model skills?"
    },
    {
      "id": 26,
      "start": 114.02000000000001,
      "end": 115.18,
      "text": "Is this supposed to be teaching meta learning?"
    },
    {
      "id": 27,
      "start": 115.18,
      "end": 118.72000000000001,
      "text": "What is the scaling hypothesis at this point?"
    },
    {
      "id": 28,
      "start": 118.94000000000001,
      "end": 119.04,
      "text": "Yeah."
    },
    {
      "id": 29,
      "start": 119.22000000000001,
      "end": 125.10000000000001,
      "text": "So I have actually the same hypothesis that I had even all the way back in 2017."
    },
    {
      "id": 30,
      "start": 125.4,
      "end": 131.26000000000002,
      "text": "So in 2017, I think I talked about it last time, but I wrote a doc called the Big Blob of Compute Hypothesis."
    },
    {
      "id": 31,
      "start": 131.88,
      "end": 135.46,
      "text": "And, you know, it wasn't about the scaling of language models in particular."
    },
    {
      "id": 32,
      "start": 135.62,
      "end": 139.3,
      "text": "When I wrote it, GPT-1 had just come out, right?"
    },
    {
      "id": 33,
      "start": 139.34,
      "end": 141.74,
      "text": "So that was, you know, one among many things, right?"
    },
    {
      "id": 34,
      "start": 141.74,
      "end": 144.3,
      "text": "There was back in those days, there was robotics."
    },
    {
      "id": 35,
      "start": 144.3,
      "end": 147.78,
      "text": "People tried to work on reasoning as a separate thing from language models."
    },
    {
      "id": 36,
      "start": 147.94,
      "end": 153.0,
      "text": "There was scaling of the kind of RL that happened, you know, kind of happened in AlphaGo."
    },
    {
      "id": 37,
      "start": 153.48000000000002,
      "end": 156.44,
      "text": "And, you know, that happened at Dota at OpenAI."
    },
    {
      "id": 38,
      "start": 157.0,
      "end": 161.58,
      "text": "And, you know, people remember StarCraft at DeepMind, you know, the AlphaStar."
    },
    {
      "id": 39,
      "start": 161.58,
      "end": 165.48000000000002,
      "text": "So it was written as a more general document."
    },
    {
      "id": 40,
      "start": 165.48000000000002,
      "end": 168.70000000000002,
      "text": "And the specific thing I said was the following that."
    },
    {
      "id": 41,
      "start": 168.9,
      "end": 173.24,
      "text": "And, you know, it's very, you know, Rich Sutton put out the bitter lesson a couple of years later."
    },
    {
      "id": 42,
      "start": 174.08,
      "end": 177.02,
      "text": "But, you know, the hypothesis is basically the same."
    },
    {
      "id": 43,
      "start": 177.02,
      "end": 186.88,
      "text": "So what it says is all the cleverness, all the techniques, all the kind of we need a new method to do something like that doesn't matter very much."
    },
    {
      "id": 44,
      "start": 186.94,
      "end": 188.34,
      "text": "There are only a few things that matter."
    },
    {
      "id": 45,
      "start": 188.4,
      "end": 189.9,
      "text": "And I think I listed seven of them."
    },
    {
      "id": 46,
      "start": 190.12,
      "end": 193.06,
      "text": "One is like how much raw compute you have."
    },
    {
      "id": 47,
      "start": 193.28,
      "end": 196.34,
      "text": "The other is the quantity of data that you have."
    },
    {
      "id": 48,
      "start": 196.70000000000002,
      "end": 200.20000000000002,
      "text": "Then the third is kind of the quality and distribution of data, right?"
    },
    {
      "id": 49,
      "start": 200.2,
      "end": 203.32,
      "text": "It needs to be a broad, broad distribution of data."
    },
    {
      "id": 50,
      "start": 203.7,
      "end": 206.22,
      "text": "The fourth is, I think, how long you train for."
    },
    {
      "id": 51,
      "start": 206.89999999999998,
      "end": 211.39999999999998,
      "text": "The fifth is you need an objective function that can scale to the moon."
    },
    {
      "id": 52,
      "start": 211.51999999999998,
      "end": 215.95999999999998,
      "text": "So the pre-training objective function is one such objective function, right?"
    },
    {
      "id": 53,
      "start": 216.56,
      "end": 222.5,
      "text": "Another objective function is, you know, the kind of RL objective function that says, like, you have a goal."
    },
    {
      "id": 54,
      "start": 222.62,
      "end": 224.2,
      "text": "You're going to go out and reach the goal."
    },
    {
      "id": 55,
      "start": 224.2,
      "end": 229.32,
      "text": "Within that, of course, there's objective rewards like, you know, like you see in math and coding."
    },
    {
      "id": 56,
      "start": 229.32,
      "end": 235.95999999999998,
      "text": "And there's more subjective rewards like you see in RL from human feedback or kind of higher order versions of that."
    },
    {
      "id": 57,
      "start": 236.5,
      "end": 250.62,
      "text": "And then the sixth and seventh were things around kind of like normalization or conditioning, like, you know, just getting the numerical stability so that kind of the big blob of compute flows in this laminar way instead of running into problems."
    },
    {
      "id": 58,
      "start": 250.76,
      "end": 252.56,
      "text": "So that was the hypothesis."
    },
    {
      "id": 59,
      "start": 252.98,
      "end": 255.14,
      "text": "And it's a hypothesis I still hold."
    },
    {
      "id": 60,
      "start": 255.14,
      "end": 259.97999999999996,
      "text": "I don't think I've seen very much that is not in line with that hypothesis."
    },
    {
      "id": 61,
      "start": 260.24,
      "end": 266.12,
      "text": "And so the pre-training scaling laws were one example of kind of what we see there."
    },
    {
      "id": 62,
      "start": 266.32,
      "end": 268.88,
      "text": "And indeed, those have continued going."
    },
    {
      "id": 63,
      "start": 269.26,
      "end": 272.97999999999996,
      "text": "Like, you know, you know, I think now it's been widely reported."
    },
    {
      "id": 64,
      "start": 273.15999999999997,
      "end": 275.36,
      "text": "Like, you know, we feel good about pre-training."
    },
    {
      "id": 65,
      "start": 275.47999999999996,
      "end": 278.52,
      "text": "Like, pre-training is continuing to give us gains."
    },
    {
      "id": 66,
      "start": 278.52,
      "end": 284.62,
      "text": "What has changed is that now we're also seeing the same thing for RL, right?"
    },
    {
      "id": 67,
      "start": 284.65999999999997,
      "end": 288.91999999999996,
      "text": "So we're seeing a pre-training phase and then we're seeing like an RL phase on top of that."
    },
    {
      "id": 68,
      "start": 288.92,
      "end": 293.96000000000004,
      "text": "And with RL, it's actually just the same."
    },
    {
      "id": 69,
      "start": 293.96000000000004,
      "end": 309.90000000000003,
      "text": "Like, you know, even other companies have published like, you know, in some of their releases have published things that say, look, you know, we train the model on math contests, you know, AIME or the kind of other things."
    },
    {
      "id": 70,
      "start": 309.9,
      "end": 315.5,
      "text": "And, you know, how well the model does is log linear and how long we've trained it."
    },
    {
      "id": 71,
      "start": 315.82,
      "end": 317.47999999999996,
      "text": "And we see that as well."
    },
    {
      "id": 72,
      "start": 317.52,
      "end": 318.84,
      "text": "And it's not just math contests."
    },
    {
      "id": 73,
      "start": 318.9,
      "end": 321.0,
      "text": "It's a wide variety of RL tasks."
    },
    {
      "id": 74,
      "start": 321.44,
      "end": 326.29999999999995,
      "text": "And so we're seeing the same scaling in RL that we saw for pre-training."
    },
    {
      "id": 75,
      "start": 327.4,
      "end": 329.09999999999997,
      "text": "You mentioned Richard Sutton and the Bitter Lesson."
    },
    {
      "id": 76,
      "start": 329.26,
      "end": 329.41999999999996,
      "text": "Yeah."
    },
    {
      "id": 77,
      "start": 329.5,
      "end": 331.26,
      "text": "I interviewed him last year."
    },
    {
      "id": 78,
      "start": 331.76,
      "end": 335.52,
      "text": "And he is actually very non-LLM pilled."
    },
    {
      "id": 79,
      "start": 335.52,
      "end": 357.15999999999997,
      "text": "And if I'm, I don't know if this is his perspective, but one way to paraphrase this objection is something like, look, something which possesses the true core of human learning would not require all these billions of dollars of data and compute and these bespoke environments to learn how to use Excel or how to use PowerPoint, how to navigate a web browser."
    },
    {
      "id": 80,
      "start": 357.16,
      "end": 369.46000000000004,
      "text": "And the fact that we have to build in these skills using these RL environments hints that we're actually lacking this core human learning algorithm."
    },
    {
      "id": 81,
      "start": 370.0,
      "end": 371.62,
      "text": "And so we're scaling the wrong thing."
    },
    {
      "id": 82,
      "start": 372.16,
      "end": 373.18,
      "text": "And so, yeah, that does raise the question."
    },
    {
      "id": 83,
      "start": 373.26000000000005,
      "end": 378.52000000000004,
      "text": "Why are we doing all this RL scaling if we do think there's something that's going to be human-like in its ability to learn on the fly?"
    },
    {
      "id": 84,
      "start": 379.04,
      "end": 379.66,
      "text": "Yeah, yeah."
    },
    {
      "id": 85,
      "start": 379.74,
      "end": 385.58000000000004,
      "text": "So I think this kind of puts together several things that should be kind of thought of differently."
    },
    {
      "id": 86,
      "start": 385.58,
      "end": 386.06,
      "text": "Yeah."
    },
    {
      "id": 87,
      "start": 386.38,
      "end": 390.58,
      "text": "I think there is a genuine puzzle here, but it may not matter."
    },
    {
      "id": 88,
      "start": 391.24,
      "end": 394.15999999999997,
      "text": "In fact, I would guess it probably doesn't matter."
    },
    {
      "id": 89,
      "start": 394.32,
      "end": 402.46,
      "text": "So let's take the RL out of it for a second because I actually think RL and it's a red herring to say that RL is any different from pre-training in this matter."
    },
    {
      "id": 90,
      "start": 402.96,
      "end": 407.46,
      "text": "So if we look at pre-training scaling, it was very interesting."
    },
    {
      "id": 91,
      "start": 407.46,
      "end": 413.53999999999996,
      "text": "Back in, you know, 2017 when Alec Radford was doing GPT-1."
    },
    {
      "id": 92,
      "start": 413.7,
      "end": 424.06,
      "text": "If you look at the models before GPT-1, they were trained on these data sets that didn't represent a wide, you know, distribution of text, right?"
    },
    {
      "id": 93,
      "start": 424.08,
      "end": 428.96,
      "text": "You had like, you know, these very standard, you know, kind of language modeling benchmarks."
    },
    {
      "id": 94,
      "start": 428.96,
      "end": 433.12,
      "text": "And GPT-1 itself was trained on a bunch of – I think it was fan fiction actually."
    },
    {
      "id": 95,
      "start": 433.84,
      "end": 439.35999999999996,
      "text": "But, you know, it was like literary – you know, it was like literary text, which is a very small fraction of the text that you get."
    },
    {
      "id": 96,
      "start": 439.68,
      "end": 443.64,
      "text": "And what we found with that, you know, and in those days it was like a billion words or something."
    },
    {
      "id": 97,
      "start": 443.64,
      "end": 449.47999999999996,
      "text": "So small data sets and represented a pretty narrow distribution, right?"
    },
    {
      "id": 98,
      "start": 449.5,
      "end": 453.47999999999996,
      "text": "Like a narrow distribution of kind of what you can see in the world."
    },
    {
      "id": 99,
      "start": 453.74,
      "end": 455.12,
      "text": "And it didn't generalize well."
    },
    {
      "id": 100,
      "start": 455.18,
      "end": 473.38,
      "text": "If you did better on, you know, the – you know, I forgot what it was, some kind of fan fiction corpus, it wouldn't generalize that well to kind of the other – you know, we had all these measures of like, you know, how well does the model do at predicting all of these other kinds of texts."
    },
    {
      "id": 101,
      "start": 473.38,
      "end": 475.34,
      "text": "You really didn't see the generalization."
    },
    {
      "id": 102,
      "start": 475.58,
      "end": 487.6,
      "text": "It was only when you trained over all the tasks on the internet, when you kind of did a general internet scrape, right, from something like, you know, Common Crawl or scraping links on Reddit, which is what we did for GPT-2."
    },
    {
      "id": 103,
      "start": 487.82,
      "end": 491.74,
      "text": "It's only when you do that that you kind of started to get generalization."
    },
    {
      "id": 104,
      "start": 493.12,
      "end": 500.78,
      "text": "And I think we're seeing the same thing on RL, that we're starting with first very simple RL tasks like training on math competitions."
    },
    {
      "id": 105,
      "start": 500.78,
      "end": 507.5,
      "text": "Then we're kind of moving to, you know, kind of broader training that involves things like code as a task."
    },
    {
      "id": 106,
      "start": 507.9,
      "end": 510.96,
      "text": "And now we're moving to do kind of many, many other tasks."
    },
    {
      "id": 107,
      "start": 511.21999999999997,
      "end": 514.72,
      "text": "And then I think we're going to increasingly get generalization."
    },
    {
      "id": 108,
      "start": 515.1,
      "end": 519.56,
      "text": "So that kind of takes out the RL versus the pre-training side of it."
    },
    {
      "id": 109,
      "start": 519.56,
      "end": 529.6999999999999,
      "text": "But I think there is a puzzle here either way, which is that on pre-training, when we train the model on pre-training, you know, we use like trillions of tokens, right?"
    },
    {
      "id": 110,
      "start": 529.9,
      "end": 532.68,
      "text": "And humans don't see trillions of words."
    },
    {
      "id": 111,
      "start": 532.76,
      "end": 535.5,
      "text": "So there is an actual sample efficiency difference here."
    },
    {
      "id": 112,
      "start": 535.8599999999999,
      "end": 541.26,
      "text": "There is actually something different that's happening here, which is that the models start from scratch."
    },
    {
      "id": 113,
      "start": 541.26,
      "end": 545.76,
      "text": "And, you know, they have to get much more, much more training."
    },
    {
      "id": 114,
      "start": 546.06,
      "end": 553.22,
      "text": "But we also see that once they're trained, if we give them a long context length, the only thing blocking a long context length is like inference."
    },
    {
      "id": 115,
      "start": 553.38,
      "end": 559.34,
      "text": "But if we give them like a context length of a million, they're very good at learning and adapting within that context length."
    },
    {
      "id": 116,
      "start": 559.34,
      "end": 569.7800000000001,
      "text": "And so I don't know the full answer to this, but I think there's something going on that pre-training, it's not like the process of humans learning."
    },
    {
      "id": 117,
      "start": 570.02,
      "end": 574.12,
      "text": "It's somewhere between the process of humans learning and the process of human evolution."
    },
    {
      "id": 118,
      "start": 574.5600000000001,
      "end": 578.5600000000001,
      "text": "It's like it's somewhere between like we get many of our priors from evolution."
    },
    {
      "id": 119,
      "start": 578.86,
      "end": 581.08,
      "text": "Our brain isn't just a blank slate, right?"
    },
    {
      "id": 120,
      "start": 581.1600000000001,
      "end": 582.46,
      "text": "Whole books have been written about."
    },
    {
      "id": 121,
      "start": 582.82,
      "end": 585.26,
      "text": "I think the language models, they're much more blank slates."
    },
    {
      "id": 122,
      "start": 585.26,
      "end": 589.9399999999999,
      "text": "They literally start as like random weights, whereas the human brain starts with all these regions."
    },
    {
      "id": 123,
      "start": 589.9399999999999,
      "end": 592.3,
      "text": "It's connected to all these inputs and outputs."
    },
    {
      "id": 124,
      "start": 593.6,
      "end": 607.76,
      "text": "And so maybe we should think of pre-training and for that matter, RL as well as being something that exists in the middle space between human evolution and, you know, kind of human on the spot learning."
    },
    {
      "id": 125,
      "start": 607.76,
      "end": 616.9399999999999,
      "text": "And as the in-context learning that the models do as something between long-term human learning and short-term human learning."
    },
    {
      "id": 126,
      "start": 617.16,
      "end": 624.28,
      "text": "So, you know, there's this hierarchy of like there's evolution, there's long-term learning, there's short-term learning, and there's just human reaction."
    },
    {
      "id": 127,
      "start": 624.86,
      "end": 631.78,
      "text": "And the LOM phases exist along this spectrum, but not necessarily exactly at the same points."
    },
    {
      "id": 128,
      "start": 631.78,
      "end": 638.16,
      "text": "There's no analog to some of the human modes of learning that LOMs are kind of falling between the points."
    },
    {
      "id": 129,
      "start": 638.3199999999999,
      "end": 639.4,
      "text": "Does that make sense?"
    },
    {
      "id": 130,
      "start": 639.74,
      "end": 642.16,
      "text": "Yes, although some things are still a bit confusing."
    },
    {
      "id": 131,
      "start": 642.3399999999999,
      "end": 661.64,
      "text": "For example, if the analogy is that this is like evolution, so it's fine that it's not that sample efficient, then like, well, if we're going to get the kind of super sample efficient Asian from in-context learning, why are we bothering to build in, you know, there's RL environment companies which are, it seems like what they're doing is they're teaching it how to use this API, how to use Slack, how to use whatever."
    },
    {
      "id": 132,
      "start": 661.78,
      "end": 670.64,
      "text": "It's confusing to me why there's so much emphasis on that if the kind of agent that can just learn on the fly is emerging or is going to soon emerge or has already emerged."
    },
    {
      "id": 133,
      "start": 670.64,
      "end": 670.74,
      "text": "Yeah, yeah."
    },
    {
      "id": 134,
      "start": 670.8399999999999,
      "end": 673.26,
      "text": "So, I mean, I can't speak for the emphasis of anyone else."
    },
    {
      "id": 135,
      "start": 673.4,
      "end": 676.66,
      "text": "I can only talk about how we think about it."
    },
    {
      "id": 136,
      "start": 676.9599999999999,
      "end": 687.04,
      "text": "I think the way we think about it is the goal is not to teach the model every possible skill within RL, just as we don't do that within pre-training, right?"
    },
    {
      "id": 137,
      "start": 687.04,
      "end": 695.66,
      "text": "Within pre-training, we're not trying to expose the model to, you know, every possible, you know, way that words could be put together, right?"
    },
    {
      "id": 138,
      "start": 695.76,
      "end": 703.4399999999999,
      "text": "You know, it's rather that the model trains on a lot of things and then it reaches generalization across pre-training, right?"
    },
    {
      "id": 139,
      "start": 703.44,
      "end": 711.9000000000001,
      "text": "That was the transition from GPT-1 to GPT-2 that I saw up close, which is like, you know, the model reaches a point, you know?"
    },
    {
      "id": 140,
      "start": 712.44,
      "end": 722.2600000000001,
      "text": "I like had these moments where I was like, oh, yeah, you just give the model like, you just give the model a list of numbers that's like, you know, you know, this is the cost of the house."
    },
    {
      "id": 141,
      "start": 722.36,
      "end": 723.7,
      "text": "This is the square feet of the house."
    },
    {
      "id": 142,
      "start": 723.7,
      "end": 726.9000000000001,
      "text": "And the model completes the pattern and does linear regression."
    },
    {
      "id": 143,
      "start": 727.1,
      "end": 728.3000000000001,
      "text": "Like, not great, but it does it."
    },
    {
      "id": 144,
      "start": 728.38,
      "end": 730.86,
      "text": "But it's never seen that exact thing before."
    },
    {
      "id": 145,
      "start": 731.58,
      "end": 744.2800000000001,
      "text": "And so, you know, to the extent that we are building these RL environments, the goal is very similar to what is, you know, to what was done five or ten years ago with pre-training."
    },
    {
      "id": 146,
      "start": 744.28,
      "end": 754.68,
      "text": "With we're trying to get a whole bunch of data, not because we want to cover a specific document or a specific skill, but because we want to generalize."
    },
    {
      "id": 147,
      "start": 755.14,
      "end": 760.14,
      "text": "I mean, I think the framework you're laying down obviously makes sense."
    },
    {
      "id": 148,
      "start": 760.1999999999999,
      "end": 761.56,
      "text": "Like, we're making progress towards AGI."
    },
    {
      "id": 149,
      "start": 762.06,
      "end": 767.36,
      "text": "I think the crux is something like nobody at this point disagrees that we're going to achieve AGI in this century."
    },
    {
      "id": 150,
      "start": 767.36,
      "end": 775.86,
      "text": "And the crux is you say we're hitting the end of the exponential and somebody else looks at this and says, oh, yeah, we're making progress."
    },
    {
      "id": 151,
      "start": 775.96,
      "end": 776.96,
      "text": "We've been making progress since 2012."
    },
    {
      "id": 152,
      "start": 777.22,
      "end": 780.02,
      "text": "And then 2035 will have a human-like agent."
    },
    {
      "id": 153,
      "start": 780.34,
      "end": 790.72,
      "text": "And so I want to understand what it is that you're seeing, which makes you think, yeah, obviously we're seeing the kinds of things that evolution did or that within human lifetime learning is like in these models."
    },
    {
      "id": 154,
      "start": 790.78,
      "end": 793.6800000000001,
      "text": "And why think that it's one year away and not ten years away?"
    },
    {
      "id": 155,
      "start": 793.68,
      "end": 800.14,
      "text": "I actually think of it as like two – there's kind of two cases to be made here, all right?"
    },
    {
      "id": 156,
      "start": 800.1999999999999,
      "end": 804.8399999999999,
      "text": "Two claims you could make, one of which is like stronger and the other of which is weaker."
    },
    {
      "id": 157,
      "start": 805.04,
      "end": 815.8199999999999,
      "text": "So I think starting with the weaker claim, you know, when I first saw the scaling back in like, you know, 2019, you know, I wasn't sure."
    },
    {
      "id": 158,
      "start": 815.9599999999999,
      "end": 818.9799999999999,
      "text": "You know, this was the whole – this was kind of a 50-50 thing, right?"
    },
    {
      "id": 159,
      "start": 818.98,
      "end": 825.34,
      "text": "I thought I saw something that was, you know, and my claim was this is much more likely than anyone thinks it is."
    },
    {
      "id": 160,
      "start": 825.38,
      "end": 826.28,
      "text": "Like this is wild."
    },
    {
      "id": 161,
      "start": 826.62,
      "end": 828.1,
      "text": "No one else would even consider this."
    },
    {
      "id": 162,
      "start": 828.46,
      "end": 830.3000000000001,
      "text": "Maybe there's a 50% chance this happens."
    },
    {
      "id": 163,
      "start": 831.36,
      "end": 841.66,
      "text": "On the basic hypothesis of, you know, as you put it, within 10 years we'll get to, you know, what I call kind of country of geniuses in a data center."
    },
    {
      "id": 164,
      "start": 841.66,
      "end": 844.12,
      "text": "I'm at like 90% on that."
    },
    {
      "id": 165,
      "start": 844.52,
      "end": 848.5,
      "text": "And it's hard to go much higher than 90% because the world is so unpredictable."
    },
    {
      "id": 166,
      "start": 849.48,
      "end": 861.3199999999999,
      "text": "Maybe the irreducible uncertainty would be if we were at 95% where you get to things like, I don't know, maybe, you know, multiple companies have, you know, kind of internal turmoil and nothing happens."
    },
    {
      "id": 167,
      "start": 861.5,
      "end": 867.4599999999999,
      "text": "And then Taiwan gets invaded and like all the fabs get blown up by missiles and, you know, and then –"
    },
    {
      "id": 168,
      "start": 867.4599999999999,
      "end": 868.24,
      "text": "Now you would drink to Staria."
    },
    {
      "id": 169,
      "start": 868.24,
      "end": 869.16,
      "text": "Yeah, yeah, yeah."
    },
    {
      "id": 170,
      "start": 869.16,
      "end": 881.8,
      "text": "You know, just you could construct a scenario where there's like a 5% chance that it – or, you know, you can construct a 5% world where like things get delayed for 10 years."
    },
    {
      "id": 171,
      "start": 882.0600000000001,
      "end": 883.44,
      "text": "That's maybe 5%."
    },
    {
      "id": 172,
      "start": 883.44,
      "end": 889.0,
      "text": "There's another 5% which is that I'm very confident on tasks that can be verified."
    },
    {
      "id": 173,
      "start": 889.26,
      "end": 895.6,
      "text": "So I think with coding, I'm just – except for that irreducible uncertainty, there's just – I mean, I think we'll be there in one or two years."
    },
    {
      "id": 174,
      "start": 895.6,
      "end": 900.44,
      "text": "There's no way we will not be there in 10 years in terms of being able to do it end-to-end coding."
    },
    {
      "id": 175,
      "start": 900.86,
      "end": 920.32,
      "text": "My one little bit, the one little bit of fundamental uncertainty even on long timescales is this thing about tasks that aren't verifiable like planning a mission to Mars, like, you know, doing some fundamental scientific discovery like CRISPR, like, you know, writing a novel."
    },
    {
      "id": 176,
      "start": 920.32,
      "end": 922.96,
      "text": "Hard to verify those tasks."
    },
    {
      "id": 177,
      "start": 923.1800000000001,
      "end": 928.86,
      "text": "I am almost certain that we have a reliable path to get there."
    },
    {
      "id": 178,
      "start": 929.0200000000001,
      "end": 932.98,
      "text": "But like if there was a little bit uncertainty, it's there."
    },
    {
      "id": 179,
      "start": 933.1400000000001,
      "end": 939.4200000000001,
      "text": "So on the 10 years, I'm like, you know, 90%, which is about as certain as you can be."
    },
    {
      "id": 180,
      "start": 939.42,
      "end": 945.54,
      "text": "Like, I think it's crazy to say that this won't happen by 2035."
    },
    {
      "id": 181,
      "start": 945.76,
      "end": 948.5,
      "text": "Like, in some sane world, it would be outside the mainstream."
    },
    {
      "id": 182,
      "start": 948.9,
      "end": 958.4599999999999,
      "text": "But the emphasis on verification hints to me as a lack of belief that these models are generalized."
    },
    {
      "id": 183,
      "start": 958.68,
      "end": 964.6999999999999,
      "text": "If you think about humans, we are good at things that both of which we get verifiable reward and things which we don't."
    },
    {
      "id": 184,
      "start": 964.7,
      "end": 965.48,
      "text": "You're like, you haven't started."
    },
    {
      "id": 185,
      "start": 965.9000000000001,
      "end": 966.48,
      "text": "No, no, no."
    },
    {
      "id": 186,
      "start": 966.5600000000001,
      "end": 967.7,
      "text": "This is why I'm almost sure."
    },
    {
      "id": 187,
      "start": 967.76,
      "end": 972.9200000000001,
      "text": "We already see substantial generalization from things that verify to things that don't verify."
    },
    {
      "id": 188,
      "start": 972.96,
      "end": 974.0200000000001,
      "text": "We're already seeing that."
    },
    {
      "id": 189,
      "start": 974.0200000000001,
      "end": 980.82,
      "text": "But it seems like you were emphasizing this as a spectrum which will split apart which domains we see more progress."
    },
    {
      "id": 190,
      "start": 980.9200000000001,
      "end": 983.1400000000001,
      "text": "And I'm like, but that doesn't seem like how humans get better."
    },
    {
      "id": 191,
      "start": 983.24,
      "end": 990.7,
      "text": "The world in which we don't make it or the world in which we don't get there is the world in which we do all the things that are verifiable."
    },
    {
      "id": 192,
      "start": 990.7,
      "end": 996.44,
      "text": "And then they like, you know, many of them generalize, but we kind of don't get fully there."
    },
    {
      "id": 193,
      "start": 996.5600000000001,
      "end": 1000.46,
      "text": "We don't fully color in this side of the box."
    },
    {
      "id": 194,
      "start": 1001.24,
      "end": 1003.38,
      "text": "It's not a binary thing."
    },
    {
      "id": 195,
      "start": 1003.6600000000001,
      "end": 1011.8000000000001,
      "text": "But it also seems to me, even if in the world where generalization is weak when you only say verifiable domains, it's not clear to me in such a world you could automate software engineering."
    },
    {
      "id": 196,
      "start": 1011.94,
      "end": 1016.26,
      "text": "Because software, like, in some sense, you are, quote, unquote, a software engineer."
    },
    {
      "id": 197,
      "start": 1016.4200000000001,
      "end": 1016.5400000000001,
      "text": "Yeah."
    },
    {
      "id": 198,
      "start": 1016.54,
      "end": 1021.16,
      "text": "But part of being a software engineer for you involves writing these, like, long memos about your grand vision about different things."
    },
    {
      "id": 199,
      "start": 1021.3,
      "end": 1022.9,
      "text": "Well, I don't think that's part of the job of SWE."
    },
    {
      "id": 200,
      "start": 1023.02,
      "end": 1024.74,
      "text": "That's part of the job of the company."
    },
    {
      "id": 201,
      "start": 1024.74,
      "end": 1028.94,
      "text": "But I do think SWE involves, like, design documents and other things like that."
    },
    {
      "id": 202,
      "start": 1029.46,
      "end": 1031.36,
      "text": "Which, by the way, the models are not bad."
    },
    {
      "id": 203,
      "start": 1031.48,
      "end": 1033.06,
      "text": "They're already pretty good at writing comments."
    },
    {
      "id": 204,
      "start": 1033.06,
      "end": 1043.82,
      "text": "And so, again, I'm making, like, much weaker claims here than I believe to, like, you know, to kind of set up a, you know, to distinguish between two things."
    },
    {
      "id": 205,
      "start": 1043.9199999999998,
      "end": 1046.48,
      "text": "Like, we're already almost there for software engineering."
    },
    {
      "id": 206,
      "start": 1046.58,
      "end": 1047.6799999999998,
      "text": "We are already almost there."
    },
    {
      "id": 207,
      "start": 1047.9199999999998,
      "end": 1048.58,
      "text": "By what metric?"
    },
    {
      "id": 208,
      "start": 1048.72,
      "end": 1051.04,
      "text": "There's one metric, which is, like, how many lines of code are written by AI."
    },
    {
      "id": 209,
      "start": 1051.04,
      "end": 1058.44,
      "text": "And if you use, if you consider other productivity improvements in the course of the history of software engineering, compilers write all the lines of software."
    },
    {
      "id": 210,
      "start": 1059.1399999999999,
      "end": 1062.72,
      "text": "But there's a difference between how many lines are written and how big the productivity improvement is."
    },
    {
      "id": 211,
      "start": 1062.86,
      "end": 1063.22,
      "text": "Oh, yeah."
    },
    {
      "id": 212,
      "start": 1063.7,
      "end": 1069.3,
      "text": "And then, like, we're almost there meaning, like, how big is the productivity improvement, not just how many lines are written."
    },
    {
      "id": 213,
      "start": 1069.3,
      "end": 1069.6599999999999,
      "text": "Yeah, yeah."
    },
    {
      "id": 214,
      "start": 1069.72,
      "end": 1073.1399999999999,
      "text": "So, I actually agree with you on this."
    },
    {
      "id": 215,
      "start": 1073.2,
      "end": 1078.3799999999999,
      "text": "So, I've made this series of predictions on code and software engineering."
    },
    {
      "id": 216,
      "start": 1078.9199999999998,
      "end": 1081.8999999999999,
      "text": "And I think people have repeatedly kind of misunderstood them."
    },
    {
      "id": 217,
      "start": 1082.06,
      "end": 1085.22,
      "text": "So, let me lay out the spectrum, right?"
    },
    {
      "id": 218,
      "start": 1085.44,
      "end": 1096.4199999999998,
      "text": "Like, I think it was, you know, like, you know, eight or nine months ago or something, I said, you know, the AI model will be writing 90% of the lines of code in, like, you know, three to six months."
    },
    {
      "id": 219,
      "start": 1096.42,
      "end": 1099.0600000000002,
      "text": "Which happened at least at some places, right?"
    },
    {
      "id": 220,
      "start": 1099.14,
      "end": 1103.8600000000001,
      "text": "Happened at Anthropic, happened with many people downstream using our models."
    },
    {
      "id": 221,
      "start": 1104.04,
      "end": 1107.14,
      "text": "But that's actually a very weak criterion, right?"
    },
    {
      "id": 222,
      "start": 1107.28,
      "end": 1111.5,
      "text": "People thought I was saying, like, we won't need 90% of the software engineers."
    },
    {
      "id": 223,
      "start": 1111.72,
      "end": 1113.68,
      "text": "Those things are worlds apart, right?"
    },
    {
      "id": 224,
      "start": 1113.74,
      "end": 1122.44,
      "text": "Like, I would put the spectrum as 90% of code is written by the model, 100% of code is written by the model, and that's a big difference in productivity."
    },
    {
      "id": 225,
      "start": 1122.44,
      "end": 1137.48,
      "text": "90% of the end-to-end SWE tasks, right, including things like compiling, including things like setting up clusters and environments, testing features, writing memos, 90% of the SWE tasks are written by the models."
    },
    {
      "id": 226,
      "start": 1137.92,
      "end": 1141.74,
      "text": "100% of today's SWE tasks are written by the models."
    },
    {
      "id": 227,
      "start": 1142.22,
      "end": 1145.42,
      "text": "And even when that happens, it doesn't mean software engineers are out of a job."
    },
    {
      "id": 228,
      "start": 1145.42,
      "end": 1149.44,
      "text": "Like, there's, like, new higher-level things they can do where they can manage."
    },
    {
      "id": 229,
      "start": 1149.8200000000002,
      "end": 1156.24,
      "text": "And then there's a further down the spectrum, like, you know, there's 90% less demand for SWE, which I think will happen."
    },
    {
      "id": 230,
      "start": 1156.24,
      "end": 1158.78,
      "text": "But, like, this is a spectrum."
    },
    {
      "id": 231,
      "start": 1158.94,
      "end": 1164.48,
      "text": "And, you know, I wrote about it in The Adolescence of Technology where I went through this kind of spectrum with farming."
    },
    {
      "id": 232,
      "start": 1164.48,
      "end": 1168.72,
      "text": "And so I actually totally agree with you on that."
    },
    {
      "id": 233,
      "start": 1169.0,
      "end": 1174.0,
      "text": "It's just these are very different benchmarks from each other, but we're proceeding through them super fast."
    },
    {
      "id": 234,
      "start": 1174.46,
      "end": 1176.66,
      "text": "It seems like in part of your vision, it's, like, going from 90 to 100."
    },
    {
      "id": 235,
      "start": 1178.04,
      "end": 1179.38,
      "text": "First, it's going to happen fast."
    },
    {
      "id": 236,
      "start": 1179.6,
      "end": 1184.92,
      "text": "And, two, that somehow that leads to huge productivity improvements."
    },
    {
      "id": 237,
      "start": 1185.78,
      "end": 1191.38,
      "text": "Whereas when I notice, even in greenfield projects that people start with Cloud Code or something, people report starting a lot of projects."
    },
    {
      "id": 238,
      "start": 1191.38,
      "end": 1197.76,
      "text": "And I'm, like, do we see in the world out there a renaissance of software, all these new features that wouldn't exist otherwise?"
    },
    {
      "id": 239,
      "start": 1197.88,
      "end": 1200.18,
      "text": "And at least so far, it doesn't seem like we see that."
    },
    {
      "id": 240,
      "start": 1200.18,
      "end": 1209.8600000000001,
      "text": "And so that does make me wonder, even if, like, I never had to intervene on Cloud Code, there is this thing of, like, there's just the world is complicated, jobs are complicated."
    },
    {
      "id": 241,
      "start": 1210.3,
      "end": 1219.68,
      "text": "And closing the loop on self-contained systems, whether it's just writing software or something, how much sort of how much broader gains we would see just from that?"
    },
    {
      "id": 242,
      "start": 1219.78,
      "end": 1224.5,
      "text": "And so maybe that makes us this should dilute our estimation of the country of geniuses."
    },
    {
      "id": 243,
      "start": 1224.5,
      "end": 1235.12,
      "text": "Well, I actually, I, like, simultaneously, I simultaneously agree with you, agree that it's a reason why these things don't happen instantly."
    },
    {
      "id": 244,
      "start": 1235.38,
      "end": 1239.64,
      "text": "But at the same time, I think the effect is going to be very fast."
    },
    {
      "id": 245,
      "start": 1239.86,
      "end": 1241.26,
      "text": "So, like, I don't know."
    },
    {
      "id": 246,
      "start": 1241.3,
      "end": 1242.76,
      "text": "You could have these two poles, right?"
    },
    {
      "id": 247,
      "start": 1242.82,
      "end": 1247.36,
      "text": "One is, like, you know, AI is, like, you know, it's not going to make progress."
    },
    {
      "id": 248,
      "start": 1247.5,
      "end": 1248.4,
      "text": "It's slow."
    },
    {
      "id": 249,
      "start": 1248.62,
      "end": 1252.3,
      "text": "Like, it's going to take, you know, kind of forever to diffuse within the economy, right?"
    },
    {
      "id": 250,
      "start": 1252.3,
      "end": 1259.54,
      "text": "Economic diffusion has become one of these buzzwords that's, like, a reason why we're not going to make AI progress or why AI progress doesn't matter."
    },
    {
      "id": 251,
      "start": 1260.08,
      "end": 1267.54,
      "text": "And, you know, the other axis is, like, we'll get recursive self-improvement, you know, the whole thing, you know, can't you just draw an exponential line on the curve?"
    },
    {
      "id": 252,
      "start": 1267.68,
      "end": 1268.3799999999999,
      "text": "You know, it's good."
    },
    {
      "id": 253,
      "start": 1268.3999999999999,
      "end": 1277.78,
      "text": "We're going to have, you know, Dyson spheres around the sun in, like, you know, so many nanoseconds after, you know, after we get recursive."
    },
    {
      "id": 254,
      "start": 1277.78,
      "end": 1282.86,
      "text": "I mean, I'm completely caricaturing the view here, but, like, you know, there are these two extremes."
    },
    {
      "id": 255,
      "start": 1283.44,
      "end": 1294.44,
      "text": "But what we've seen from the beginning, you know, at least if you look within Anthropic, there's this bizarre 10x per year growth in revenue that we've seen, right?"
    },
    {
      "id": 256,
      "start": 1294.44,
      "end": 1297.9,
      "text": "So, you know, in 2023, it was, like, zero to 100 million."
    },
    {
      "id": 257,
      "start": 1298.3,
      "end": 1300.76,
      "text": "2024, it was 100 million to a billion."
    },
    {
      "id": 258,
      "start": 1301.4,
      "end": 1305.02,
      "text": "2025, it was a billion to, like, nine or 10 billion."
    },
    {
      "id": 259,
      "start": 1305.7,
      "end": 1306.44,
      "text": "And then..."
    },
    {
      "id": 260,
      "start": 1306.44,
      "end": 1309.6200000000001,
      "text": "You guys should have just bought, like, a billion dollars with your own products so you could just, like, have a clean 10b."
    },
    {
      "id": 261,
      "start": 1309.62,
      "end": 1313.9599999999998,
      "text": "And the first month of this year, like, that exponential is..."
    },
    {
      "id": 262,
      "start": 1313.9599999999998,
      "end": 1323.3799999999999,
      "text": "You would think it would slow down, but it would, like, you know, we added another few billion to, like, you know, we added another few billion to revenue in January."
    },
    {
      "id": 263,
      "start": 1323.84,
      "end": 1328.1,
      "text": "And so, you know, obviously that curve can't go on forever, right?"
    },
    {
      "id": 264,
      "start": 1328.1599999999999,
      "end": 1329.9599999999998,
      "text": "You know, the GDP is only so large."
    },
    {
      "id": 265,
      "start": 1330.32,
      "end": 1330.6999999999998,
      "text": "I don't..."
    },
    {
      "id": 266,
      "start": 1330.6999999999998,
      "end": 1335.34,
      "text": "You know, I would even guess that it bends somewhat this year."
    },
    {
      "id": 267,
      "start": 1335.4599999999998,
      "end": 1338.4799999999998,
      "text": "But, like, that is, like, a fast curve, right?"
    },
    {
      "id": 268,
      "start": 1338.48,
      "end": 1340.8,
      "text": "That's, like, a really fast curve."
    },
    {
      "id": 269,
      "start": 1341.2,
      "end": 1345.3,
      "text": "And I would bet it stays pretty fast even as the scale goes to the entire economy."
    },
    {
      "id": 270,
      "start": 1345.76,
      "end": 1364.88,
      "text": "So, like, I think we should be thinking about this middle world where things are, like, extremely fast but not instant, where they take time because of economic diffusion, because of the need to close the loop, because, you know, it's, like, this fiddly, oh, man, I have to do change management within my enterprise."
    },
    {
      "id": 271,
      "start": 1364.88,
      "end": 1374.5400000000002,
      "text": "You know, I have to, like, you know, you know, I, like, I set this up, but, you know, I have to change the security permissions on this in order to make it actually work."
    },
    {
      "id": 272,
      "start": 1374.8200000000002,
      "end": 1382.8200000000002,
      "text": "Or, you know, I had this, like, old piece of software that, you know, that, like, you know, checks the model before it's compiled and, like, released."
    },
    {
      "id": 273,
      "start": 1382.88,
      "end": 1383.8600000000001,
      "text": "And I have to rewrite it."
    },
    {
      "id": 274,
      "start": 1383.86,
      "end": 1386.4599999999998,
      "text": "And, yes, the model can do that, but I have to tell the model to do that."
    },
    {
      "id": 275,
      "start": 1386.5,
      "end": 1388.84,
      "text": "And it has to take time to do that."
    },
    {
      "id": 276,
      "start": 1388.84,
      "end": 1405.6,
      "text": "And so I think everything we've seen so far is compatible with the idea that there's one fast exponential that's the capability of the model, and then there's another fast exponential that's downstream of that, which is the diffusion of the model into the economy."
    },
    {
      "id": 277,
      "start": 1406.04,
      "end": 1413.62,
      "text": "Not instant, not slow, much faster than any previous technology, but it has its limits."
    },
    {
      "id": 278,
      "start": 1413.62,
      "end": 1423.02,
      "text": "And this is what we, you know, when I look inside Anthropic, when I look at our customers, fast adoption, but not infinitely fast."
    },
    {
      "id": 279,
      "start": 1423.76,
      "end": 1425.02,
      "text": "Can I try a hot take on you?"
    },
    {
      "id": 280,
      "start": 1425.1599999999999,
      "end": 1425.36,
      "text": "Yeah."
    },
    {
      "id": 281,
      "start": 1425.4799999999998,
      "end": 1433.76,
      "text": "I feel like diffusion is cope that people use to say when it's, like, if the model isn't able to do something, they're, like, oh, but it's, like, a diffusion issue."
    },
    {
      "id": 282,
      "start": 1433.76,
      "end": 1436.4,
      "text": "But then you should use the comparison to humans."
    },
    {
      "id": 283,
      "start": 1436.68,
      "end": 1445.54,
      "text": "You would think that the inherent advantages that AIs have would make diffusion a much easier problem for new AIs getting onboarded than new humans getting onboarded."
    },
    {
      "id": 284,
      "start": 1445.64,
      "end": 1448.5,
      "text": "So an AI can read your entire Slack and your drive in minutes."
    },
    {
      "id": 285,
      "start": 1448.64,
      "end": 1451.94,
      "text": "They can share all the knowledge that the other copies of the same instance have."
    },
    {
      "id": 286,
      "start": 1452.18,
      "end": 1456.42,
      "text": "You don't have this adverse selection problem when you're hiring AIs because you can just hire copies of a vetted AI model."
    },
    {
      "id": 287,
      "start": 1457.64,
      "end": 1459.56,
      "text": "Hiring a human is, like, so much more hassle."
    },
    {
      "id": 288,
      "start": 1460.0,
      "end": 1461.4,
      "text": "And people hire humans all the time, right?"
    },
    {
      "id": 289,
      "start": 1461.4,
      "end": 1471.94,
      "text": "We pay humans upwards of $50 trillion in wages because they're useful, even though it's, like, in principle, it would be much easier to integrate AIs into the economy than it is to hire humans."
    },
    {
      "id": 290,
      "start": 1472.0400000000002,
      "end": 1474.42,
      "text": "I think, like, the diffusion, I feel like, doesn't really explain."
    },
    {
      "id": 291,
      "start": 1474.42,
      "end": 1485.72,
      "text": "I think diffusion is very real and doesn't have to, you know, doesn't exclusively have to do with limitations on the AI models."
    },
    {
      "id": 292,
      "start": 1485.72,
      "end": 1491.82,
      "text": "Like, again, there are people who use diffusion to, you know, as kind of a buzzword to say this isn't a big deal."
    },
    {
      "id": 293,
      "start": 1492.1000000000001,
      "end": 1493.48,
      "text": "I'm not talking about that."
    },
    {
      "id": 294,
      "start": 1493.7,
      "end": 1497.7,
      "text": "I'm not talking about, you know, AI will diffuse at the speed that previous."
    },
    {
      "id": 295,
      "start": 1497.96,
      "end": 1503.54,
      "text": "I think AI will diffuse much faster than previous technologies have, but not infinitely fast."
    },
    {
      "id": 296,
      "start": 1503.58,
      "end": 1505.52,
      "text": "So I'll just give an example of this, right?"
    },
    {
      "id": 297,
      "start": 1505.6000000000001,
      "end": 1507.2,
      "text": "Like, there's, like, Claude Code."
    },
    {
      "id": 298,
      "start": 1507.3,
      "end": 1509.72,
      "text": "Like, Claude Code is extremely easy to set up."
    },
    {
      "id": 299,
      "start": 1509.72,
      "end": 1513.8,
      "text": "You know, if you're a developer, you can kind of just start using Claude Code."
    },
    {
      "id": 300,
      "start": 1513.94,
      "end": 1524.72,
      "text": "There is no reason why a developer at a large enterprise should not be adopting Claude Code as quickly as, you know, an individual developer or developer at a startup."
    },
    {
      "id": 301,
      "start": 1524.8600000000001,
      "end": 1526.96,
      "text": "And we do everything we can to promote it, right?"
    },
    {
      "id": 302,
      "start": 1526.96,
      "end": 1544.74,
      "text": "We sell Claude Code to enterprises and big enterprises, like, you know, big financial companies, big pharmaceutical companies, all of them, they're adopting Claude Code much faster than enterprises typically adopt new technology, right?"
    },
    {
      "id": 303,
      "start": 1544.74,
      "end": 1548.7,
      "text": "But, again, like, it takes time."
    },
    {
      "id": 304,
      "start": 1549.06,
      "end": 1570.74,
      "text": "Like, any given feature or any given product, like Claude Code or, like, Co-Work will get adopted by the, you know, the individual developers who are on Twitter all the time, by the, like, Series A startups many months faster than, you know, than they will get adopted by, like, you know, a, like, large enterprise that does food sales."
    },
    {
      "id": 305,
      "start": 1570.74,
      "end": 1574.4,
      "text": "There are a number of factors, like, you have to go through legal."
    },
    {
      "id": 306,
      "start": 1574.6,
      "end": 1576.1200000000001,
      "text": "You have to provision it for everyone."
    },
    {
      "id": 307,
      "start": 1576.46,
      "end": 1580.6,
      "text": "It has to, you know, like, it has to pass security and compliance."
    },
    {
      "id": 308,
      "start": 1580.92,
      "end": 1590.86,
      "text": "The leaders of the company who are further away from the AI revolution, you know, are forward-looking, but they have to say, oh, it makes sense for us to spend 50 million."
    },
    {
      "id": 309,
      "start": 1591.04,
      "end": 1592.94,
      "text": "This is what this Claude Code thing is."
    },
    {
      "id": 310,
      "start": 1593.22,
      "end": 1594.7,
      "text": "This is why it helps our company."
    },
    {
      "id": 311,
      "start": 1595.02,
      "end": 1596.58,
      "text": "This is why it makes us more productive."
    },
    {
      "id": 312,
      "start": 1596.58,
      "end": 1602.26,
      "text": "And then they have to explain to the people two levels below, and they have to say, okay, we have 3,000 developers."
    },
    {
      "id": 313,
      "start": 1602.48,
      "end": 1604.6,
      "text": "Like, here's how we're going to roll it out to our developers."
    },
    {
      "id": 314,
      "start": 1604.84,
      "end": 1607.1799999999998,
      "text": "And we have conversations like this every day."
    },
    {
      "id": 315,
      "start": 1607.32,
      "end": 1615.3999999999999,
      "text": "Like, you know, we are doing everything we can to make Anthropics revenue grow 20 or 30x a year instead of 10x a year."
    },
    {
      "id": 316,
      "start": 1616.06,
      "end": 1620.8999999999999,
      "text": "You know, and, again, you know, many enterprises are just saying this is so productive."
    },
    {
      "id": 317,
      "start": 1621.1999999999998,
      "end": 1624.86,
      "text": "Like, you know, we're going to take shortcuts on our usual procurement process, right?"
    },
    {
      "id": 318,
      "start": 1624.86,
      "end": 1630.3799999999999,
      "text": "They're moving much faster than, you know, when we tried to sell them just the ordinary API, which many of them use."
    },
    {
      "id": 319,
      "start": 1630.5,
      "end": 1632.9599999999998,
      "text": "But Claude Code is a more compelling product."
    },
    {
      "id": 320,
      "start": 1633.4799999999998,
      "end": 1635.74,
      "text": "But it's not an infinitely compelling product."
    },
    {
      "id": 321,
      "start": 1635.82,
      "end": 1642.6,
      "text": "And I don't think even AGI or powerful AI or country of geniuses in the data center will be an infinitely compelling product."
    },
    {
      "id": 322,
      "start": 1642.82,
      "end": 1654.06,
      "text": "It will be a compelling product enough maybe to get 3 or 5 or 10x a year growth even when you're in the hundreds of billions of dollars, which is extremely hard to do and has never been done in history before, but not infinitely fast."
    },
    {
      "id": 323,
      "start": 1654.06,
      "end": 1656.1399999999999,
      "text": "I buy that it would be a slight slowdown."
    },
    {
      "id": 324,
      "start": 1656.3,
      "end": 1661.3799999999999,
      "text": "And maybe this is not your claim, but sometimes people talk about this like, oh, the capabilities are there, but because of diffusion."
    },
    {
      "id": 325,
      "start": 1662.8999999999999,
      "end": 1665.6799999999998,
      "text": "Otherwise, like, we're basically at AGI and then."
    },
    {
      "id": 326,
      "start": 1665.8999999999999,
      "end": 1668.02,
      "text": "I don't believe we're basically at AGI."
    },
    {
      "id": 327,
      "start": 1668.22,
      "end": 1673.0,
      "text": "I think if you had the country of geniuses in a data center, if your company didn't adopt the country of geniuses in a data center."
    },
    {
      "id": 328,
      "start": 1673.0,
      "end": 1675.6599999999999,
      "text": "If you had the country of geniuses in a data center, we would know it."
    },
    {
      "id": 329,
      "start": 1675.74,
      "end": 1675.94,
      "text": "Right."
    },
    {
      "id": 330,
      "start": 1675.94,
      "end": 1678.8200000000002,
      "text": "We would know it if you had the country of geniuses in a data center."
    },
    {
      "id": 331,
      "start": 1679.02,
      "end": 1681.1000000000001,
      "text": "Like, everyone in this room would know it."
    },
    {
      "id": 332,
      "start": 1681.52,
      "end": 1683.38,
      "text": "Everyone in Washington would know it."
    },
    {
      "id": 333,
      "start": 1683.68,
      "end": 1687.72,
      "text": "Like, you know, people in rural parts might not know it."
    },
    {
      "id": 334,
      "start": 1687.8600000000001,
      "end": 1690.4,
      "text": "But, like, we would know it."
    },
    {
      "id": 335,
      "start": 1690.48,
      "end": 1691.44,
      "text": "We don't have that now."
    },
    {
      "id": 336,
      "start": 1691.54,
      "end": 1692.3200000000002,
      "text": "That's very clear."
    },
    {
      "id": 337,
      "start": 1692.32,
      "end": 1699.56,
      "text": "As Dario was hinting at, to get generalization, you need to train across a wide variety of realistic tasks and environments."
    },
    {
      "id": 338,
      "start": 1699.98,
      "end": 1705.52,
      "text": "For example, with a sales agent, the hardest part isn't teaching it to mash buttons in a specific database in Salesforce."
    },
    {
      "id": 339,
      "start": 1705.86,
      "end": 1708.8,
      "text": "It's training the agent's judgment across ambiguous situations."
    },
    {
      "id": 340,
      "start": 1708.8,
      "end": 1712.76,
      "text": "How do you sort through a database with thousands of leads to figure out which ones are hot?"
    },
    {
      "id": 341,
      "start": 1713.2,
      "end": 1714.3799999999999,
      "text": "How do you actually reach out?"
    },
    {
      "id": 342,
      "start": 1714.78,
      "end": 1715.8999999999999,
      "text": "What do you do when you get ghosted?"
    },
    {
      "id": 343,
      "start": 1716.12,
      "end": 1723.76,
      "text": "When an AI lab wanted to train a sales agent, Labelbox brought in dozens of Fortune 500 salespeople to build a bunch of different RL environments."
    },
    {
      "id": 344,
      "start": 1724.1399999999999,
      "end": 1730.22,
      "text": "They created thousands of scenarios where the sales agent had to engage with a potential customer, which was role played by a second AI."
    },
    {
      "id": 345,
      "start": 1730.62,
      "end": 1733.8999999999999,
      "text": "Labelbox made sure that this customer AI had a few different personas."
    },
    {
      "id": 346,
      "start": 1734.28,
      "end": 1737.32,
      "text": "Because when you cold call, you have no idea who's going to be on the other end."
    },
    {
      "id": 347,
      "start": 1737.32,
      "end": 1740.08,
      "text": "You need to be able to deal with a whole range of possibilities."
    },
    {
      "id": 348,
      "start": 1740.7,
      "end": 1748.26,
      "text": "Labelbox's sales experts monitored these conversations turn by turn, tweaking the role-playing agent to ensure it did the kinds of things an actual customer would do."
    },
    {
      "id": 349,
      "start": 1748.56,
      "end": 1751.1799999999998,
      "text": "Labelbox could iterate faster than anybody else in the industry."
    },
    {
      "id": 350,
      "start": 1751.5,
      "end": 1754.0,
      "text": "This is super important because RL is an empirical science."
    },
    {
      "id": 351,
      "start": 1754.32,
      "end": 1755.1799999999998,
      "text": "It's not a solved problem."
    },
    {
      "id": 352,
      "start": 1755.58,
      "end": 1759.34,
      "text": "Labelbox has a bunch of tools for monitoring agent performance in real time."
    },
    {
      "id": 353,
      "start": 1759.6399999999999,
      "end": 1766.8,
      "text": "This lets their experts keep coming up with tasks so that the model stays in the right distribution of difficulty and gets the optimal reward signal during training."
    },
    {
      "id": 354,
      "start": 1766.8,
      "end": 1769.1599999999999,
      "text": "Labelbox can do this sort of thing in almost every domain."
    },
    {
      "id": 355,
      "start": 1769.54,
      "end": 1773.0,
      "text": "They've got hedge fund managers, radiologists, even airline pilots."
    },
    {
      "id": 356,
      "start": 1773.4199999999998,
      "end": 1775.96,
      "text": "So whatever you're working on, Labelbox can help."
    },
    {
      "id": 357,
      "start": 1776.3999999999999,
      "end": 1780.1399999999999,
      "text": "Learn more at labelbox.com slash thorkhesh."
    },
    {
      "id": 358,
      "start": 1780.14,
      "end": 1790.16,
      "text": "Coming back to concrete predictions, because I think because there's so many different things to disambiguate, it can be easy to talk past each other when we're talking about capabilities."
    },
    {
      "id": 359,
      "start": 1790.16,
      "end": 1795.72,
      "text": "So, for example, when I interviewed you three years ago, I asked you a prediction about what should we expect three years from now."
    },
    {
      "id": 360,
      "start": 1796.0800000000002,
      "end": 1797.5600000000002,
      "text": "I think you were right."
    },
    {
      "id": 361,
      "start": 1797.56,
      "end": 1805.22,
      "text": "So you said we should expect systems, which if you talk to them for the course of an hour, it's hard to tell them apart from a generally well-educated human."
    },
    {
      "id": 362,
      "start": 1805.46,
      "end": 1805.6599999999999,
      "text": "Yes."
    },
    {
      "id": 363,
      "start": 1805.74,
      "end": 1806.6,
      "text": "I think you were right about that."
    },
    {
      "id": 364,
      "start": 1806.86,
      "end": 1815.6599999999999,
      "text": "And I think spiritually I feel unsatisfied because my internal expectation was that such a system could automate large parts of white-collar work."
    },
    {
      "id": 365,
      "start": 1816.0,
      "end": 1819.52,
      "text": "And so it might be more productive to talk about the actual end capabilities."
    },
    {
      "id": 366,
      "start": 1819.62,
      "end": 1820.24,
      "text": "You want such a system."
    },
    {
      "id": 367,
      "start": 1820.24,
      "end": 1825.64,
      "text": "So I will basically tell you where I think we are."
    },
    {
      "id": 368,
      "start": 1826.0,
      "end": 1831.24,
      "text": "But let me ask you in a very specific question so that we can figure out exactly what kinds of capabilities we should expect soon."
    },
    {
      "id": 369,
      "start": 1831.6,
      "end": 1839.56,
      "text": "So maybe I'll ask about it in the context of a job I understand well, not because it's the most relevant job, but just because I can evaluate the claims about it."
    },
    {
      "id": 370,
      "start": 1840.7,
      "end": 1841.78,
      "text": "Take video editors, right?"
    },
    {
      "id": 371,
      "start": 1841.78,
      "end": 1850.36,
      "text": "I have video editors, and part of their job involves learning about our audience's preferences, learning about my preferences and tastes and the different trade-offs we have."
    },
    {
      "id": 372,
      "start": 1850.54,
      "end": 1854.3799999999999,
      "text": "And just over the course of many months building up this understanding of context."
    },
    {
      "id": 373,
      "start": 1854.98,
      "end": 1863.3,
      "text": "And so the skill and ability they have six months into the job, a model that can pick up that skill on the job, on the fly, when should we expect such an AI system?"
    },
    {
      "id": 374,
      "start": 1863.3,
      "end": 1863.7,
      "text": "Yeah."
    },
    {
      "id": 375,
      "start": 1863.7,
      "end": 1863.78,
      "text": "Yeah."
    },
    {
      "id": 376,
      "start": 1864.0,
      "end": 1868.28,
      "text": "So I guess what you're talking about is like, you know, we're doing this interview for three hours."
    },
    {
      "id": 377,
      "start": 1868.28,
      "end": 1871.86,
      "text": "And then like, you know, someone's going to come in, someone's going to edit it."
    },
    {
      "id": 378,
      "start": 1871.9199999999998,
      "end": 1878.58,
      "text": "They're going to be like, oh, you know, you know, I don't know, Dario like, you know, scratched his head and, you know, we could edit that out."
    },
    {
      "id": 379,
      "start": 1878.8,
      "end": 1884.74,
      "text": "And, you know, there was this like long discussion that like is less interesting to people."
    },
    {
      "id": 380,
      "start": 1884.74,
      "end": 1887.7,
      "text": "And then, you know, then there's other thing that's like more interesting to people."
    },
    {
      "id": 381,
      "start": 1887.86,
      "end": 1890.54,
      "text": "So, you know, let's kind of make this edit."
    },
    {
      "id": 382,
      "start": 1890.72,
      "end": 1895.4,
      "text": "So, you know, I think the country of geniuses in a data center will be able to do that."
    },
    {
      "id": 383,
      "start": 1895.5,
      "end": 1900.5,
      "text": "The way it will be able to do that is, you know, it will have general control of a computer screen, right?"
    },
    {
      "id": 384,
      "start": 1900.56,
      "end": 1909.54,
      "text": "Like, you know, and you'll be able to feed this in and it'll be able to also use the computer screen to like go on the web, look at all your previous interviews."
    },
    {
      "id": 385,
      "start": 1909.54,
      "end": 1919.58,
      "text": "Like look at what people are saying on Twitter in response to your interviews, like talk to you, ask you questions, talk to your staff, look at the history of kind of edits, edits that you did."
    },
    {
      "id": 386,
      "start": 1919.6599999999999,
      "end": 1921.7,
      "text": "And from that, like do the job."
    },
    {
      "id": 387,
      "start": 1921.82,
      "end": 1921.8999999999999,
      "text": "Yeah."
    },
    {
      "id": 388,
      "start": 1922.34,
      "end": 1924.36,
      "text": "So I think that's dependent on several things."
    },
    {
      "id": 389,
      "start": 1924.48,
      "end": 1925.3,
      "text": "One that's dependent."
    },
    {
      "id": 390,
      "start": 1925.78,
      "end": 1934.8,
      "text": "And I think this is one of the things that's actually blocking deployment, getting to the point on computer use where the models are really masters at using the computer."
    },
    {
      "id": 391,
      "start": 1934.8,
      "end": 1935.28,
      "text": "Right."
    },
    {
      "id": 392,
      "start": 1935.28,
      "end": 1940.74,
      "text": "And, you know, we've seen this climb in benchmarks and benchmarks are always, you know, imperfect measures."
    },
    {
      "id": 393,
      "start": 1940.74,
      "end": 1953.58,
      "text": "But like, you know, OS world is, you know, went from, you know, like 5%, you know, like I think when we first released, you know, computer use like a year and a quarter ago, it was like maybe 15%."
    },
    {
      "id": 394,
      "start": 1953.58,
      "end": 1954.58,
      "text": "I don't remember exactly."
    },
    {
      "id": 395,
      "start": 1954.8799999999999,
      "end": 1958.2,
      "text": "But we've climbed from that to like 65% or 70%."
    },
    {
      "id": 396,
      "start": 1958.2,
      "end": 1962.06,
      "text": "And, you know, there may be harder measures as well."
    },
    {
      "id": 397,
      "start": 1962.42,
      "end": 1965.92,
      "text": "But I think computer use has to pass a point of reliability."
    },
    {
      "id": 398,
      "start": 1966.42,
      "end": 1968.48,
      "text": "Can I just ask a follow up on that before we move on to the next point?"
    },
    {
      "id": 399,
      "start": 1969.1000000000001,
      "end": 1973.06,
      "text": "I often, for years, I've been trying to build different internal LLM tools for myself."
    },
    {
      "id": 400,
      "start": 1973.06,
      "end": 1980.52,
      "text": "And I often I have these text in, text out tasks, which should be dead center in the repertoire of these models."
    },
    {
      "id": 401,
      "start": 1980.52,
      "end": 1987.42,
      "text": "And yet I still hire humans to do them just because it's if it's something like make identify what the best clips would be in this transcript."
    },
    {
      "id": 402,
      "start": 1987.52,
      "end": 1989.32,
      "text": "And maybe they'll do like a seven out of 10 job at them."
    },
    {
      "id": 403,
      "start": 1989.54,
      "end": 1995.54,
      "text": "But there's not this ongoing way I can engage with them to help them get better at the job the way I could with a human employee."
    },
    {
      "id": 404,
      "start": 1995.74,
      "end": 2003.44,
      "text": "And so that missing ability, even if you saw computer use, would still block my ability to like offload an actual job to them."
    },
    {
      "id": 405,
      "start": 2003.44,
      "end": 2011.14,
      "text": "Again, there's there's this gets back to what to kind of to kind of what we were talking about before with learning on the job where it's it's very interesting."
    },
    {
      "id": 406,
      "start": 2011.24,
      "end": 2022.98,
      "text": "You know, I think I think with the coding agents, like I don't think people would say that learning on the job is what is what is, you know, preventing the coding agents from like, you know, doing everything end to end."
    },
    {
      "id": 407,
      "start": 2023.04,
      "end": 2024.8600000000001,
      "text": "Like they keep they keep getting better."
    },
    {
      "id": 408,
      "start": 2025.26,
      "end": 2028.76,
      "text": "We have engineers at Anthropic who like don't write any code."
    },
    {
      "id": 409,
      "start": 2028.76,
      "end": 2036.78,
      "text": "And when I look at the productivity to your to your previous question, you know, we have folks who say this this GPU kernel, this chip."
    },
    {
      "id": 410,
      "start": 2036.96,
      "end": 2037.84,
      "text": "I used to write it myself."
    },
    {
      "id": 411,
      "start": 2037.84,
      "end": 2038.96,
      "text": "I just have Claude do it."
    },
    {
      "id": 412,
      "start": 2039.04,
      "end": 2042.36,
      "text": "And so there's this there's this enormous improvement in productivity."
    },
    {
      "id": 413,
      "start": 2042.92,
      "end": 2055.14,
      "text": "And I don't know, like when I see Claude code, like familiarity with the code base or like, you know, or or a feeling that the model hasn't worked at the company for a year."
    },
    {
      "id": 414,
      "start": 2055.4,
      "end": 2058.06,
      "text": "That's not high up on the list of complaints I see."
    },
    {
      "id": 415,
      "start": 2058.06,
      "end": 2062.32,
      "text": "And so I think what I'm saying is we're like we're kind of taking a different path."
    },
    {
      "id": 416,
      "start": 2062.64,
      "end": 2077.4,
      "text": "Don't you think with coding that's because there is an external scaffold of memory which exists instantiated in the code base, which I don't know how many other jobs have coding made fast progress precisely because it has its unique advantage that other economic activity doesn't."
    },
    {
      "id": 417,
      "start": 2077.4,
      "end": 2087.54,
      "text": "But but but when you say that what you're what you're implying is that by reading the code base into the context, I have everything that the human needed to learn on the job."
    },
    {
      "id": 418,
      "start": 2087.54,
      "end": 2099.0,
      "text": "So that would be an example of whether it's written or not, whether it's available or not, a case where everything you needed to know you got from the context window."
    },
    {
      "id": 419,
      "start": 2099.0,
      "end": 2103.2599999999998,
      "text": "Right. And that and that what we think of as learning like, oh, man, I started this job."
    },
    {
      "id": 420,
      "start": 2103.38,
      "end": 2105.66,
      "text": "It's going to take me six months to understand the code base."
    },
    {
      "id": 421,
      "start": 2105.8,
      "end": 2107.24,
      "text": "The model just did it in the context."
    },
    {
      "id": 422,
      "start": 2107.24,
      "end": 2113.56,
      "text": "Yeah, I honestly don't know how to think about this because there are people who qualitative report what you're saying."
    },
    {
      "id": 423,
      "start": 2113.56,
      "end": 2116.88,
      "text": "There was a meter study I'm sure you saw last year."
    },
    {
      "id": 424,
      "start": 2116.88,
      "end": 2117.18,
      "text": "Yes."
    },
    {
      "id": 425,
      "start": 2117.34,
      "end": 2125.98,
      "text": "Where they had experienced developers try to close pull request in repositories that they were familiar with."
    },
    {
      "id": 426,
      "start": 2126.14,
      "end": 2129.14,
      "text": "And those developers reported an uplift."
    },
    {
      "id": 427,
      "start": 2129.4,
      "end": 2131.5,
      "text": "They reported that they felt more productive with the use of these models."
    },
    {
      "id": 428,
      "start": 2131.54,
      "end": 2136.2599999999998,
      "text": "But in fact, if you look at their output and how much was actually merged back in, there's a 20 percent downlift."
    },
    {
      "id": 429,
      "start": 2136.52,
      "end": 2137.98,
      "text": "They were less productive as a result of these models."
    },
    {
      "id": 430,
      "start": 2137.98,
      "end": 2146.64,
      "text": "And so I'm trying to square the qualitative feeling that people feel with these models versus one, in a macro level, where are all these where is this like renaissance of software?"
    },
    {
      "id": 431,
      "start": 2146.84,
      "end": 2151.1,
      "text": "And two, when people do these independent evaluations, why are we not seeing the."
    },
    {
      "id": 432,
      "start": 2151.34,
      "end": 2151.96,
      "text": "Yeah. So."
    },
    {
      "id": 433,
      "start": 2152.18,
      "end": 2153.14,
      "text": "Productive benefits that we would expect."
    },
    {
      "id": 434,
      "start": 2153.28,
      "end": 2155.76,
      "text": "Within Anthropic, this is just really unambiguous."
    },
    {
      "id": 435,
      "start": 2155.76,
      "end": 2156.1,
      "text": "Right."
    },
    {
      "id": 436,
      "start": 2156.3,
      "end": 2166.18,
      "text": "We're under an incredible amount of commercial pressure and make it even hard, harder for ourselves because we have all the safety stuff we do that I think we do more than than other companies."
    },
    {
      "id": 437,
      "start": 2166.18,
      "end": 2174.12,
      "text": "So like the the the pressure to survive economically while also keeping our values is is just incredible."
    },
    {
      "id": 438,
      "start": 2174.12,
      "end": 2177.7,
      "text": "Right. We're trying to keep this 10x revenue curve going."
    },
    {
      "id": 439,
      "start": 2177.9199999999996,
      "end": 2180.62,
      "text": "There's like there is zero time for bullshit."
    },
    {
      "id": 440,
      "start": 2180.8599999999997,
      "end": 2189.2599999999998,
      "text": "There is zero time for feeling like we're productive when we're not like these tools make us a lot more productive."
    },
    {
      "id": 441,
      "start": 2189.62,
      "end": 2193.8999999999996,
      "text": "Like why why do you think we're concerned about competitors using the tools?"
    },
    {
      "id": 442,
      "start": 2193.9,
      "end": 2198.3,
      "text": "Because we think we're ahead of the competitors and like we don't we don't want to sell."
    },
    {
      "id": 443,
      "start": 2198.3,
      "end": 2206.38,
      "text": "We we we wouldn't be going through all this trouble if this was secretly reducing reducing our productivity."
    },
    {
      "id": 444,
      "start": 2206.38,
      "end": 2211.76,
      "text": "Like we see the end productivity every few months in the form of model launches."
    },
    {
      "id": 445,
      "start": 2211.76,
      "end": 2214.12,
      "text": "Like there's no kidding yourself about this."
    },
    {
      "id": 446,
      "start": 2214.12,
      "end": 2215.7200000000003,
      "text": "Like the models make you more productive."
    },
    {
      "id": 447,
      "start": 2215.72,
      "end": 2222.12,
      "text": "One that people feeling like they're more productive is qualitatively predicted by studies like this."
    },
    {
      "id": 448,
      "start": 2222.12,
      "end": 2225.56,
      "text": "But two if I just look at the end output obviously you guys are making fast progress."
    },
    {
      "id": 449,
      "start": 2225.56,
      "end": 2232.6,
      "text": "But the fact you know the the the idea was supposed to be with recursive self-improvement is that you make a better AI."
    },
    {
      "id": 450,
      "start": 2232.6,
      "end": 2235.0,
      "text": "The AI helps you build a better next AI etc etc."
    },
    {
      "id": 451,
      "start": 2235.0,
      "end": 2242.6,
      "text": "And what I see instead if I look at the you open AI deep mind is that people are just shifting around the podium every few months."
    },
    {
      "id": 452,
      "start": 2242.6,
      "end": 2244.84,
      "text": "And maybe you think that stops because you you've won or whatever."
    },
    {
      "id": 453,
      "start": 2244.84,
      "end": 2256.12,
      "text": "But but why are we not seeing the person with the best coding model have this lasting advantage if in fact there are these enormous productivity gains from the last coding model."
    },
    {
      "id": 454,
      "start": 2256.12,
      "end": 2276.44,
      "text": "So no no no I I mean I mean I mean I think it's all like my my model of the situation is there's there's an advantage that's gradually growing like I would say right now the coding models give maybe I don't know a a like 15 maybe 20 percent total factor speed up like that's my view."
    },
    {
      "id": 455,
      "start": 2276.44,
      "end": 2283.4,
      "text": "Um uh and six months ago it was maybe five percent and so and so it didn't matter like five percent doesn't register."
    },
    {
      "id": 456,
      "start": 2283.4,
      "end": 2289.0,
      "text": "It's now just getting to the point where it's like one of several factors that that kind of matters."
    },
    {
      "id": 457,
      "start": 2289.32,
      "end": 2291.64,
      "text": "And and that's gonna that's gonna keep speeding up."
    },
    {
      "id": 458,
      "start": 2291.64,
      "end": 2298.52,
      "text": "And so I think six months ago like you know there were several there were several companies that were at roughly the same point."
    },
    {
      "id": 459,
      "start": 2299.08,
      "end": 2305.56,
      "text": "Because uh you know this this wasn't uh this wasn't a notable factor, but I think it's starting to speed up more and more."
    },
    {
      "id": 460,
      "start": 2305.56,
      "end": 2311.0,
      "text": "I you know I I would I would also say there are multiple companies that you know write models that are used for code."
    },
    {
      "id": 461,
      "start": 2311.0,
      "end": 2320.36,
      "text": "And you know we're not perfectly good at you know preventing some of these other companies from from from using from from from kind of using our models internally."
    },
    {
      "id": 462,
      "start": 2320.36,
      "end": 2329.48,
      "text": "Um so uh you know I think I think everything we're kind of everything we're seeing is consistent with this kind of um this kind of snowball model."
    },
    {
      "id": 463,
      "start": 2329.48,
      "end": 2340.84,
      "text": "Where where you know there's no hard again my my my my my theme in all of this is like all of this is soft takeoff like soft smooth exponentials."
    },
    {
      "id": 464,
      "start": 2340.84,
      "end": 2342.84,
      "text": "Although the exponentials are relatively steep."
    },
    {
      "id": 465,
      "start": 2342.84,
      "end": 2351.0,
      "text": "And so and so we're seeing this snowball gather momentum where it's like 10 percent 20 percent 25 percent you know for 40 percent."
    },
    {
      "id": 466,
      "start": 2351.0,
      "end": 2357.1600000000003,
      "text": "And as you go yeah and all's all you have to get all the like things that are preventing you from from closing the loop out of the way."
    },
    {
      "id": 467,
      "start": 2357.1600000000003,
      "end": 2360.6800000000003,
      "text": "But like this is one of the biggest priorities within anthropic."
    },
    {
      "id": 468,
      "start": 2360.6800000000003,
      "end": 2369.32,
      "text": "Um stepping back I think before in the stack we were talking about um well when do we get this on the job learning."
    },
    {
      "id": 469,
      "start": 2369.32,
      "end": 2374.2000000000003,
      "text": "And it seems like the coding the point you were making the coding thing is we actually don't need on the job learning."
    },
    {
      "id": 470,
      "start": 2374.2000000000003,
      "end": 2376.44,
      "text": "Uh that you can have tremendous productivity improvements."
    },
    {
      "id": 471,
      "start": 2376.44,
      "end": 2381.7200000000003,
      "text": "You can have potentially trillions of dollars of revenue for AI companies without this basic human ability."
    },
    {
      "id": 472,
      "start": 2381.7200000000003,
      "end": 2383.1600000000003,
      "text": "Maybe that's not your claim you should clarify."
    },
    {
      "id": 473,
      "start": 2383.1600000000003,
      "end": 2387.1600000000003,
      "text": "Um but without this basic human ability to learn on the job."
    },
    {
      "id": 474,
      "start": 2387.1600000000003,
      "end": 2394.6800000000003,
      "text": "But I just look at like in in most domains of economic activity people say I hired somebody they weren't that useful for the first few months."
    },
    {
      "id": 475,
      "start": 2394.68,
      "end": 2400.2,
      "text": "And then over time they built up the context understanding it's actually hard to define what we're talking about here."
    },
    {
      "id": 476,
      "start": 2400.2,
      "end": 2404.7599999999998,
      "text": "But they got something and then now now they're they're power horse and they're so valuable to us."
    },
    {
      "id": 477,
      "start": 2405.3199999999997,
      "end": 2413.16,
      "text": "And if AI doesn't develop this ability to learn on the fly I'm not I'm a bit skeptical that we're going to see huge changes to the world."
    },
    {
      "id": 478,
      "start": 2413.16,
      "end": 2413.7999999999997,
      "text": "Yeah."
    },
    {
      "id": 479,
      "start": 2413.7999999999997,
      "end": 2416.7599999999998,
      "text": "So I think I think I think two things here right."
    },
    {
      "id": 480,
      "start": 2416.7599999999998,
      "end": 2419.48,
      "text": "There's the state of the technology right now."
    },
    {
      "id": 481,
      "start": 2419.48,
      "end": 2421.96,
      "text": "Um which is again we have these two stages."
    },
    {
      "id": 482,
      "start": 2421.96,
      "end": 2429.64,
      "text": "We have the pre-training and RL stage where you throw you throw a bunch of data and tasks into the models and then they generalize."
    },
    {
      "id": 483,
      "start": 2429.64,
      "end": 2438.36,
      "text": "So it's like learning but it's like learning from more data and and not you know not learning over kind of one human or one models lifetime."
    },
    {
      "id": 484,
      "start": 2438.36,
      "end": 2442.52,
      "text": "So again this is situated between evolution and and and human learning."
    },
    {
      "id": 485,
      "start": 2442.52,
      "end": 2445.16,
      "text": "But once you learn all those skills you have them."
    },
    {
      "id": 486,
      "start": 2445.16,
      "end": 2455.8799999999997,
      "text": "And and just like with pre-training just how the models know more you know if if I look at a pre-trained model you know it knows more about the history of samurai in Japan than I do."
    },
    {
      "id": 487,
      "start": 2455.8799999999997,
      "end": 2458.2,
      "text": "It knows more about baseball than I do."
    },
    {
      "id": 488,
      "start": 2458.2,
      "end": 2466.12,
      "text": "It knows you know it knows more about you know low pass filters and electronics than you know all all of these things."
    },
    {
      "id": 489,
      "start": 2466.12,
      "end": 2468.8399999999997,
      "text": "It's knowledge is way broader than mine."
    },
    {
      "id": 490,
      "start": 2468.8399999999997,
      "end": 2477.72,
      "text": "So I think I think even even just that um you know may get us to the point where the models are better at you know kind of better at everything."
    },
    {
      "id": 491,
      "start": 2477.72,
      "end": 2481.96,
      "text": "And then we also have again just with scaling the kind of existing setup."
    },
    {
      "id": 492,
      "start": 2481.96,
      "end": 2487.48,
      "text": "We have the in context learning which I would describe as kind of like human on the job learning."
    },
    {
      "id": 493,
      "start": 2487.48,
      "end": 2497.48,
      "text": "But like a little weaker and a little short term like you look at in context learning the you give the model a bunch of examples it does get it there's real learning that happens in context."
    },
    {
      "id": 494,
      "start": 2497.48,
      "end": 2502.44,
      "text": "And like a million tokens is a lot that's that's you know that can be days of human learning right."
    },
    {
      "id": 495,
      "start": 2502.44,
      "end": 2511.08,
      "text": "You know if you think about the model you know you know kind of read reading that being a million words you know it you know it takes me how long would it take me to read a million."
    },
    {
      "id": 496,
      "start": 2511.08,
      "end": 2523.08,
      "text": "I mean you know like days or weeks at least um uh so you have these two things and and I think these two these two things within the existing paradigm may just be enough to get you the country of geniuses in the data center."
    },
    {
      "id": 497,
      "start": 2523.08,
      "end": 2527.0,
      "text": "I don't know for sure but I think they're going to get you a large fraction of it."
    },
    {
      "id": 498,
      "start": 2527.0,
      "end": 2534.84,
      "text": "There may be gaps but I I certainly think just as things are this I believe is enough to generate trillions of dollars of revenue."
    },
    {
      "id": 499,
      "start": 2534.84,
      "end": 2536.68,
      "text": "That's one that's all one."
    },
    {
      "id": 500,
      "start": 2536.68,
      "end": 2543.08,
      "text": "Two is this idea of continual learning this idea of a single model learning on the job."
    },
    {
      "id": 501,
      "start": 2544.2,
      "end": 2552.6,
      "text": "I think we're working on that too and I think there's a good chance that in the next year or two we also make we also solve that."
    },
    {
      "id": 502,
      "start": 2552.6,
      "end": 2558.6,
      "text": "Um I again I I I you know I think you get most of the way there without it."
    },
    {
      "id": 503,
      "start": 2558.6,
      "end": 2565.08,
      "text": "I think the trillions of dollars of of you know the the I think the trillions of dollars a year market."
    },
    {
      "id": 504,
      "start": 2565.08,
      "end": 2571.72,
      "text": "Maybe all the national security implications and the safety implications that I wrote about in adolescence of technology can happen without it."
    },
    {
      "id": 505,
      "start": 2571.72,
      "end": 2582.92,
      "text": "But I I I also think we and I imagine others are working on it and I think there's a good chance that that you know that we get there within the next year or two."
    },
    {
      "id": 506,
      "start": 2582.92,
      "end": 2583.96,
      "text": "There are a bunch of ideas."
    },
    {
      "id": 507,
      "start": 2583.96,
      "end": 2589.08,
      "text": "I won't go into all of them in detail but um you know one is just make the context longer."
    },
    {
      "id": 508,
      "start": 2589.08,
      "end": 2592.36,
      "text": "There's there's nothing preventing longer context from working."
    },
    {
      "id": 509,
      "start": 2592.36,
      "end": 2601.48,
      "text": "You just have to train at longer context and then learn to to serve them at inference and both of those are engineering problems that we are working on and that I would assume others are working on as well."
    },
    {
      "id": 510,
      "start": 2601.48,
      "end": 2611.48,
      "text": "Yeah so this context line increase it seemed like there was a period from 2020 to 2023 where from gpd3 to gpd4 turbo there was an increase from like 2000 context lines to 128k."
    },
    {
      "id": 511,
      "start": 2611.48,
      "end": 2616.36,
      "text": "I feel like for the next for the two-ish years since then we've been in the same-ish ballpark."
    },
    {
      "id": 512,
      "start": 2616.36,
      "end": 2616.76,
      "text": "Yeah."
    },
    {
      "id": 513,
      "start": 2616.76,
      "end": 2625.72,
      "text": "And when model context lines get much longer than that people report qualitative degradation in the ability of the model to consider that full context."
    },
    {
      "id": 514,
      "start": 2625.72,
      "end": 2634.2,
      "text": "Um so I'm curious what you're internally seeing that makes you think like oh 10 million context 100 million context to get human like six months learning billion billion context."
    },
    {
      "id": 515,
      "start": 2634.2,
      "end": 2651.08,
      "text": "This isn't a research problem this is a this is an engineering and inference problem right if you want to serve long context you have to like store your entire kv cache you have to you know um uh you know it's it's it's it's difficult to store all the memory in the gpus to juggle the memory around."
    },
    {
      "id": 516,
      "start": 2651.08,
      "end": 2663.08,
      "text": "I don't even know the detail you know at this point this is at a level of detail that that that i'm no longer able to follow although you know i knew it in the gpd3 era of like you know these are the weights these are the activations you have to store."
    },
    {
      "id": 517,
      "start": 2663.08,
      "end": 2683.0,
      "text": "Um uh but you know you know these days the whole thing is flipped because we have moe models and and and kind of all of that but um uh and and this degradation you're talking about like again without getting too specific like a question i would ask is like there's two things there's the context length you train at and there's a context length that you serve at."
    },
    {
      "id": 518,
      "start": 2683.0,
      "end": 2692.0,
      "text": "If you train at a small context length and then try to serve at a long context length like maybe you get these degradations it's better than nothing you might still offer it but you get these degradations."
    },
    {
      "id": 519,
      "start": 2692.0,
      "end": 2696.0,
      "text": "And maybe it's harder to train at a long context length yeah so you know there's there's a lot."
    },
    {
      "id": 520,
      "start": 2696.0,
      "end": 2720.0,
      "text": "I i want to at the same time ask about like maybe some rabbit holes of like well wouldn't you expect that if you had to train on longer context length that would mean that um you're able to get sort of like less samples in for the same amount of compute but before maybe it's not worth diving deep on that i want to get an answer to the bigger picture question which is like okay so um i don't feel a preference for a human editor."
    },
    {
      "id": 521,
      "start": 2720.0,
      "end": 2726.0,
      "text": "A preference for a human editor that's been working for me for six months versus an ai that's been working with me for six months."
    },
    {
      "id": 522,
      "start": 2726.0,
      "end": 2730.0,
      "text": "What year do you predict that that will be the case?"
    },
    {
      "id": 523,
      "start": 2730.0,
      "end": 2739.0,
      "text": "I my i mean you know my guess for that is you know there's there's a lot of problems that are basically like we can do this when we have the country of geniuses in a data center."
    },
    {
      "id": 524,
      "start": 2739.0,
      "end": 2759.0,
      "text": "Um and so you know my my my my picture for that is you know again if you if you if you if you know if you made me guess it's like one to two years maybe one to three years it's really hard to tell i have a i have a strong view 99 95 percent that like all this will happen in 10 years like that's i think that's just a super safe bet yeah."
    },
    {
      "id": 525,
      "start": 2759.0,
      "end": 2766.0,
      "text": "And then i have a hunch this is more like a 50 50 thing that it's going to be more like one to two maybe more like one to three."
    },
    {
      "id": 526,
      "start": 2766.0,
      "end": 2772.0,
      "text": "So one to three years the country of geniuses um and the slightly less economically valuable task of editing videos."
    },
    {
      "id": 527,
      "start": 2772.0,
      "end": 2782.0,
      "text": "It seems pretty economically valuable let me tell you it's just there are a lot of use cases like that right there are a lot of similar exactly so you're predicting that within one to three years."
    },
    {
      "id": 528,
      "start": 2782.0,
      "end": 2796.0,
      "text": "Um and then generally anthropic has predicted that by late 26 early 27 we will have ai systems that are quote um have the ability to navigate interfaces available to humans doing digital work today intellectual capabilities mashing or exceeding that of noble prize winners."
    },
    {
      "id": 529,
      "start": 2796.0,
      "end": 2809.0,
      "text": "And the ability to interface with the physical world and then you give an interview two months ago with deal book where you're emphasizing your um your company's more responsible compute scaling as compared to your competitors."
    },
    {
      "id": 530,
      "start": 2809.0,
      "end": 2825.0,
      "text": "And i'm trying to square these two views where if you really believe that we're gonna have a country of geniuses you you want as big a data center as you can get there's no reason to slow down the tam of a noble prize winner that is actually can do everything a prize winner can do is like treat it."
    },
    {
      "id": 531,
      "start": 2825.0,
      "end": 2835.0,
      "text": "And so i'm trying to square this conservatism uh which seems rational if you have more moderate timelines with your stated views about ai progress."
    },
    {
      "id": 532,
      "start": 2835.0,
      "end": 2842.0,
      "text": "Yeah so so it actually all fits together and we go back to this fast but not infinitely fast diffusion."
    },
    {
      "id": 533,
      "start": 2842.0,
      "end": 2859.0,
      "text": "So like let's say that we're making progress at this rate um you know the the technology is making progress this fast again i have you know very high conviction that like it's going you know the the you know we're we're gonna get there within within a few years."
    },
    {
      "id": 534,
      "start": 2859.0,
      "end": 2873.0,
      "text": "So a little uncertainty on the technical side but like you know pretty pretty strong confidence that it won't be off by much what i'm less certain about is again the economic diffusion side."
    },
    {
      "id": 535,
      "start": 2873.0,
      "end": 2883.0,
      "text": "Like i really do believe that we could have models that are a country of geniuses 100 country of geniuses in the data center in one to two years."
    },
    {
      "id": 536,
      "start": 2883.0,
      "end": 2893.0,
      "text": "One question is how many years after that do the trillions in you know do the trillions in revenue start rolling in."
    },
    {
      "id": 537,
      "start": 2893.0,
      "end": 2909.0,
      "text": "Um i don't think it's guaranteed that it's going to be immediate um you know i think it could be um one year it could be two years i could even stretch it to five years although i'm like i'm skeptical of that."
    },
    {
      "id": 538,
      "start": 2909.0,
      "end": 2916.0,
      "text": "And so we have this uncertainty which is even if the technology goes as fast as i suspect that it will."
    },
    {
      "id": 539,
      "start": 2917.0,
      "end": 2928.0,
      "text": "We we don't know exactly how fast it's going to drive revenue we we know it's coming but with the way you buy these data centers if you're off by a couple years that can be ruinous."
    },
    {
      "id": 540,
      "start": 2928.0,
      "end": 2938.0,
      "text": "It is just like how i wrote you know in machines of loving grace i said look i think we might get this powerful ai this country geniuses in the data center that description you gave comes from the machines of loving grace."
    },
    {
      "id": 541,
      "start": 2938.0,
      "end": 2947.0,
      "text": "I said we'll get that 2026 maybe 2027 again that is that is my hunch wouldn't be surprised if i'm off by a year or two but like that is my hunch."
    },
    {
      "id": 542,
      "start": 2947.0,
      "end": 2967.0,
      "text": "Let's say that happens that's the starting gun how long does it take to cure all the diseases right that's that's one of the ways that like drives a huge amount of of of of economic value right like you cure you cure every disease you know there's a question of how much of that goes to the pharmaceutical company to the ai company but there's an enormous consumer surplus because everyone you know every as soon as soon as possible."
    },
    {
      "id": 543,
      "start": 2967.0,
      "end": 2991.0,
      "text": "We can get access for everyone which i care about greatly we you know we cure all of these diseases how long does it take you have to do the biological discovery you have to you know you have to you know manufacture the new drug you have to you know go through the regulatory process and we saw this with like vaccines and covid right like it there's just this we got the vaccine out to everyone but it took a year and a half right."
    },
    {
      "id": 544,
      "start": 2991.0,
      "end": 3007.0,
      "text": "Right and and so my question is how long does it take to get the cure for everything which ai is the genius that can in theory invent out to everyone how long from when that ai first exists in the lab to when diseases have actually been cured for everyone."
    },
    {
      "id": 545,
      "start": 3007.0,
      "end": 3022.0,
      "text": "Right and you know we've had a polio vaccine for 50 years we're still trying to eradicate it in the most remote corners of Africa and you know the gates foundation is trying as hard as they can others are trying as hard as they can but you know that's difficult."
    },
    {
      "id": 546,
      "start": 3022.0,
      "end": 3028.0,
      "text": "That's difficult again i you know i don't expect most of the economic diffusion to be as difficult as that right that's like the most difficult case."
    },
    {
      "id": 547,
      "start": 3028.0,
      "end": 3041.0,
      "text": "But but but there's a there's a real dilemma here and and where i've settled on it is it will be it will be it will be faster than anything we've seen in the world but it still has its limits."
    },
    {
      "id": 548,
      "start": 3041.0,
      "end": 3062.0,
      "text": "And and and so then when we go to buying data centers you know you again again the curve i'm looking at is okay we you know we've had a 10x a year increase every year so beginning of this year we're looking at 10 billion in in in annual in you know rate of annualized revenue at the beginning of the year."
    },
    {
      "id": 549,
      "start": 3062.0,
      "end": 3091.82,
      "text": "We have to decide how much compute to buy um and you know it takes a year or two to actually build out the data centers to reserve the data center so basically i'm saying like in uh 2027 how much compute do i get well i could assume um uh that uh the uh revenue will continue growing 10x a year so it'll be uh one uh one uh 100 billion at the end of 2026."
    },
    {
      "id": 550,
      "start": 3092.0,
      "end": 3108.0,
      "text": "And so i could buy a trillion dollars actually would be like five trillion dollars of compute because it would be a trillion dollar a year for for five years right i could buy a trillion dollars of compute that starts at the end of 2027."
    },
    {
      "id": 551,
      "start": 3108.0,
      "end": 3133.6,
      "text": "And if my if my revenue is not a trillion dollars if it's even 800 billion there's no force on earth there's there's no hedge on earth that could stop me from going bankrupt if i if i buy that much compute and and so even though a part of my brain wonders if it's going to keep going 10x i can't buy a trillion dollars a year of compute in in in in in in in in in in in in in in in in 2027."
    },
    {
      "id": 552,
      "start": 3133.6,
      "end": 3158.02,
      "text": "And if i'm just off by a year in that rate of growth or if the the growth rate is 5x a year instead of 10x a year then then you know that you go bankrupt um and and and and so you end up in a world where you know you're supporting hundreds of billions not trillions and you accept you accept some risk that there's so much demand that you can't support the revenue."
    },
    {
      "id": 553,
      "start": 3158.02,
      "end": 3188.0,
      "text": "And you accept still some risk that you know you got it wrong and it's still so and so when i talked about behaving responsibly what i meant actually was not the absolute amount that that actually was not um you know i think it is true we're spending somewhat less than some of the other players it's actually the other things like have we been thoughtful about it or are we yoloing and saying oh we're gonna do 100 billion dollars here 100 billion dollars there i kind of get the impression that you know some of the"
    },
    {
      "id": 554,
      "start": 3188.0,
      "end": 3217.98,
      "text": "other companies have not written down the other companies have not written down the spreadsheet that they don't really understand the risks they're taking they're just kind of doing stuff because it sounds cool um uh and and we've thought carefully about it right we're an enterprise business therefore you know we can rely more on revenue it's less fickle than consumer we have better margins which is the buffer between buying too much and buying too little and so i think we bought an amount that allows us to capture pretty strong upside worlds it won't capture"
    },
    {
      "id": 555,
      "start": 3218.0,
      "end": 3230.0,
      "text": "the full 10x a year um and things would have to go pretty badly for us to be for us to be in financial trouble so i think we thought carefully and we've made that balance and and that's what i mean when i say that we're being responsible"
    },
    {
      "id": 556,
      "start": 3230.0,
      "end": 3242.0,
      "text": "okay so it seems like um it's possible that we're we actually just have different definitions of country of a genius in a data center because when i think of like actual human geniuses an actual country of human geniuses in the data center"
    },
    {
      "id": 557,
      "start": 3242.0,
      "end": 3260.0,
      "text": "i'm like i would happily buy five trillion dollars worth of compute to run uh actual culture of human geniuses in the data center so let's say jp morgan or moderna or whatever doesn't want to use them also i've got a country of geniuses they'll start their own company and if like they can't start their own company and they're bottlenecked by clinical trials it is worth stating with clinical trials like most clinical trials fail because the drug doesn't work there's not efficacy right and i make exactly that point in in machines of love and grace i say the clinical trials are going to go much faster than we're used to but not not"
    },
    {
      "id": 558,
      "start": 3260.0,
      "end": 3289.84,
      "text": "not instant not infinitely fast and then suppose it takes a year to for the clinical trials to work out so that you're getting revenue from that and can make more drugs okay well you've got a country of geniuses and you're an ai lab and you have you could use uh many more ai researchers um you also think that there's these like self-reinforcing"
    },
    {
      "id": 559,
      "start": 3289.84,
      "end": 3314.56,
      "text": "gains from you know smart people working on ai tech so like okay you can have the that's right but you can have the data center working on like ai progress is there more gains from buying like substantially more gains from buying a trillion dollars a year of compute versus 300 billion dollars a year of compute if your competitor is buying a trillion yes there is well no there's some gain but then but again there's this chance that they go bankrupt before"
    },
    {
      "id": 560,
      "start": 3314.56,
      "end": 3344.32,
      "text": "uh you know before uh you know before again if you're off by only a year you destroy yourselves that's that that's the balance we're buying a lot we're buying a hell of a lot like we're not we're you know we're buying an amount that's comparable to that that you know the the the the the biggest players in the game are buying um but but if you're asking me why why haven't we signed you know 10 10 trillion of compute starting and starting in mid 2027 first of all it can't be produced"
    },
    {
      "id": 561,
      "start": 3344.32,
      "end": 3356.0800000000004,
      "text": "there isn't that much in the world um uh but but second um what if the country of geniuses comes but it comes in mid 2028 instead of mid 2027 you go bankrupt so"
    },
    {
      "id": 562,
      "start": 3356.0800000000004,
      "end": 3363.52,
      "text": "if your projection is one to three years it seems like you should have won 10 trillion dollars to compute by um 2029"
    },
    {
      "id": 563,
      "start": 3363.52,
      "end": 3371.92,
      "text": "2020 and maybe 2020 latest like i mean you know you like are you like it seems like even in your the longest version of the timelines you state"
    },
    {
      "id": 564,
      "start": 3371.92,
      "end": 3377.12,
      "text": "the compute you are ramping up to build doesn't seem what what what what makes you think that"
    },
    {
      "id": 565,
      "start": 3377.84,
      "end": 3383.6800000000003,
      "text": "well you as you said you want the 10 trillion the human wages let's say are um on the order of 50 trillion a"
    },
    {
      "id": 566,
      "start": 3383.6800000000003,
      "end": 3388.2400000000002,
      "text": "year if you look at so so i won't i won't talk about anthropic in particular but if you talk about the"
    },
    {
      "id": 567,
      "start": 3388.2400000000002,
      "end": 3396.0,
      "text": "industry like um the amount of compute the industry hit you know the the the amount of compute the industry's"
    },
    {
      "id": 568,
      "start": 3396.0,
      "end": 3403.52,
      "text": "building this year is probably in the you know i don't know very low tens of you know call it 10 15"
    },
    {
      "id": 569,
      "start": 3403.52,
      "end": 3411.52,
      "text": "gigawatts next year i you know it goes up by roughly 3x a year so like next year's 30 or 40 gigawatts and um"
    },
    {
      "id": 570,
      "start": 3412.24,
      "end": 3420.56,
      "text": "2028 might be 100 20 29 might be like three 300 gigawatts and like each gigawatt costs like"
    },
    {
      "id": 571,
      "start": 3421.44,
      "end": 3427.04,
      "text": "um maybe 10 i mean i'm doing the math in my head but each gigawatt it costs maybe 10 billion dollars"
    },
    {
      "id": 572,
      "start": 3427.04,
      "end": 3432.7999999999997,
      "text": "you know or order 10 to 15 billion dollars a year so you know you kind of you you know you put that"
    },
    {
      "id": 573,
      "start": 3432.7999999999997,
      "end": 3436.56,
      "text": "all together and you're getting about about what you described you're getting multiple trillions a"
    },
    {
      "id": 574,
      "start": 3436.56,
      "end": 3441.2799999999997,
      "text": "year by 2028 or 2029 so you're you're getting exactly that you're getting you're getting exactly"
    },
    {
      "id": 575,
      "start": 3441.2799999999997,
      "end": 3446.08,
      "text": "what you predict um that's for the industry that that's for the industry that's right so suppose"
    },
    {
      "id": 576,
      "start": 3446.08,
      "end": 3453.84,
      "text": "anthropics compute keeps 3xing a year and then by like 27 you have uh or 27 28 you have 10 gigawatts"
    },
    {
      "id": 577,
      "start": 3453.84,
      "end": 3460.48,
      "text": "and like multiply that by as you say um 10 billion so then it's like 100 billion a year"
    },
    {
      "id": 578,
      "start": 3460.48,
      "end": 3465.2799999999997,
      "text": "but then you're saying the tam by 2028 29 i don't want to give exact numbers for anthropic but but"
    },
    {
      "id": 579,
      "start": 3465.2799999999997,
      "end": 3470.64,
      "text": "these numbers are too small these numbers are too small okay interesting i'm really proud that the"
    },
    {
      "id": 580,
      "start": 3470.64,
      "end": 3474.72,
      "text": "puzzles i've worked on with jane street have resulted in them hiring a bunch of people from my audience"
    },
    {
      "id": 581,
      "start": 3474.72,
      "end": 3479.68,
      "text": "well they're still hiring and they just send me another puzzle for this one they spent about 20 000"
    },
    {
      "id": 582,
      "start": 3479.68,
      "end": 3485.04,
      "text": "gpu hours trading backdoors into three different language models each one has a hidden prompt that"
    },
    {
      "id": 583,
      "start": 3485.04,
      "end": 3489.8399999999997,
      "text": "elicits completely different behavior you just have to find the trigger this is particularly cool because"
    },
    {
      "id": 584,
      "start": 3489.8399999999997,
      "end": 3495.12,
      "text": "finding backdoors is actually an open question in frontier ai research anthropic actually released a"
    },
    {
      "id": 585,
      "start": 3495.12,
      "end": 3499.52,
      "text": "couple of papers about sleeper agents and they showed that you can build a simple classifier on the"
    },
    {
      "id": 586,
      "start": 3499.52,
      "end": 3504.64,
      "text": "residual stream to detect when a backdoor is about to fire but they already knew what the trigger"
    },
    {
      "id": 587,
      "start": 3504.64,
      "end": 3510.64,
      "text": "were because they built them here you don't and it's not feasible to check the activations for"
    },
    {
      "id": 588,
      "start": 3510.64,
      "end": 3515.2,
      "text": "all possible trigger phrases unlike the other puzzles they made for this podcast jane street isn't even"
    },
    {
      "id": 589,
      "start": 3515.2,
      "end": 3519.2799999999997,
      "text": "sure this one is solvable but they've set aside fifty thousand dollars for the best attempts and"
    },
    {
      "id": 590,
      "start": 3519.2799999999997,
      "end": 3526.16,
      "text": "write-ups the puzzle's live at janestreet.com slash thwarkesh and they're accepting submissions until april 1st"
    },
    {
      "id": 591,
      "start": 3526.8799999999997,
      "end": 3533.7599999999998,
      "text": "all right back to daria you've told investors that you plan to be profitable starting in 28 and this is"
    },
    {
      "id": 592,
      "start": 3533.76,
      "end": 3538.96,
      "text": "the year where we're like potentially getting the country of geniuses the data center and you know"
    },
    {
      "id": 593,
      "start": 3538.96,
      "end": 3546.5600000000004,
      "text": "this is like gonna now unlock all this uh progress and uh medicine and uh health and etc etc and new"
    },
    {
      "id": 594,
      "start": 3546.5600000000004,
      "end": 3551.92,
      "text": "technologies wouldn't this be particularly exactly the time where you'd like want to reinvest in the"
    },
    {
      "id": 595,
      "start": 3551.92,
      "end": 3556.96,
      "text": "business and build bigger countries so they can make more discoveries so i mean profit profitability is"
    },
    {
      "id": 596,
      "start": 3556.96,
      "end": 3562.7200000000003,
      "text": "this kind of like weird thing in this field i i like like i don't think i don't think in this field"
    },
    {
      "id": 597,
      "start": 3562.7200000000003,
      "end": 3572.8,
      "text": "profitability is actually a measure of uh um you know kind of spending down versus investing in the"
    },
    {
      "id": 598,
      "start": 3572.8,
      "end": 3578.56,
      "text": "business like let's let's just let's just take a model of this i actually think profitability happens"
    },
    {
      "id": 599,
      "start": 3578.56,
      "end": 3583.28,
      "text": "when you underestimated the amount of demand you were going to get and loss happens when you"
    },
    {
      "id": 600,
      "start": 3583.28,
      "end": 3587.28,
      "text": "overestimated the amount of demand you were going to get um because you're buying the data centers"
    },
    {
      "id": 601,
      "start": 3587.28,
      "end": 3593.36,
      "text": "ahead of time so think about it this way um ideally you would like and again these are stylized facts"
    },
    {
      "id": 602,
      "start": 3593.36,
      "end": 3597.84,
      "text": "these numbers are not exact for i'm just trying to make a toy model here let's say half of your"
    },
    {
      "id": 603,
      "start": 3597.84,
      "end": 3603.44,
      "text": "compute is for training and half of your compute is for inference um and you know the inference has some"
    },
    {
      "id": 604,
      "start": 3603.44,
      "end": 3609.6800000000003,
      "text": "gross margin that's like more than 50 percent um and so what that means is that if you were in steady"
    },
    {
      "id": 605,
      "start": 3609.68,
      "end": 3615.12,
      "text": "state you build a data center if you knew exactly exactly exactly the demand you were getting you"
    },
    {
      "id": 606,
      "start": 3615.12,
      "end": 3620.72,
      "text": "would um uh uh uh uh you know you you would you would you would you would you would get a certain"
    },
    {
      "id": 607,
      "start": 3620.72,
      "end": 3626.0,
      "text": "amount of revenue say i don't know uh uh let's say you pay a hundred billion dollars a year for compute"
    },
    {
      "id": 608,
      "start": 3626.0,
      "end": 3631.7599999999998,
      "text": "and on 50 billion dollars a year you support 150 billion dollars on of of of of of revenue"
    },
    {
      "id": 609,
      "start": 3631.7599999999998,
      "end": 3637.2,
      "text": "and the other 50 billion the other 50 billion are used for training um so basically you're profitable you"
    },
    {
      "id": 610,
      "start": 3637.2,
      "end": 3641.52,
      "text": "make fit you make fit you make 50 billion dollars of profit those are the economics of the industry"
    },
    {
      "id": 611,
      "start": 3641.52,
      "end": 3647.2,
      "text": "today or sorry not today but like that's where we're where we're projecting forward in a year or two"
    },
    {
      "id": 612,
      "start": 3647.2,
      "end": 3655.2799999999997,
      "text": "the only thing that makes that not the case is if you get less demand than 50 billion um then you have"
    },
    {
      "id": 613,
      "start": 3655.2799999999997,
      "end": 3660.24,
      "text": "more than 50 percent of your your data center for research and you're not profitable so you you know"
    },
    {
      "id": 614,
      "start": 3660.24,
      "end": 3666.48,
      "text": "you train stronger models but you're like not profitable um if you uh get more demand than you thought"
    },
    {
      "id": 615,
      "start": 3666.48,
      "end": 3672.8,
      "text": "then your research gets squeezed um but uh you know you're you're kind of able to support more"
    },
    {
      "id": 616,
      "start": 3672.8,
      "end": 3677.52,
      "text": "inference and you're more profitable so it's uh maybe i'm not explaining it well but but the thing"
    },
    {
      "id": 617,
      "start": 3677.52,
      "end": 3684.2400000000002,
      "text": "i'm trying to say is you decide the amount of compute first and then you have some target desire of"
    },
    {
      "id": 618,
      "start": 3684.2400000000002,
      "end": 3689.6,
      "text": "inference versus versus training but that gets determined by demand it doesn't get determined by"
    },
    {
      "id": 619,
      "start": 3689.6,
      "end": 3694.32,
      "text": "what i'm hearing is the reason you're predicting profit is that you are systematically underestimate"
    },
    {
      "id": 620,
      "start": 3694.32,
      "end": 3698.56,
      "text": "under investing in compute right because if you actually like i'm saying i'm saying it's hard"
    },
    {
      "id": 621,
      "start": 3698.56,
      "end": 3704.56,
      "text": "to predict so so these things about 2028 and when it will happen that's our that's our attempt to do"
    },
    {
      "id": 622,
      "start": 3704.56,
      "end": 3709.84,
      "text": "the best we can with investors all of this stuff is really uncertain because of the cone of uncertainty"
    },
    {
      "id": 623,
      "start": 3709.84,
      "end": 3717.2000000000003,
      "text": "like we could be profitable in 2026 if the if the revenue grows fast enough and then and then um"
    },
    {
      "id": 624,
      "start": 3717.2,
      "end": 3724.48,
      "text": "uh you know if we if we overestimate or underestimate the next year that could swing wildly like i i i what"
    },
    {
      "id": 625,
      "start": 3724.48,
      "end": 3730.64,
      "text": "i'm trying to get is you have a model in your head of like the the business invest invest invest invest"
    },
    {
      "id": 626,
      "start": 3730.64,
      "end": 3735.2799999999997,
      "text": "get scale and and and and kind of then becomes profitable there's a single point at which things"
    },
    {
      "id": 627,
      "start": 3735.2799999999997,
      "end": 3741.4399999999996,
      "text": "turn around i don't think the economics of this industry work that way i see so if i'm understanding"
    },
    {
      "id": 628,
      "start": 3741.4399999999996,
      "end": 3745.9199999999996,
      "text": "correctly you're saying because of the discrepancy between the amount of compute we should have gotten"
    },
    {
      "id": 629,
      "start": 3745.92,
      "end": 3750.16,
      "text": "and the amount of compute we got we we were like sort of forced to make profit but that that doesn't"
    },
    {
      "id": 630,
      "start": 3750.16,
      "end": 3754.48,
      "text": "mean we're going to continue making profit we're going to like reinvest the money because well now"
    },
    {
      "id": 631,
      "start": 3754.48,
      "end": 3761.2000000000003,
      "text": "ai has made so much progress and we want the bigger country of geniuses and so then back into uh revenue"
    },
    {
      "id": 632,
      "start": 3761.2000000000003,
      "end": 3767.2000000000003,
      "text": "is high but losses are also high if we if we predict if every year we predict exactly what the demand"
    },
    {
      "id": 633,
      "start": 3767.2000000000003,
      "end": 3774.16,
      "text": "is going to be will be profitable every year because grow because spending spending 50 of your compute on"
    },
    {
      "id": 634,
      "start": 3774.16,
      "end": 3781.52,
      "text": "on on 50 of your compute on research roughly um plus a gross margin that's higher than 50 and and"
    },
    {
      "id": 635,
      "start": 3781.52,
      "end": 3786.56,
      "text": "correct demand prediction leads to profit that's the prof that's that's the profitable business model"
    },
    {
      "id": 636,
      "start": 3786.56,
      "end": 3793.2799999999997,
      "text": "that i think is kind of like there but like obscured by these like building ahead and prediction errors"
    },
    {
      "id": 637,
      "start": 3793.2799999999997,
      "end": 3800.08,
      "text": "i guess you're treating the 50 as a uh as a sort of like you know just like a given constant whereas"
    },
    {
      "id": 638,
      "start": 3800.08,
      "end": 3803.84,
      "text": "you in fact if you if the ai progress is fast and you can increase the progress by scaling up more"
    },
    {
      "id": 639,
      "start": 3803.84,
      "end": 3807.2799999999997,
      "text": "you just have more than 50 percent and not make profit here's what i'll say you might want to scale"
    },
    {
      "id": 640,
      "start": 3807.2799999999997,
      "end": 3812.88,
      "text": "up it more you might want to scale it up more but but but you know remember the log returns to scale"
    },
    {
      "id": 641,
      "start": 3812.88,
      "end": 3821.2799999999997,
      "text": "right if if 70 percent would get you a a a very little bit of a smaller model through a factor of of 1.4x"
    },
    {
      "id": 642,
      "start": 3821.28,
      "end": 3827.84,
      "text": "right like that extra 20 billion dollars is is is you know that each each dollar there is worth much"
    },
    {
      "id": 643,
      "start": 3827.84,
      "end": 3833.2000000000003,
      "text": "less to you because it because because the log linear setup and so you might find that it's better"
    },
    {
      "id": 644,
      "start": 3833.2000000000003,
      "end": 3838.8,
      "text": "to invest that that that it's better to invest that 20 billion dollars in you know in in serving"
    },
    {
      "id": 645,
      "start": 3838.8,
      "end": 3843.2000000000003,
      "text": "inference or in hiring engineers who are who are who are kind of better who are who are kind of better"
    },
    {
      "id": 646,
      "start": 3843.2000000000003,
      "end": 3848.0800000000004,
      "text": "who are kind of better at what they're doing so the reason i said 50 that's not that's not exactly"
    },
    {
      "id": 647,
      "start": 3848.08,
      "end": 3853.6,
      "text": "our target it's not exactly going to be 50 it'll probably vary vary over time what i'm saying is"
    },
    {
      "id": 648,
      "start": 3853.6,
      "end": 3860.96,
      "text": "the the the the like log linear return what it leads to is you spend of order one fraction of the"
    },
    {
      "id": 649,
      "start": 3860.96,
      "end": 3867.92,
      "text": "business right like not five percent not 95 and then it then it that you know then then you get"
    },
    {
      "id": 650,
      "start": 3867.92,
      "end": 3871.84,
      "text": "diminishing returns because of the because of the law everyone's strange that i'm like convincing"
    },
    {
      "id": 651,
      "start": 3871.84,
      "end": 3876.16,
      "text": "dario to like believe in ai progress or something but like uh you okay you don't invest in"
    },
    {
      "id": 652,
      "start": 3876.16,
      "end": 3879.44,
      "text": "research because it has diminishing returns but you invest in the other things you mentioned"
    },
    {
      "id": 653,
      "start": 3879.44,
      "end": 3885.52,
      "text": "again again we're talking about diminishing returns after you're spending 50 billion a year right like"
    },
    {
      "id": 654,
      "start": 3886.16,
      "end": 3892.56,
      "text": "this is a point i'm sure you would make but like diminishing returns on a genius is could be quite high"
    },
    {
      "id": 655,
      "start": 3892.56,
      "end": 3896.56,
      "text": "and more generally like what is profit in the market economy profit is basically saying"
    },
    {
      "id": 656,
      "start": 3897.2,
      "end": 3902.0,
      "text": "the other companies in the market can like do more things with this money that i yeah i mean put"
    },
    {
      "id": 657,
      "start": 3902.0,
      "end": 3905.84,
      "text": "aside anthropic i'm just trying to like because i you know i don't want to give information about"
    },
    {
      "id": 658,
      "start": 3905.84,
      "end": 3910.64,
      "text": "anthropic is why i'm giving these stylized numbers but like let's just derive the equilibrium of the"
    },
    {
      "id": 659,
      "start": 3910.64,
      "end": 3920.56,
      "text": "industry right i think the so so so why doesn't everyone spend 100 of their um uh you know 100 of"
    },
    {
      "id": 660,
      "start": 3920.56,
      "end": 3925.28,
      "text": "their compute on training and not serve any customers right it's because if they didn't get any revenue"
    },
    {
      "id": 661,
      "start": 3925.28,
      "end": 3929.04,
      "text": "they couldn't raise money they couldn't do compute deals they couldn't buy more compute the next year"
    },
    {
      "id": 662,
      "start": 3929.04,
      "end": 3936.16,
      "text": "so there's going to be an equilibrium where every every company spends less than 100 on on on on"
    },
    {
      "id": 663,
      "start": 3936.16,
      "end": 3940.72,
      "text": "training and certainly less than 100 on inference it should be clear why you don't just serve the"
    },
    {
      "id": 664,
      "start": 3940.72,
      "end": 3945.68,
      "text": "current models and and you know and and and and never train another model because then you don't"
    },
    {
      "id": 665,
      "start": 3945.68,
      "end": 3950.24,
      "text": "have any demand because you'll because you'll fall behind so there's some equilibrium it's it's not"
    },
    {
      "id": 666,
      "start": 3950.24,
      "end": 3955.84,
      "text": "going to be 10 it's not going to be 90 let's just say as a stylized fact it's 50 that's what i'm"
    },
    {
      "id": 667,
      "start": 3955.84,
      "end": 3961.1200000000003,
      "text": "getting at and and and i think we're going to be in a position where that equilibrium of how much you"
    },
    {
      "id": 668,
      "start": 3961.1200000000003,
      "end": 3967.04,
      "text": "spend on training is less than the gross margins that that you're at you that that that you're able"
    },
    {
      "id": 669,
      "start": 3967.04,
      "end": 3972.48,
      "text": "to get on compute and so the the the underlying economics are profitable the problem is you have"
    },
    {
      "id": 670,
      "start": 3972.48,
      "end": 3978.32,
      "text": "this this hellish demand prediction problem when you're when you're buying the next year of compute"
    },
    {
      "id": 671,
      "start": 3978.32,
      "end": 3985.04,
      "text": "and you might guess under and be very profitable but have no compute for research or you might guess"
    },
    {
      "id": 672,
      "start": 3985.04,
      "end": 3991.92,
      "text": "over and you know you're you're you're um uh you you are not profitable and you have all the compute"
    },
    {
      "id": 673,
      "start": 3992.4,
      "end": 3998.4,
      "text": "compute for research in the world does that make sense just as a dynamic model of the industry maybe"
    },
    {
      "id": 674,
      "start": 3998.4,
      "end": 4004.32,
      "text": "stepping back i'm like uh i i'm not saying i i think the country of genius is going to come in two years"
    },
    {
      "id": 675,
      "start": 4004.32,
      "end": 4010.0,
      "text": "and therefore you should buy this compute um to me what you're saying the end conclusion you're arriving"
    },
    {
      "id": 676,
      "start": 4010.0,
      "end": 4015.76,
      "text": "it makes a lot of sense but uh that's because like oh it seems like country geniuses is hard and"
    },
    {
      "id": 677,
      "start": 4015.76,
      "end": 4020.4,
      "text": "there's a long way to go and so the stepping back the thing i'm trying to get at is more like"
    },
    {
      "id": 678,
      "start": 4022.08,
      "end": 4026.24,
      "text": "it seems like your worldview is compatible with somebody who says uh we're like 10 years away"
    },
    {
      "id": 679,
      "start": 4026.24,
      "end": 4029.68,
      "text": "from a world in which like we're generating trillions of dollars that's just that's just not"
    },
    {
      "id": 680,
      "start": 4029.68,
      "end": 4035.76,
      "text": "my view yeah that is that is not my view like i i so so so i'll like i'll like make another prediction"
    },
    {
      "id": 681,
      "start": 4035.76,
      "end": 4042.32,
      "text": "it is hard for me to see that that there won't be trillions of dollars in revenue before 2030."
    },
    {
      "id": 682,
      "start": 4042.32,
      "end": 4049.2000000000003,
      "text": "um like uh i can i can construct a plausible world it takes maybe three years so that would"
    },
    {
      "id": 683,
      "start": 4049.2000000000003,
      "end": 4054.1600000000003,
      "text": "you know that would be the end of what i think it's plausible like in 2028 we get the the real"
    },
    {
      "id": 684,
      "start": 4054.1600000000003,
      "end": 4058.96,
      "text": "country of geniuses in the data center you know the revenue's been been go you know the revenue's been"
    },
    {
      "id": 685,
      "start": 4058.96,
      "end": 4066.0,
      "text": "going into the maybe is in the low hundreds of billions by by by by 2028 and and and then the"
    },
    {
      "id": 686,
      "start": 4066.0,
      "end": 4070.7200000000003,
      "text": "country of geniuses accelerates it to trillions you know and and we're basically we're basically on the"
    },
    {
      "id": 687,
      "start": 4070.7200000000003,
      "end": 4075.84,
      "text": "slow end of diffusion it takes two years to get to the trillions that that would that that would be"
    },
    {
      "id": 688,
      "start": 4075.84,
      "end": 4081.52,
      "text": "the world where it takes until that would be the world where it takes until 2030. i i i suspect even"
    },
    {
      "id": 689,
      "start": 4081.52,
      "end": 4086.4,
      "text": "composing the technical exponential and diffusion exponential will get there before 2030."
    },
    {
      "id": 690,
      "start": 4086.4,
      "end": 4093.28,
      "text": "so you laid out a model where anthropic makes profit because it seems like fundamentally we're"
    },
    {
      "id": 691,
      "start": 4093.28,
      "end": 4097.4400000000005,
      "text": "in a compute constrained world and so it's like eventually we keep growing compute no i think i"
    },
    {
      "id": 692,
      "start": 4097.4400000000005,
      "end": 4102.56,
      "text": "think the way the profit comes is again and and you know let's let's just abstract the whole industry"
    },
    {
      "id": 693,
      "start": 4102.56,
      "end": 4107.36,
      "text": "here like we have a you know let's just imagine we're we're in like an economics textbook we have a"
    },
    {
      "id": 694,
      "start": 4107.36,
      "end": 4114.4,
      "text": "small number of firms each can invest a limited amount in you know or or like each can invest some"
    },
    {
      "id": 695,
      "start": 4114.4,
      "end": 4120.639999999999,
      "text": "fraction fraction in r d they have some marginal cost to serve the margins on that the profit margin"
    },
    {
      "id": 696,
      "start": 4120.639999999999,
      "end": 4126.16,
      "text": "the gross profit margins on that marginal cost are like very high because because because inference"
    },
    {
      "id": 697,
      "start": 4126.16,
      "end": 4131.5199999999995,
      "text": "is efficient there's some competition but the models are also differentiated there's some there's some"
    },
    {
      "id": 698,
      "start": 4131.5199999999995,
      "end": 4136.879999999999,
      "text": "um you know companies will compete to push their research budgets up but like because there's a"
    },
    {
      "id": 699,
      "start": 4136.879999999999,
      "end": 4141.679999999999,
      "text": "small number of players you know we have the what is it called the neck and the corno equilibrium i"
    },
    {
      "id": 700,
      "start": 4141.68,
      "end": 4147.04,
      "text": "think is what the what the uh small number of firm equal equilibrium is it the point is it doesn't"
    },
    {
      "id": 701,
      "start": 4147.04,
      "end": 4154.0,
      "text": "equilibrate to perfect competition with with with with with with with zero margins if there's like"
    },
    {
      "id": 702,
      "start": 4154.0,
      "end": 4159.76,
      "text": "three firms if there's three firms in the economy all are kind of independently behaving behaving"
    },
    {
      "id": 703,
      "start": 4159.76,
      "end": 4165.52,
      "text": "rationally it doesn't equilibrate to zero um help me understand that because right now we do have three"
    },
    {
      "id": 704,
      "start": 4165.52,
      "end": 4172.88,
      "text": "leading firms and they're not making profit um and so what what uh yeah what what is changing yeah so"
    },
    {
      "id": 705,
      "start": 4172.88,
      "end": 4180.240000000001,
      "text": "the the again the gross margins right now are very positive what's happening what what's happening is a"
    },
    {
      "id": 706,
      "start": 4180.240000000001,
      "end": 4186.400000000001,
      "text": "combination of two things one is we're still in the exponential scale up phase of compute yeah um so"
    },
    {
      "id": 707,
      "start": 4186.400000000001,
      "end": 4192.64,
      "text": "what basically what that means is we're training like a model gets trained yeah it costs you know let's"
    },
    {
      "id": 708,
      "start": 4192.64,
      "end": 4200.08,
      "text": "say a model got trained that costs uh a billion dollars last year um and then uh this year it"
    },
    {
      "id": 709,
      "start": 4200.08,
      "end": 4209.200000000001,
      "text": "produced uh four billion dollars of revenue and cost one billion dollars to to uh to to to inference from"
    },
    {
      "id": 710,
      "start": 4209.200000000001,
      "end": 4214.64,
      "text": "um so you know again i'm using stylized number here but you know that would be 75 percent you know gross"
    },
    {
      "id": 711,
      "start": 4214.64,
      "end": 4222.240000000001,
      "text": "gross gross margins and you know this this 25 tax so that model as a whole makes two billion dollars"
    },
    {
      "id": 712,
      "start": 4222.64,
      "end": 4227.6,
      "text": "um but at the same time we're spending 10 billion dollars to train the next model because there's"
    },
    {
      "id": 713,
      "start": 4227.6,
      "end": 4232.88,
      "text": "an exponential scale up and so the company loses money each model makes money but the company loses"
    },
    {
      "id": 714,
      "start": 4232.88,
      "end": 4237.84,
      "text": "money the equilibrium i'm talking about is an equilibrium where we have the country of geniuses"
    },
    {
      "id": 715,
      "start": 4237.84,
      "end": 4244.96,
      "text": "we have the country of geniuses in the data center but that that um model training scale up has"
    },
    {
      "id": 716,
      "start": 4244.96,
      "end": 4250.240000000001,
      "text": "equilibrated more maybe maybe it's still it's still going up we're still trying to predict the demand"
    },
    {
      "id": 717,
      "start": 4250.24,
      "end": 4256.96,
      "text": "but it's more it's more um leveled out um i'm curious a couple things there so um let's start"
    },
    {
      "id": 718,
      "start": 4256.96,
      "end": 4262.4,
      "text": "with the current world um in the current world you're right that as you said before if you treat"
    },
    {
      "id": 719,
      "start": 4262.4,
      "end": 4267.5199999999995,
      "text": "each individual model as a company it's profitable but of course a big part of the production function"
    },
    {
      "id": 720,
      "start": 4268.16,
      "end": 4274.32,
      "text": "of being a frontier lab is training the next model right so if you didn't do that then you'd make"
    },
    {
      "id": 721,
      "start": 4274.32,
      "end": 4277.44,
      "text": "profit for two months that's right and you wouldn't have margins because you wouldn't have the best"
    },
    {
      "id": 722,
      "start": 4277.44,
      "end": 4282.4,
      "text": "model and then so yeah you can make profits right at some point that reaches the biggest scale that"
    },
    {
      "id": 723,
      "start": 4282.4,
      "end": 4287.679999999999,
      "text": "it can reach and then and then in equilibrium we have algorithmic improvements but we're spending"
    },
    {
      "id": 724,
      "start": 4287.679999999999,
      "end": 4294.32,
      "text": "roughly the same amount to train the next model as as we spent to train the current model um so"
    },
    {
      "id": 725,
      "start": 4294.32,
      "end": 4299.28,
      "text": "this equilibrium relies i mean at some point at some at some point you run out of money in the economy"
    },
    {
      "id": 726,
      "start": 4299.28,
      "end": 4303.28,
      "text": "uh a fixed lump of labor follows the economy is going to grow right that's one of your"
    },
    {
      "id": 727,
      "start": 4303.28,
      "end": 4307.679999999999,
      "text": "predictions well we're gonna have yes but this is this is this but this is another example of the"
    },
    {
      "id": 728,
      "start": 4307.679999999999,
      "end": 4314.08,
      "text": "theme i was talking about which is that the economy will grow much faster with ai than i think it"
    },
    {
      "id": 729,
      "start": 4314.08,
      "end": 4320.48,
      "text": "ever has before but it's not like right now the computer is growing 3x a year yeah i don't believe"
    },
    {
      "id": 730,
      "start": 4320.48,
      "end": 4325.84,
      "text": "the economy is going to grow 300 a year like i said this in machines of loving grace like i think we"
    },
    {
      "id": 731,
      "start": 4325.84,
      "end": 4333.04,
      "text": "may get 10 or 20 per year growth in the economy but we're not going to get 300 growth in the economy"
    },
    {
      "id": 732,
      "start": 4333.04,
      "end": 4338.8,
      "text": "so i think i think in the end you know if compute becomes the majority of what the economy produces"
    },
    {
      "id": 733,
      "start": 4338.8,
      "end": 4342.88,
      "text": "it's it's gonna it's gonna be capped by that so let's okay now let's assume a model where compute"
    },
    {
      "id": 734,
      "start": 4342.88,
      "end": 4349.04,
      "text": "stays capped yeah um the world where frontier labs are making money is one where they continue to make"
    },
    {
      "id": 735,
      "start": 4349.68,
      "end": 4356.32,
      "text": "um fast progress because fundamentally your margin is limited by how good the alternative is and so you"
    },
    {
      "id": 736,
      "start": 4356.32,
      "end": 4359.5199999999995,
      "text": "are able to make money because you have a frontier model um if you didn't have a frontier model you"
    },
    {
      "id": 737,
      "start": 4359.52,
      "end": 4366.160000000001,
      "text": "wouldn't be making money um and and so this this model requires there never to be a steady state"
    },
    {
      "id": 738,
      "start": 4366.160000000001,
      "end": 4372.0,
      "text": "like forever and ever you keep making no i don't i don't think that's true i mean i i feel i feel like"
    },
    {
      "id": 739,
      "start": 4372.0,
      "end": 4376.64,
      "text": "we're we're like we're taught we're you know we're they feel like this is an economics uh like uh you"
    },
    {
      "id": 740,
      "start": 4376.64,
      "end": 4381.200000000001,
      "text": "know this is like an economics class you know the tyler cowen quote we never stop talking about economics"
    },
    {
      "id": 741,
      "start": 4381.200000000001,
      "end": 4386.64,
      "text": "we never we never stop talking about economics so no but but there there are there are worlds in which"
    },
    {
      "id": 742,
      "start": 4387.200000000001,
      "end": 4391.76,
      "text": "um you know they're the so i i don't think this field's gonna be i don't think this field's gonna"
    },
    {
      "id": 743,
      "start": 4391.76,
      "end": 4396.4800000000005,
      "text": "be a monopoly all my lawyers never want me to say the word monopoly um but i don't think this field's"
    },
    {
      "id": 744,
      "start": 4396.4800000000005,
      "end": 4400.88,
      "text": "gonna be a monopoly but but you do get you get industries in which there are a small number of"
    },
    {
      "id": 745,
      "start": 4400.88,
      "end": 4407.76,
      "text": "players not one but a small number of players and ordinarily like the the way you get monopolies like"
    },
    {
      "id": 746,
      "start": 4408.72,
      "end": 4415.52,
      "text": "facebook or or meta i always call them facebook but um uh uh is is these kind of net is these kind of"
    },
    {
      "id": 747,
      "start": 4415.52,
      "end": 4420.0,
      "text": "these kind of network effects the way you get industries in which there are small number of"
    },
    {
      "id": 748,
      "start": 4420.0,
      "end": 4427.76,
      "text": "players are very high costs of entry right um so you know uh cloud is like this i think cloud is a"
    },
    {
      "id": 749,
      "start": 4427.76,
      "end": 4433.280000000001,
      "text": "good example of this you have three maybe four players within cloud i think i think that's the same"
    },
    {
      "id": 750,
      "start": 4433.28,
      "end": 4440.8,
      "text": "for ai three maybe four um uh and the reason is that it's it's so expensive it requires so much"
    },
    {
      "id": 751,
      "start": 4440.8,
      "end": 4447.12,
      "text": "expertise and so much capital to like run a cloud company right and so you have to put up all this"
    },
    {
      "id": 752,
      "start": 4447.12,
      "end": 4451.92,
      "text": "capital and then in addition to putting up all this capital you have to get all this other stuff that"
    },
    {
      "id": 753,
      "start": 4451.92,
      "end": 4456.4,
      "text": "like you know requires a lot of skill to you know to make it happen and so it's like if you go to"
    },
    {
      "id": 754,
      "start": 4456.4,
      "end": 4459.84,
      "text": "someone and you're like i want to disrupt this industry here's 100 billion dollars you're like"
    },
    {
      "id": 755,
      "start": 4459.84,
      "end": 4464.64,
      "text": "okay i'm putting 100 billion dollars and also betting that you can do all these other things"
    },
    {
      "id": 756,
      "start": 4464.64,
      "end": 4468.88,
      "text": "that these people have been doing only to decrease the profit and then and then the effect of your"
    },
    {
      "id": 757,
      "start": 4468.88,
      "end": 4473.92,
      "text": "entering is the profit margins go down so you know we have equilibria like this all the time in the"
    },
    {
      "id": 758,
      "start": 4473.92,
      "end": 4480.0,
      "text": "economy where we have a few we have a few players profits are not astronomical margins are not"
    },
    {
      "id": 759,
      "start": 4480.0,
      "end": 4485.4400000000005,
      "text": "astronomical but they're they're not zero right um uh and and you know i think i think that's what we"
    },
    {
      "id": 760,
      "start": 4485.44,
      "end": 4491.599999999999,
      "text": "see on cloud cloud is very undifferentiated models are more differentiated than cloud right like"
    },
    {
      "id": 761,
      "start": 4491.599999999999,
      "end": 4497.36,
      "text": "everyone knows claude is claude claude is good at different things than gpt is good at is then then"
    },
    {
      "id": 762,
      "start": 4497.36,
      "end": 4503.12,
      "text": "gemini is good at and it's not just claude's good at coding gpt's good at you know math and reasoning"
    },
    {
      "id": 763,
      "start": 4503.12,
      "end": 4509.12,
      "text": "you know um uh it's more subtle than that like models are good at different types of coding models have"
    },
    {
      "id": 764,
      "start": 4509.12,
      "end": 4514.32,
      "text": "different styles like i think i think these things are actually you know quite different from each"
    },
    {
      "id": 765,
      "start": 4514.32,
      "end": 4521.5199999999995,
      "text": "other and so i would expect more differentiation than you see in in um cloud now there there actually"
    },
    {
      "id": 766,
      "start": 4521.5199999999995,
      "end": 4528.639999999999,
      "text": "is a uh counter there there is one counter argument um and that counter argument is that if all of that"
    },
    {
      "id": 767,
      "start": 4528.639999999999,
      "end": 4535.92,
      "text": "the process of producing models um becomes uh if ai models can do that themselves then that could"
    },
    {
      "id": 768,
      "start": 4535.92,
      "end": 4541.04,
      "text": "spread throughout the economy but that is not an argument for commoditizing ai models in general"
    },
    {
      "id": 769,
      "start": 4541.04,
      "end": 4546.32,
      "text": "that's kind of an argument for commoditizing the whole economy at once um i don't know what what"
    },
    {
      "id": 770,
      "start": 4546.32,
      "end": 4551.12,
      "text": "quite happens in that world where basically anyone can do anything anyone can build anything and there's"
    },
    {
      "id": 771,
      "start": 4551.12,
      "end": 4556.32,
      "text": "like no mode around anything at all i mean i don't know maybe we want that world like like maybe that's"
    },
    {
      "id": 772,
      "start": 4556.32,
      "end": 4562.32,
      "text": "the maybe that's the end state here like maybe maybe um you know when what maybe when when kind of ai"
    },
    {
      "id": 773,
      "start": 4562.32,
      "end": 4567.68,
      "text": "models can do you know when when when am i was can do everything if we've solved all the safety and"
    },
    {
      "id": 774,
      "start": 4567.68,
      "end": 4573.68,
      "text": "security problems like you know that's one of the one of the one of the mechanisms for for uh you know"
    },
    {
      "id": 775,
      "start": 4573.68,
      "end": 4578.8,
      "text": "um uh uh you know just just kind of the economy flattening itself again but but that's kind of like"
    },
    {
      "id": 776,
      "start": 4578.8,
      "end": 4586.240000000001,
      "text": "post like far post country geniuses in a data center um maybe a finer way to put that uh potential point"
    },
    {
      "id": 777,
      "start": 4586.240000000001,
      "end": 4595.52,
      "text": "is one um it seems like ai research is especially loaded on raw intellectual power which will be"
    },
    {
      "id": 778,
      "start": 4595.52,
      "end": 4600.64,
      "text": "especially abundant in a world with agi and two if you just look at the world today there's very"
    },
    {
      "id": 779,
      "start": 4600.64,
      "end": 4608.72,
      "text": "few technologies that seem to be diffusing as fast as um as ai algorithmic progress and so that does"
    },
    {
      "id": 780,
      "start": 4608.72,
      "end": 4614.160000000001,
      "text": "hint that this industry is sort of structurally diffusive so i think coding is going fast but i"
    },
    {
      "id": 781,
      "start": 4614.160000000001,
      "end": 4618.56,
      "text": "think ai research is a superset of coding and there are aspects of it that are not going fast"
    },
    {
      "id": 782,
      "start": 4618.56,
      "end": 4624.64,
      "text": "um uh but i but i do think again once we get coding once we get ai models going fast"
    },
    {
      "id": 783,
      "start": 4624.64,
      "end": 4629.4400000000005,
      "text": "then you know ai you know that will speed up the ability of ai models to kind of to kind of do"
    },
    {
      "id": 784,
      "start": 4629.4400000000005,
      "end": 4634.8,
      "text": "everything else so i think while coding is going fast now i think once the ai models are building"
    },
    {
      "id": 785,
      "start": 4634.8,
      "end": 4639.92,
      "text": "the next ai models and building everything else the kind of whole the whole economy will sort of kind"
    },
    {
      "id": 786,
      "start": 4639.92,
      "end": 4646.64,
      "text": "of go at the same pace i am i am worried geographically though i'm a little worried that like"
    },
    {
      "id": 787,
      "start": 4646.64,
      "end": 4654.320000000001,
      "text": "just proximity to ai having heard about ai um uh that that that may be one differentiator and so"
    },
    {
      "id": 788,
      "start": 4654.32,
      "end": 4660.639999999999,
      "text": "when i said the like you know 10 or 20 growth rate a worry i have is that the growth rate could be"
    },
    {
      "id": 789,
      "start": 4660.639999999999,
      "end": 4666.48,
      "text": "like 50 in silicon valley and you know parts of the world that are kind of socially connected to"
    },
    {
      "id": 790,
      "start": 4666.48,
      "end": 4672.88,
      "text": "silicon valley and you know not that much faster than its current pace elsewhere and i think that'd be"
    },
    {
      "id": 791,
      "start": 4672.88,
      "end": 4677.44,
      "text": "a pretty messed up world so i one of the things i think about a lot is how to prevent that yeah do you"
    },
    {
      "id": 792,
      "start": 4677.44,
      "end": 4683.679999999999,
      "text": "think that once we have uh this country of geniuses at a center that robotics is sort of quickly solved"
    },
    {
      "id": 793,
      "start": 4683.68,
      "end": 4688.72,
      "text": "afterwards because it seems like a big problem with robotics is that um a human can learn how to"
    },
    {
      "id": 794,
      "start": 4688.72,
      "end": 4694.64,
      "text": "teleoperate current hardware but current ai models can't at least not not in a way that's super"
    },
    {
      "id": 795,
      "start": 4694.64,
      "end": 4698.4800000000005,
      "text": "productive and so if we have this ability to learn like a human should it solve robotics immediately"
    },
    {
      "id": 796,
      "start": 4698.4800000000005,
      "end": 4703.04,
      "text": "as well i don't think it's dependent on learning like a human it could happen in different ways again"
    },
    {
      "id": 797,
      "start": 4703.6,
      "end": 4708.4800000000005,
      "text": "we could have trained the model on many different video games which are like robotic controls or many"
    },
    {
      "id": 798,
      "start": 4708.4800000000005,
      "end": 4713.6,
      "text": "different simulated robotics environments or just you know train them to control computer screens and they"
    },
    {
      "id": 799,
      "start": 4713.6,
      "end": 4721.52,
      "text": "learn to generalize so it will happen it it's not necessarily dependent on human-like learning"
    },
    {
      "id": 800,
      "start": 4721.52,
      "end": 4725.04,
      "text": "human-like learning is one way it could happen if the model's like oh i pick up a robot i don't know"
    },
    {
      "id": 801,
      "start": 4725.04,
      "end": 4730.56,
      "text": "how to use it i learn that that could happen because we discovered uh discovering continual learning"
    },
    {
      "id": 802,
      "start": 4730.56,
      "end": 4734.96,
      "text": "that could also happen because we train the model on a bunch of environments and then generalized"
    },
    {
      "id": 803,
      "start": 4734.96,
      "end": 4740.160000000001,
      "text": "or it could happen because the model learns that in the context length it it doesn't actually matter"
    },
    {
      "id": 804,
      "start": 4740.16,
      "end": 4745.84,
      "text": "which way if we go back to the discussion we had like like an hour ago that type of thing can happen"
    },
    {
      "id": 805,
      "start": 4745.84,
      "end": 4751.36,
      "text": "in that type of thing can happen in several different ways yeah um uh uh but but i do think"
    },
    {
      "id": 806,
      "start": 4751.36,
      "end": 4757.599999999999,
      "text": "when for for whatever reason the models have those skills then uh robotics will be revolutionized both"
    },
    {
      "id": 807,
      "start": 4757.599999999999,
      "end": 4764.16,
      "text": "the design of robots because the models will be much better than humans at that um and also the the"
    },
    {
      "id": 808,
      "start": 4764.16,
      "end": 4769.12,
      "text": "ability to kind of control robots so we'll get better at the physical building the physical hardware"
    },
    {
      "id": 809,
      "start": 4769.12,
      "end": 4773.44,
      "text": "building the physical robots and we'll also get better at controlling it now you know does that"
    },
    {
      "id": 810,
      "start": 4773.44,
      "end": 4778.72,
      "text": "mean the robotics industry will also be generating trillions of dollars of revenue my answer there is"
    },
    {
      "id": 811,
      "start": 4778.72,
      "end": 4785.2,
      "text": "yes but there will be the same extremely fast but not infinitely fast diffusion so will robotics be"
    },
    {
      "id": 812,
      "start": 4785.2,
      "end": 4790.96,
      "text": "be revolutionized yeah maybe tack on another year or two that's that's my that's the way i think"
    },
    {
      "id": 813,
      "start": 4790.96,
      "end": 4797.599999999999,
      "text": "about these things uh there's a general skepticism about extremely fast progress like here's my"
    },
    {
      "id": 814,
      "start": 4797.6,
      "end": 4801.360000000001,
      "text": "be which is like it sounds like you are going to solve continual learning one or another within"
    },
    {
      "id": 815,
      "start": 4801.360000000001,
      "end": 4806.56,
      "text": "the matter of years but just as people weren't talking about continual learning a couple years"
    },
    {
      "id": 816,
      "start": 4806.56,
      "end": 4810.56,
      "text": "ago and then we realized oh why aren't these models as useful as they could be right now even though they"
    },
    {
      "id": 817,
      "start": 4810.56,
      "end": 4814.8,
      "text": "are clearly passing the turing test and are experts in so many different domains maybe it's this thing"
    },
    {
      "id": 818,
      "start": 4814.8,
      "end": 4820.160000000001,
      "text": "and then we solve this thing and we realize actually there's another um another thing that human"
    },
    {
      "id": 819,
      "start": 4820.160000000001,
      "end": 4824.0,
      "text": "intelligence can do and that's a basis of human labor that these models can't do and then"
    },
    {
      "id": 820,
      "start": 4824.0,
      "end": 4828.88,
      "text": "so why not think there will be more things like this so i think that like we're you know we've like"
    },
    {
      "id": 821,
      "start": 4828.88,
      "end": 4833.6,
      "text": "found the pieces of human intelligence well well to be clear i mean i think continual learning as i"
    },
    {
      "id": 822,
      "start": 4833.6,
      "end": 4838.56,
      "text": "said before might not be a barrier at all right like like you know i think i think we maybe just get"
    },
    {
      "id": 823,
      "start": 4838.56,
      "end": 4844.56,
      "text": "there by pre-training generalization and and and and and and and and rl generalization like i i think"
    },
    {
      "id": 824,
      "start": 4844.56,
      "end": 4850.72,
      "text": "there just might not be um there basically might not be such a thing at all in fact i would point to the"
    },
    {
      "id": 825,
      "start": 4850.72,
      "end": 4857.12,
      "text": "history in in ml of people coming up with things that are barriers that end up kind of dissolving"
    },
    {
      "id": 826,
      "start": 4857.12,
      "end": 4863.4400000000005,
      "text": "within the big blob of compute right that you know people talked about you know you know how do you"
    },
    {
      "id": 827,
      "start": 4863.4400000000005,
      "end": 4869.76,
      "text": "have you know how do how do your models keep track of nouns and verbs and you know how do they you know"
    },
    {
      "id": 828,
      "start": 4869.76,
      "end": 4875.04,
      "text": "they can understand semantic syntactically but they can't understand semantically you know it's only"
    },
    {
      "id": 829,
      "start": 4875.04,
      "end": 4880.16,
      "text": "statistical correlations you can understand a paragraph you can understand a word there's reasoning you"
    },
    {
      "id": 830,
      "start": 4880.16,
      "end": 4885.84,
      "text": "can't do reasoning but then suddenly it turns out you can do code and math very well at all so i i"
    },
    {
      "id": 831,
      "start": 4885.84,
      "end": 4890.88,
      "text": "think they're actually there's there's actually a stronger history of some of these things seeming"
    },
    {
      "id": 832,
      "start": 4890.88,
      "end": 4896.5599999999995,
      "text": "like a big deal and then and then kind of and then kind of dissolving some of them are real i mean the"
    },
    {
      "id": 833,
      "start": 4896.5599999999995,
      "end": 4902.96,
      "text": "need for data is real maybe continual continual learn continual learning is a real thing but again"
    },
    {
      "id": 834,
      "start": 4903.5199999999995,
      "end": 4909.28,
      "text": "i would ground us in something like code like i think we may get to the point in like a year or two"
    },
    {
      "id": 835,
      "start": 4909.28,
      "end": 4915.28,
      "text": "where the models can just do sweet and end like that's a whole task that's a whole sphere of human"
    },
    {
      "id": 836,
      "start": 4915.28,
      "end": 4920.96,
      "text": "activity that that we're just saying models can do it now um when you say end to end do you mean um"
    },
    {
      "id": 837,
      "start": 4921.92,
      "end": 4927.28,
      "text": "setting technical direction understanding the context of the problem yes et cetera yes yes i mean all of"
    },
    {
      "id": 838,
      "start": 4927.28,
      "end": 4934.88,
      "text": "that interesting i mean that that is i feel like agi complete um which maybe is internally consistent"
    },
    {
      "id": 839,
      "start": 4934.88,
      "end": 4940.24,
      "text": "but um it's not like saying 90 of code or 100 of code it's like no no no the other parts of the job"
    },
    {
      "id": 840,
      "start": 4940.24,
      "end": 4947.36,
      "text": "no no no i gave this i gave the spectrum 90 of code 100 of code 90 of end-to-end suite 100 of"
    },
    {
      "id": 841,
      "start": 4947.36,
      "end": 4952.24,
      "text": "end-to-end suite new tasks are created for sweez eventually those get done as well but it's a long"
    },
    {
      "id": 842,
      "start": 4952.24,
      "end": 4957.12,
      "text": "spectrum there but we're traversing the spectrum very quickly yeah um i do think it's funny that"
    },
    {
      "id": 843,
      "start": 4957.12,
      "end": 4961.84,
      "text": "i i've seen a couple of podcasts you've done where um the host will be like ah but work has"
    },
    {
      "id": 844,
      "start": 4961.84,
      "end": 4965.36,
      "text": "wrote the session about the continual learning thing and it always makes me crack up because you're like"
    },
    {
      "id": 845,
      "start": 4965.36,
      "end": 4970.719999999999,
      "text": "you know you've been an ai researcher for like 10 years and i'm sure there's like some uh feeling of"
    },
    {
      "id": 846,
      "start": 4970.719999999999,
      "end": 4976.24,
      "text": "like okay so a podcaster wrote an essay and like every interview i get asked about it you know the"
    },
    {
      "id": 847,
      "start": 4976.24,
      "end": 4981.28,
      "text": "the truth of the truth of the matter is that we're all trying to figure this out together yeah right"
    },
    {
      "id": 848,
      "start": 4981.28,
      "end": 4987.5199999999995,
      "text": "there there are some ways in which i'm able to see things that others aren't these days that"
    },
    {
      "id": 849,
      "start": 4987.5199999999995,
      "end": 4992.16,
      "text": "probably has more to do with like i can see a bunch of stuff within anthropic and have to make"
    },
    {
      "id": 850,
      "start": 4992.16,
      "end": 4997.5199999999995,
      "text": "a bunch of decisions than i have any great research insight that that others don't right i you know"
    },
    {
      "id": 851,
      "start": 4997.5199999999995,
      "end": 5003.28,
      "text": "i'm running a 2500 person company like it's it's actually pretty hard for me to have have concrete"
    },
    {
      "id": 852,
      "start": 5003.28,
      "end": 5009.04,
      "text": "research insight you know much harder than you know than it would have been you know 10 years ago or"
    },
    {
      "id": 853,
      "start": 5009.04,
      "end": 5016.08,
      "text": "or you know or even two or three years ago um as we go towards a world of a full drop in remote"
    },
    {
      "id": 854,
      "start": 5016.08,
      "end": 5022.8,
      "text": "worker replacement does a api pricing model still make the most sense and if not what is the correct"
    },
    {
      "id": 855,
      "start": 5022.8,
      "end": 5027.84,
      "text": "way to price agi or serve agi yeah i mean i think there's going to be a bunch of different business"
    },
    {
      "id": 856,
      "start": 5027.84,
      "end": 5034.48,
      "text": "models here sort of all at once that are going to be that are going to be experimented with um i i i"
    },
    {
      "id": 857,
      "start": 5034.48,
      "end": 5043.5199999999995,
      "text": "actually do think that the the api um model is is more durable than many people think um one way i"
    },
    {
      "id": 858,
      "start": 5043.5199999999995,
      "end": 5050.24,
      "text": "think about it is if the technology is kind of advancing quickly if it's advancing exponentially"
    },
    {
      "id": 859,
      "start": 5050.24,
      "end": 5055.599999999999,
      "text": "what that means is there's there's always kind of like a surface area of of kind of new use cases that"
    },
    {
      "id": 860,
      "start": 5055.599999999999,
      "end": 5061.759999999999,
      "text": "have been developed in in the last uh in the last three months and any kind of product surface you put in"
    },
    {
      "id": 861,
      "start": 5061.76,
      "end": 5068.88,
      "text": "place is always at risk of sort of becoming irrelevant right any given product surface"
    },
    {
      "id": 862,
      "start": 5068.88,
      "end": 5073.76,
      "text": "probably makes sense for our you know a range of capabilities of the model right the the chatbot is"
    },
    {
      "id": 863,
      "start": 5073.76,
      "end": 5080.24,
      "text": "already running into limitations of you know making it smarter doesn't really help the average consumer"
    },
    {
      "id": 864,
      "start": 5080.24,
      "end": 5085.76,
      "text": "that much but i don't think that's a limitation of ai models i don't think that's evidence that you"
    },
    {
      "id": 865,
      "start": 5085.76,
      "end": 5090.16,
      "text": "know the models are the models are good enough and they're they're you know them getting better doesn't"
    },
    {
      "id": 866,
      "start": 5090.16,
      "end": 5095.92,
      "text": "matter to the economy it doesn't matter to that particular product um and and so i think the value"
    },
    {
      "id": 867,
      "start": 5095.92,
      "end": 5103.92,
      "text": "of the api is the api always offers an opportunity you know very close to the bare metal to build on"
    },
    {
      "id": 868,
      "start": 5103.92,
      "end": 5108.48,
      "text": "what the latest thing is um and so that you know there's there's there's kind of always going to be"
    },
    {
      "id": 869,
      "start": 5108.48,
      "end": 5115.2,
      "text": "this you know this this kind of front of new startups and new ideas that weren't possible a few"
    },
    {
      "id": 870,
      "start": 5115.2,
      "end": 5121.44,
      "text": "months ago and are possible because the model is advancing and and so i i actually i i kind of"
    },
    {
      "id": 871,
      "start": 5121.44,
      "end": 5128.5599999999995,
      "text": "actually predict that we are it's going to exist alongside other models but we're always going to"
    },
    {
      "id": 872,
      "start": 5128.5599999999995,
      "end": 5134.5599999999995,
      "text": "have the api business model because there's there's always going to be a need for a thousand different"
    },
    {
      "id": 873,
      "start": 5134.5599999999995,
      "end": 5139.84,
      "text": "people to try experimenting with the model in different way and a hundred of them become startups"
    },
    {
      "id": 874,
      "start": 5139.84,
      "end": 5144.72,
      "text": "and ten of them become big successful startups and you know two or three really end up being the the"
    },
    {
      "id": 875,
      "start": 5144.72,
      "end": 5149.76,
      "text": "way that people use the model of a of a given generation so i basically think it's always going"
    },
    {
      "id": 876,
      "start": 5149.76,
      "end": 5156.64,
      "text": "to exist at the same time i'm sure there's going to be other models as well like not every token"
    },
    {
      "id": 877,
      "start": 5157.6,
      "end": 5164.08,
      "text": "that's output by the model is worth the same amount think about you know how how what is the value of the"
    },
    {
      "id": 878,
      "start": 5164.08,
      "end": 5170.08,
      "text": "tokens that are like you know that the model outputs when someone you know call you know someone"
    },
    {
      "id": 879,
      "start": 5170.08,
      "end": 5174.4800000000005,
      "text": "you know calls them up and says my mac isn't working or something you know the models like restart it right"
    },
    {
      "id": 880,
      "start": 5174.48,
      "end": 5179.2,
      "text": "yeah um and like you know someone hasn't heard that before but like you know the model said that like"
    },
    {
      "id": 881,
      "start": 5180.16,
      "end": 5185.919999999999,
      "text": "10 million times right um you know that that maybe that's worth like a dollar or a few cents or something"
    },
    {
      "id": 882,
      "start": 5185.919999999999,
      "end": 5193.839999999999,
      "text": "um whereas if uh the model you know the model goes to you know one of the one of the pharmaceutical companies"
    },
    {
      "id": 883,
      "start": 5193.839999999999,
      "end": 5203.5199999999995,
      "text": "and it says oh you know this molecule you're developing you should take the aromatic ring from that end of the molecule and put it on that end of the molecule um and and you know if you do that"
    },
    {
      "id": 884,
      "start": 5203.52,
      "end": 5210.72,
      "text": "wonderful things will happen um uh like like those tokens could be worth you know tens of millions of dollars right"
    },
    {
      "id": 885,
      "start": 5210.72,
      "end": 5216.72,
      "text": "um uh so so i think we're definitely going to see business models that that recognize that you know"
    },
    {
      "id": 886,
      "start": 5216.72,
      "end": 5224.240000000001,
      "text": "at some point we're going to see you know pay for results or you you know in some in some form or we may see"
    },
    {
      "id": 887,
      "start": 5224.88,
      "end": 5233.200000000001,
      "text": "forms of compensation that are like labor um uh you know that that kind of work by the hour um i i i"
    },
    {
      "id": 888,
      "start": 5233.2,
      "end": 5237.679999999999,
      "text": "i you know i don't know i think i think i think because it's a new industry a lot of things are"
    },
    {
      "id": 889,
      "start": 5237.679999999999,
      "end": 5242.16,
      "text": "going to be tried and i you know i don't know what will turn out to be the right thing um what i find"
    },
    {
      "id": 890,
      "start": 5242.16,
      "end": 5247.04,
      "text": "uh i i take your point that people will have to try things to figure out what is the best way to use this"
    },
    {
      "id": 891,
      "start": 5247.679999999999,
      "end": 5254.16,
      "text": "blob of intelligence but what i find striking is clawed code so i don't think in the history of"
    },
    {
      "id": 892,
      "start": 5254.16,
      "end": 5260.96,
      "text": "startups there has been a single application that has been as hotly competed in as coding agents and"
    },
    {
      "id": 893,
      "start": 5260.96,
      "end": 5269.36,
      "text": "um and and and the cloud code is a category leader here and that seems surprising to me like it"
    },
    {
      "id": 894,
      "start": 5269.36,
      "end": 5273.44,
      "text": "doesn't seem intrinsically like anthropic had to build this i wonder if you have an accounting of"
    },
    {
      "id": 895,
      "start": 5274.0,
      "end": 5278.72,
      "text": "why it had to be anthropic or why how anthropic ended up building an application in addition to the"
    },
    {
      "id": 896,
      "start": 5278.72,
      "end": 5282.96,
      "text": "model underlying it yeah so it actually happened in a pretty simple way which is we had our own"
    },
    {
      "id": 897,
      "start": 5283.68,
      "end": 5290.4800000000005,
      "text": "um you know we had our coding models which were good at coding and and you know around the beginning"
    },
    {
      "id": 898,
      "start": 5290.4800000000005,
      "end": 5296.88,
      "text": "of 2025 i said i i think the time has come where you can have non-trivial acceleration of your own"
    },
    {
      "id": 899,
      "start": 5296.88,
      "end": 5303.12,
      "text": "research um if you're an ai company by using these models and of course you know we you need an"
    },
    {
      "id": 900,
      "start": 5303.12,
      "end": 5308.24,
      "text": "interface you need a harness to use them and so i encourage people internally and i didn't say this is"
    },
    {
      "id": 901,
      "start": 5308.24,
      "end": 5314.32,
      "text": "one thing that you know that you have to use i i just said people should experiment with this and then"
    },
    {
      "id": 902,
      "start": 5314.88,
      "end": 5319.679999999999,
      "text": "you know this thing i you know i think it might have been originally called claude cli and then the"
    },
    {
      "id": 903,
      "start": 5319.679999999999,
      "end": 5326.16,
      "text": "name eventually got changed to claude code internally um was the thing that kind of everyone was using and"
    },
    {
      "id": 904,
      "start": 5326.16,
      "end": 5330.719999999999,
      "text": "it was seeing fast internal adoption and i looked at it and i said probably we should launch this"
    },
    {
      "id": 905,
      "start": 5330.719999999999,
      "end": 5337.599999999999,
      "text": "externally right um uh you know it's it's seen such fast adoption within anthropic like you know like you"
    },
    {
      "id": 906,
      "start": 5337.6,
      "end": 5342.320000000001,
      "text": "know coding is a lot of what we do and and so you know we have a we have a audience of many many"
    },
    {
      "id": 907,
      "start": 5342.320000000001,
      "end": 5346.96,
      "text": "hundreds of people that's in some ways at least representative of the external audience so it"
    },
    {
      "id": 908,
      "start": 5346.96,
      "end": 5351.92,
      "text": "looks like we already have product market fit let's launch this thing um and and then we launched it and"
    },
    {
      "id": 909,
      "start": 5351.92,
      "end": 5358.4800000000005,
      "text": "and and i think you know just just the fact that we ourselves are kind of developing the model and"
    },
    {
      "id": 910,
      "start": 5358.4800000000005,
      "end": 5363.4400000000005,
      "text": "we ourselves know what we most need to use the model i think it's it's kind of creating this feedback loop"
    },
    {
      "id": 911,
      "start": 5363.44,
      "end": 5369.599999999999,
      "text": "i see in the sense that you let's say a developer at anthropic is like ah it it'd be better if it was"
    },
    {
      "id": 912,
      "start": 5369.599999999999,
      "end": 5375.839999999999,
      "text": "better at this x thing and then you bake that into the next model that you build that that's that's"
    },
    {
      "id": 913,
      "start": 5375.839999999999,
      "end": 5380.719999999999,
      "text": "one version of it but but then there's just the ordinary product iteration of like you know we have"
    },
    {
      "id": 914,
      "start": 5380.719999999999,
      "end": 5387.12,
      "text": "a bunch of we have a bunch of coders within anthropic like we um you know they like use claude code"
    },
    {
      "id": 915,
      "start": 5387.12,
      "end": 5390.799999999999,
      "text": "every day and so we get fast feedback that was more important in the early days now of course there"
    },
    {
      "id": 916,
      "start": 5390.8,
      "end": 5396.08,
      "text": "are millions of people using it um and so we get a bunch of external feedback as well but it's you"
    },
    {
      "id": 917,
      "start": 5396.08,
      "end": 5402.24,
      "text": "know it's just great to be able to get you know kind of kind of uh um fast fast internal feedback"
    },
    {
      "id": 918,
      "start": 5402.24,
      "end": 5406.56,
      "text": "you know i think this is the reason why we launched a coding model and you know didn't launch a"
    },
    {
      "id": 919,
      "start": 5406.56,
      "end": 5411.84,
      "text": "pharmaceutical company right it's you know you know my background's in in my background's in in like"
    },
    {
      "id": 920,
      "start": 5411.84,
      "end": 5416.24,
      "text": "biology but like we don't have any of the resources that are needed to launch a pharmaceutical company"
    },
    {
      "id": 921,
      "start": 5416.88,
      "end": 5420.400000000001,
      "text": "so there's been a ton of hype around open claw and i want to check it out for myself"
    },
    {
      "id": 922,
      "start": 5420.4,
      "end": 5425.44,
      "text": "i've got a date coming up this weekend and i don't have anything planned yet so i gave open claw a"
    },
    {
      "id": 923,
      "start": 5425.44,
      "end": 5430.32,
      "text": "mercury debit card i set a couple hundred dollar limit and i said surprise me okay so here's the"
    },
    {
      "id": 924,
      "start": 5430.32,
      "end": 5434.879999999999,
      "text": "mac mini it's on and besides having access to my mercury it's totally quarantined and i actually"
    },
    {
      "id": 925,
      "start": 5434.879999999999,
      "end": 5438.719999999999,
      "text": "felt quite comfortable giving you an access to a debit card because mercury makes it super easy to"
    },
    {
      "id": 926,
      "start": 5438.719999999999,
      "end": 5442.719999999999,
      "text": "set up guard rails i was able to customize permissions cap the spend and restrict the category of"
    },
    {
      "id": 927,
      "start": 5442.719999999999,
      "end": 5446.639999999999,
      "text": "purchases i wanted to make sure the debit card worked so i asked open claw to just make a test"
    },
    {
      "id": 928,
      "start": 5446.64,
      "end": 5451.12,
      "text": "transaction and decided to donate a couple bucks to wikipedia besides that i have no idea what's"
    },
    {
      "id": 929,
      "start": 5451.12,
      "end": 5455.68,
      "text": "going to happen i will report back on the next episode about how it goes in the meantime if you"
    },
    {
      "id": 930,
      "start": 5455.68,
      "end": 5460.08,
      "text": "want a personal banking solution that can accommodate all the different ways that people use their money"
    },
    {
      "id": 931,
      "start": 5460.08,
      "end": 5468.160000000001,
      "text": "even experimental ones like this one visit mercury.com personal mercury is a fintech company not an fdic"
    },
    {
      "id": 932,
      "start": 5468.160000000001,
      "end": 5474.08,
      "text": "insured bank banking services provided through choice financial group and column na members fdic you know"
    },
    {
      "id": 933,
      "start": 5474.08,
      "end": 5476.48,
      "text": "she thinks we're getting coffee and walking around the neighborhood"
    },
    {
      "id": 934,
      "start": 5479.12,
      "end": 5486.08,
      "text": "let me ask you about now um making ai go well um it seems like whatever vision we have about"
    },
    {
      "id": 935,
      "start": 5486.96,
      "end": 5493.5199999999995,
      "text": "how ai goes well has to be compatible with two things one is the ability to build and run ais is"
    },
    {
      "id": 936,
      "start": 5493.5199999999995,
      "end": 5500.24,
      "text": "diffusing extremely rapidly and two is that the population of ais the amount we have in their"
    },
    {
      "id": 937,
      "start": 5500.24,
      "end": 5506.24,
      "text": "intelligence will also increase very rapidly and that means that lots of people will be able to"
    },
    {
      "id": 938,
      "start": 5506.24,
      "end": 5512.08,
      "text": "build huge populations of misaligned ais or uh ais which are just like companies which are trying to"
    },
    {
      "id": 939,
      "start": 5512.08,
      "end": 5518.08,
      "text": "increase their uh footprint or have weird psyches like sydney bing but now they're superhuman what is"
    },
    {
      "id": 940,
      "start": 5518.08,
      "end": 5524.16,
      "text": "a vision for a world in which we have an equilibrium that is compatible with lots of different ais some of"
    },
    {
      "id": 941,
      "start": 5524.16,
      "end": 5529.28,
      "text": "which are misaligned running around yeah yeah so i think you know in the adolescence of technology i was kind of"
    },
    {
      "id": 942,
      "start": 5529.28,
      "end": 5537.599999999999,
      "text": "you know skeptical of like the balance of power but i i think i was particularly skeptical of or the thing"
    },
    {
      "id": 943,
      "start": 5537.599999999999,
      "end": 5543.5199999999995,
      "text": "i was specifically skeptical of is you have like three or four of these companies like kind of all"
    },
    {
      "id": 944,
      "start": 5543.5199999999995,
      "end": 5551.04,
      "text": "building models that are kind of dry you know sort of sort of um uh uh like derived from the like"
    },
    {
      "id": 945,
      "start": 5551.04,
      "end": 5557.44,
      "text": "derived from the same thing and uh you know that that these would check each other or or even that kind"
    },
    {
      "id": 946,
      "start": 5557.44,
      "end": 5561.919999999999,
      "text": "of you know any number of them would would would uh would check each other like we might live in a"
    },
    {
      "id": 947,
      "start": 5561.919999999999,
      "end": 5567.44,
      "text": "offense dominant world where you know like one person or one ai model is like smart enough to do"
    },
    {
      "id": 948,
      "start": 5567.44,
      "end": 5573.5199999999995,
      "text": "something that like causes damage for everything else um i think in the i mean in the short run we"
    },
    {
      "id": 949,
      "start": 5573.5199999999995,
      "end": 5579.5199999999995,
      "text": "have a limited number of players now so we can start by within the limited number of players we uh you"
    },
    {
      "id": 950,
      "start": 5579.5199999999995,
      "end": 5584.08,
      "text": "know we kind of you know we we need to put in place the you know the safeguards we need to make sure"
    },
    {
      "id": 951,
      "start": 5584.08,
      "end": 5588.8,
      "text": "everyone does the right alignment work we need to make sure everyone has bio classifiers like you"
    },
    {
      "id": 952,
      "start": 5588.8,
      "end": 5592.8,
      "text": "know those are those are kind of the immediate things we need to do i agree that you know that"
    },
    {
      "id": 953,
      "start": 5592.8,
      "end": 5597.76,
      "text": "that doesn't solve the problem in the long run particularly if the ability of ai models to make"
    },
    {
      "id": 954,
      "start": 5597.76,
      "end": 5604.64,
      "text": "other ai models proliferates then you know the the whole thing can kind of um you know it can become"
    },
    {
      "id": 955,
      "start": 5605.28,
      "end": 5610.88,
      "text": "harder to solve you know i think i think in the long run we need some architecture of governance right"
    },
    {
      "id": 956,
      "start": 5610.88,
      "end": 5616.72,
      "text": "some are some architecture of governance that preserves human freedom but but kind of also"
    },
    {
      "id": 957,
      "start": 5616.72,
      "end": 5624.96,
      "text": "allows us to like you know govern the the very large number of kind of um you know uh uh uh human"
    },
    {
      "id": 958,
      "start": 5624.96,
      "end": 5634.08,
      "text": "systems ai systems hybrid hybrid human human um you know hybrid hybrid human ai like you know"
    },
    {
      "id": 959,
      "start": 5634.72,
      "end": 5640.56,
      "text": "companies or or like or like or like economic units so you know we're gonna need to think about like"
    },
    {
      "id": 960,
      "start": 5640.56,
      "end": 5645.4400000000005,
      "text": "you know how do we how do we protect the world against you know bioterrorism how do we protect"
    },
    {
      "id": 961,
      "start": 5645.4400000000005,
      "end": 5651.04,
      "text": "the world against like you know against like against like mirror life like you know probably"
    },
    {
      "id": 962,
      "start": 5651.04,
      "end": 5656.400000000001,
      "text": "probably we're gonna need to you know need some kind of like ai monitoring system that like mona you"
    },
    {
      "id": 963,
      "start": 5656.400000000001,
      "end": 5661.200000000001,
      "text": "know kind of monitors for for all these things but then we need to build this in a way that like"
    },
    {
      "id": 964,
      "start": 5661.200000000001,
      "end": 5666.96,
      "text": "you know preserve civil liberties and like our constitutional rights so i think just just as is as is"
    },
    {
      "id": 965,
      "start": 5666.96,
      "end": 5674.4800000000005,
      "text": "anything else like it's it's like a new security landscape with a new set of you know a new set of"
    },
    {
      "id": 966,
      "start": 5674.4800000000005,
      "end": 5680.72,
      "text": "tools and a new set of vulnerabilities and i i think my worry is if we had a hundred years for this to"
    },
    {
      "id": 967,
      "start": 5680.72,
      "end": 5686.56,
      "text": "happen all very slowly we'd get used to it you know like we've gotten used to like you know the presence"
    },
    {
      "id": 968,
      "start": 5686.56,
      "end": 5693.52,
      "text": "of you know the presence of explosives in society or like the you know the presence of various um you know"
    },
    {
      "id": 969,
      "start": 5693.52,
      "end": 5698.96,
      "text": "like new weapons or the you know the pre the presence of video cameras um we would get used to"
    },
    {
      "id": 970,
      "start": 5698.96,
      "end": 5704.96,
      "text": "it over over over over over a hundred and we develop governance mechanisms we'd make our mistakes my my"
    },
    {
      "id": 971,
      "start": 5704.96,
      "end": 5709.68,
      "text": "worry is just that this is happening all so fast and so i think maybe we need to do our thinking"
    },
    {
      "id": 972,
      "start": 5709.68,
      "end": 5715.52,
      "text": "faster about how to make these governance mechanisms work yeah it seems like in a offense dominant world"
    },
    {
      "id": 973,
      "start": 5717.120000000001,
      "end": 5720.240000000001,
      "text": "over the course of the next century so the idea is the ai is making the progress that would happen"
    },
    {
      "id": 974,
      "start": 5720.24,
      "end": 5725.44,
      "text": "over the next century happen in some period of five to ten years but we would still need the same"
    },
    {
      "id": 975,
      "start": 5725.44,
      "end": 5731.44,
      "text": "mechanisms or balance of power would be similarly intractable even if humans were the only game in"
    },
    {
      "id": 976,
      "start": 5731.44,
      "end": 5739.76,
      "text": "town um and so i guess we have the advice of ai we it it fundamentally doesn't seem like a totally"
    },
    {
      "id": 977,
      "start": 5739.76,
      "end": 5743.92,
      "text": "different ball game here if checks and balances were going to work they would work with humans as"
    },
    {
      "id": 978,
      "start": 5743.92,
      "end": 5748.48,
      "text": "well if they aren't going to work they wouldn't work with the eyes as well um and so maybe this just"
    },
    {
      "id": 979,
      "start": 5748.48,
      "end": 5753.919999999999,
      "text": "dooms human checks and balances as well but yeah again again i think there's some way to i think"
    },
    {
      "id": 980,
      "start": 5753.919999999999,
      "end": 5758.879999999999,
      "text": "there's some way to make this happen like it you know it just it just you know the governments of"
    },
    {
      "id": 981,
      "start": 5758.879999999999,
      "end": 5764.24,
      "text": "the world may have to work together to make it happen like you know we may have to you may have to talk"
    },
    {
      "id": 982,
      "start": 5764.24,
      "end": 5770.5599999999995,
      "text": "to ais about kind of you know building societal structures in such a way that like these these defenses"
    },
    {
      "id": 983,
      "start": 5770.5599999999995,
      "end": 5775.759999999999,
      "text": "are possible i i don't know i mean this is so this is you know i don't want to say so far ahead in time"
    },
    {
      "id": 984,
      "start": 5775.76,
      "end": 5781.68,
      "text": "but like so far ahead in technological ability that may happen over a short period of time that it's"
    },
    {
      "id": 985,
      "start": 5781.68,
      "end": 5786.4800000000005,
      "text": "hard for us to anticipate it in advance um speaking of governments getting involved on december 26 the"
    },
    {
      "id": 986,
      "start": 5786.4800000000005,
      "end": 5792.64,
      "text": "tennessee legislature introduced a bill which uh said quote um it would be an offense for a person to"
    },
    {
      "id": 987,
      "start": 5792.64,
      "end": 5797.280000000001,
      "text": "knowingly train artificial intelligence to provide emotional support including through open-ended"
    },
    {
      "id": 988,
      "start": 5797.280000000001,
      "end": 5805.2,
      "text": "conversations with the user and of course one of the things that claude attempts to do is be a thoughtful"
    },
    {
      "id": 989,
      "start": 5805.2,
      "end": 5810.0,
      "text": "um a thoughtful friend thoughtful knowledgeable friend and in general it seems like we're going"
    },
    {
      "id": 990,
      "start": 5810.0,
      "end": 5814.48,
      "text": "to have this patchwork of state laws a lot of the benefits that normal people could experience as a"
    },
    {
      "id": 991,
      "start": 5814.48,
      "end": 5818.8,
      "text": "result of ai are going to be curtailed especially when we get into the kinds of things you discuss"
    },
    {
      "id": 992,
      "start": 5818.8,
      "end": 5823.44,
      "text": "in machines of love and grace biological freedom mental health improvements etc etc it seems easy to"
    },
    {
      "id": 993,
      "start": 5823.44,
      "end": 5830.48,
      "text": "imagine worlds in which these can whack them all the way by different laws um whereas bills like this"
    },
    {
      "id": 994,
      "start": 5830.48,
      "end": 5836.719999999999,
      "text": "don't seem to address the actual existential threats that you're concerned about so i'm curious about"
    },
    {
      "id": 995,
      "start": 5836.719999999999,
      "end": 5842.24,
      "text": "to understand in the context of things like this your anthropics position against the federal moratorium"
    },
    {
      "id": 996,
      "start": 5842.24,
      "end": 5847.5199999999995,
      "text": "on state ai laws yes so i don't know there's there's many different things going on at once right i think"
    },
    {
      "id": 997,
      "start": 5847.5199999999995,
      "end": 5852.959999999999,
      "text": "i think that that i think that particular law is is dumb like you know i think it was it was clearly"
    },
    {
      "id": 998,
      "start": 5852.959999999999,
      "end": 5858.879999999999,
      "text": "made by legislators who just probably had little idea what ai models could do and not do they're like ai"
    },
    {
      "id": 999,
      "start": 5858.88,
      "end": 5863.52,
      "text": "models serving as that just sounds scary like i don't want i don't want that to happen so you know"
    },
    {
      "id": 1000,
      "start": 5863.52,
      "end": 5869.2,
      "text": "we're we're not we're not in favor of that right but but but you know that that wasn't the thing that"
    },
    {
      "id": 1001,
      "start": 5869.2,
      "end": 5875.84,
      "text": "was being voted on the thing that was being voted on is we're going to ban all state regulation of ai for"
    },
    {
      "id": 1002,
      "start": 5875.84,
      "end": 5882.96,
      "text": "10 years with no apparent plan to to do any federal regulation of ai which would take congress to pass"
    },
    {
      "id": 1003,
      "start": 5882.96,
      "end": 5888.4,
      "text": "which is a very high bar um so you know the idea that we'd ban states from doing anything for 10"
    },
    {
      "id": 1004,
      "start": 5888.4,
      "end": 5893.84,
      "text": "years and people said they had a plan for federal government but you know there was no actual there"
    },
    {
      "id": 1005,
      "start": 5893.84,
      "end": 5900.16,
      "text": "was no proposal on the table there was no actual attempt um given the serious dangers that i lay out"
    },
    {
      "id": 1006,
      "start": 5900.16,
      "end": 5905.76,
      "text": "in adolescence of technology around things like the you know kind of biological weapons and"
    },
    {
      "id": 1007,
      "start": 5905.76,
      "end": 5912.4,
      "text": "bioterrorism autonomy risk and the timelines we've been talking about like 10 years is an eternity"
    },
    {
      "id": 1008,
      "start": 5912.4,
      "end": 5918.32,
      "text": "like that's that's a that's a i i think that's a crazy thing to do so if if that's the choice if"
    },
    {
      "id": 1009,
      "start": 5918.32,
      "end": 5924.0,
      "text": "that's what you force us to choose then then we're gonna we're gonna choose not to have that moratorium"
    },
    {
      "id": 1010,
      "start": 5924.0,
      "end": 5929.679999999999,
      "text": "and you know i i think the the the benefits of that position exceed the costs but it's it's not a"
    },
    {
      "id": 1011,
      "start": 5929.679999999999,
      "end": 5934.639999999999,
      "text": "perfect position if that's the choice now i think the thing that we should do the thing that i would"
    },
    {
      "id": 1012,
      "start": 5934.639999999999,
      "end": 5941.36,
      "text": "support is the federal government should step in not saying states you can't regulate but here's what"
    },
    {
      "id": 1013,
      "start": 5941.36,
      "end": 5948.32,
      "text": "we're going to do and and states you can't differ from this right like i think preemption is fine in"
    },
    {
      "id": 1014,
      "start": 5948.32,
      "end": 5953.12,
      "text": "the sense of saying that federal government says here's our standard this applies to everyone states"
    },
    {
      "id": 1015,
      "start": 5953.12,
      "end": 5957.839999999999,
      "text": "can't do something different that would be something i would support if it would be done in the right way"
    },
    {
      "id": 1016,
      "start": 5957.839999999999,
      "end": 5963.599999999999,
      "text": "what um but but this idea of states you can't do anything and we're not doing anything either"
    },
    {
      "id": 1017,
      "start": 5964.32,
      "end": 5970.5599999999995,
      "text": "that that struck that struck us as you know very much not making sense and i think will not age well"
    },
    {
      "id": 1018,
      "start": 5970.56,
      "end": 5976.240000000001,
      "text": "it's already starting to not age well with with all the um backlash that that you've seen now in"
    },
    {
      "id": 1019,
      "start": 5976.240000000001,
      "end": 5981.360000000001,
      "text": "terms of in terms of what we would want i mean you know the things we've talked about are are starting"
    },
    {
      "id": 1020,
      "start": 5981.360000000001,
      "end": 5987.280000000001,
      "text": "with transparency standards um uh uh you know in order to monitor some of these autonomy risks and"
    },
    {
      "id": 1021,
      "start": 5987.280000000001,
      "end": 5994.64,
      "text": "bioterrorism risks as the risks become more serious um as we as we get more evidence for them"
    },
    {
      "id": 1022,
      "start": 5994.64,
      "end": 6000.160000000001,
      "text": "then i think we could be more aggressive in some targeted ways and say hey ai bioterrorism"
    },
    {
      "id": 1023,
      "start": 6000.160000000001,
      "end": 6005.92,
      "text": "is really a threat let's let's pass a law that kind of forces people to have classifiers and i could"
    },
    {
      "id": 1024,
      "start": 6005.92,
      "end": 6011.280000000001,
      "text": "even imagine it depends it depends how serious the threat it ends up being we don't know for sure then"
    },
    {
      "id": 1025,
      "start": 6011.280000000001,
      "end": 6015.52,
      "text": "we need to pursue this in an intellectually honest way where we say ahead of time the risk has not"
    },
    {
      "id": 1026,
      "start": 6015.52,
      "end": 6020.96,
      "text": "emerged yet but i could certainly imagine with the pace that things are going that you know i could"
    },
    {
      "id": 1027,
      "start": 6020.96,
      "end": 6026.96,
      "text": "imagine a world where later this year we say hey this this ai bioterrorism stuff is really serious we"
    },
    {
      "id": 1028,
      "start": 6026.96,
      "end": 6030.8,
      "text": "should do something about it we should put it in a federal we should you know put it in a federal"
    },
    {
      "id": 1029,
      "start": 6030.8,
      "end": 6035.2,
      "text": "standard and if the federal government won't act we should put it in a state standard i could totally see"
    },
    {
      "id": 1030,
      "start": 6035.2,
      "end": 6042.8,
      "text": "that i i'm concerned about a world where if you just consider the the pace of progress you're expecting"
    },
    {
      "id": 1031,
      "start": 6042.8,
      "end": 6049.12,
      "text": "the life cycle of of legislation you know the the benefits are as you say because of diffusion"
    },
    {
      "id": 1032,
      "start": 6049.12,
      "end": 6054.4,
      "text": "lag the benefits are slow enough that i really do think this patchwork of on the current trajectory"
    },
    {
      "id": 1033,
      "start": 6054.4,
      "end": 6059.84,
      "text": "this patchwork of state laws would prohibit i mean having an emotional chatbot friend is something"
    },
    {
      "id": 1034,
      "start": 6059.84,
      "end": 6064.96,
      "text": "that freaks people out then just imagine the kinds of actual benefits from ai we want normal people to"
    },
    {
      "id": 1035,
      "start": 6064.96,
      "end": 6069.12,
      "text": "be able to experience from improvements in health and health span and improvements in mental health and"
    },
    {
      "id": 1036,
      "start": 6069.12,
      "end": 6075.2,
      "text": "so forth whereas at the same time uh it seems like you think the dangers are already on the horizon and"
    },
    {
      "id": 1037,
      "start": 6075.2,
      "end": 6082.4,
      "text": "i just don't see that much um it seems like would be especially injurious to the benefits of ai uh as"
    },
    {
      "id": 1038,
      "start": 6082.4,
      "end": 6087.2,
      "text": "compared to the the dangers of ai so that that's maybe the where the cost benefit makes less sense to"
    },
    {
      "id": 1039,
      "start": 6087.2,
      "end": 6091.84,
      "text": "me so so so there's a few things here right i mean people talk about there being thousands of these"
    },
    {
      "id": 1040,
      "start": 6091.84,
      "end": 6099.04,
      "text": "state laws first of all the vast mass majority of them do not pass um and you know the the the you"
    },
    {
      "id": 1041,
      "start": 6099.04,
      "end": 6103.2,
      "text": "know the world works a certain way in theory but like just because the law has been passed doesn't"
    },
    {
      "id": 1042,
      "start": 6103.2,
      "end": 6107.92,
      "text": "mean it's really enforced right the people the people you know implementing it may be like oh my"
    },
    {
      "id": 1043,
      "start": 6107.92,
      "end": 6112.88,
      "text": "god this is stupid it would mean shutting off like you know everything that's ever been built and"
    },
    {
      "id": 1044,
      "start": 6112.88,
      "end": 6117.5199999999995,
      "text": "everything that's ever been built in tennessee so you know very often laws are interpreted in like"
    },
    {
      "id": 1045,
      "start": 6117.5199999999995,
      "end": 6123.04,
      "text": "you know a way that makes them that that makes them not as dangerous or not as harmful on on the same"
    },
    {
      "id": 1046,
      "start": 6123.04,
      "end": 6127.599999999999,
      "text": "side of course you have to worry if you're passing a law to stop a bad thing you had this you had this"
    },
    {
      "id": 1047,
      "start": 6127.6,
      "end": 6136.160000000001,
      "text": "problem as well yeah um uh look my my look i mean my basic view is you know if if if you know we could"
    },
    {
      "id": 1048,
      "start": 6136.160000000001,
      "end": 6141.04,
      "text": "decide you know what laws were passed and how things were done which you know we're only one small input"
    },
    {
      "id": 1049,
      "start": 6141.04,
      "end": 6149.04,
      "text": "input into that you know i would deregulate a lot of the stuff around the health benefits of ai um i think"
    },
    {
      "id": 1050,
      "start": 6149.04,
      "end": 6155.4400000000005,
      "text": "you know i i don't worry as much about the like the the the kind of chatbot laws i actually worry more"
    },
    {
      "id": 1051,
      "start": 6155.44,
      "end": 6163.919999999999,
      "text": "about the drug approval process where i think ai models are going to greatly accelerate um the rate"
    },
    {
      "id": 1052,
      "start": 6163.919999999999,
      "end": 6168.799999999999,
      "text": "at which we discover drugs and just the the pipeline will get jammed up like the pipeline will not be"
    },
    {
      "id": 1053,
      "start": 6168.799999999999,
      "end": 6175.5199999999995,
      "text": "prepared to like process all all the stuff that's going through it so um you know i i i think i think"
    },
    {
      "id": 1054,
      "start": 6175.5199999999995,
      "end": 6182.24,
      "text": "reform of the regulatory process to buy us more towards we have a lot of things coming where the safety"
    },
    {
      "id": 1055,
      "start": 6182.24,
      "end": 6187.44,
      "text": "and the efficacy is actually going to be really crisp and clear like i mean a beautiful thing"
    },
    {
      "id": 1056,
      "start": 6187.44,
      "end": 6192.96,
      "text": "really really crisp and clear and like really really effective but you know and and maybe we don't need"
    },
    {
      "id": 1057,
      "start": 6192.96,
      "end": 6200.719999999999,
      "text": "all this all this um uh uh like um all this superstructure around it that was designed around an era of"
    },
    {
      "id": 1058,
      "start": 6200.719999999999,
      "end": 6206.4,
      "text": "drugs that barely work and often have serious side effects um but at the same time i think we should be"
    },
    {
      "id": 1059,
      "start": 6206.4,
      "end": 6214.719999999999,
      "text": "ramping up quite significantly the um uh you know this this kind of safety and security legislation"
    },
    {
      "id": 1060,
      "start": 6214.719999999999,
      "end": 6221.759999999999,
      "text": "and you know like i've said um you know starting with transparency is is my view of trying not to"
    },
    {
      "id": 1061,
      "start": 6221.759999999999,
      "end": 6226.639999999999,
      "text": "hamper the industry right trying to find the right balance i'm worried about it some people criticize"
    },
    {
      "id": 1062,
      "start": 6226.639999999999,
      "end": 6232.799999999999,
      "text": "my essay for saying that's too slow the dangers of ai will come too soon if we do that well basically i"
    },
    {
      "id": 1063,
      "start": 6232.8,
      "end": 6238.24,
      "text": "kind of think like the last six months and maybe the next few months are going to be about transparency"
    },
    {
      "id": 1064,
      "start": 6238.24,
      "end": 6243.6,
      "text": "and then if these if these risks emerge when we're more certain of them which i think we might be as"
    },
    {
      "id": 1065,
      "start": 6243.6,
      "end": 6249.360000000001,
      "text": "soon as as later this year then i think we need to act very fast in the areas that we've actually seen"
    },
    {
      "id": 1066,
      "start": 6249.360000000001,
      "end": 6255.12,
      "text": "the risk like i think the only way to do this is to be nimble now the legislative process is normally"
    },
    {
      "id": 1067,
      "start": 6255.12,
      "end": 6262.400000000001,
      "text": "not nimble but we we need to emphasize to everyone involved the urgency of this that's why i'm"
    },
    {
      "id": 1068,
      "start": 6262.4,
      "end": 6267.12,
      "text": "sending this message of urgency right that's why i wrote adolescence of technology i wanted"
    },
    {
      "id": 1069,
      "start": 6267.12,
      "end": 6271.839999999999,
      "text": "policy makers to read it i wanted economists to read it i want national security professionals to"
    },
    {
      "id": 1070,
      "start": 6271.839999999999,
      "end": 6277.2,
      "text": "read it you know i want decision makers to read it so that they have some hope of acting faster than"
    },
    {
      "id": 1071,
      "start": 6277.2,
      "end": 6286.24,
      "text": "they would have otherwise is there anything you can do or advocate that would make it more certain"
    },
    {
      "id": 1072,
      "start": 6286.24,
      "end": 6292.5599999999995,
      "text": "that the benefits of ai are um are better instantiated where i feel like you have worked"
    },
    {
      "id": 1073,
      "start": 6292.5599999999995,
      "end": 6295.92,
      "text": "with legislatures to be like okay we're going to prevent bioterrorism here away we're going to"
    },
    {
      "id": 1074,
      "start": 6295.92,
      "end": 6300.32,
      "text": "increase conspiracy we're going to increase whistleblower protection and i just think by default the"
    },
    {
      "id": 1075,
      "start": 6300.32,
      "end": 6305.28,
      "text": "actual ben like the things we're looking forward to here it just seems very easy they seem very fragile"
    },
    {
      "id": 1076,
      "start": 6305.28,
      "end": 6311.679999999999,
      "text": "to uh different kinds of moral panics or political economy problems yeah i don't actually so so i don't"
    },
    {
      "id": 1077,
      "start": 6311.68,
      "end": 6316.8,
      "text": "actually agree that much in the developed world i feel like you know in the developed world like"
    },
    {
      "id": 1078,
      "start": 6316.8,
      "end": 6324.320000000001,
      "text": "markets function pretty well and when there's when there's like a lot of money to be made on something"
    },
    {
      "id": 1079,
      "start": 6324.320000000001,
      "end": 6329.12,
      "text": "and it's clearly the best available alternative it's actually hard for the regulatory system to stop it"
    },
    {
      "id": 1080,
      "start": 6329.12,
      "end": 6334.4800000000005,
      "text": "you know we're we're seeing that in ai itself right i you know like a thing i've been trying to"
    },
    {
      "id": 1081,
      "start": 6334.4800000000005,
      "end": 6340.64,
      "text": "fight for is export controls on chips to china right and like that's in the national security interests of"
    },
    {
      "id": 1082,
      "start": 6340.64,
      "end": 6347.200000000001,
      "text": "the u.s like you know that's like square within the you know the the policy beliefs of you know"
    },
    {
      "id": 1083,
      "start": 6347.200000000001,
      "end": 6353.280000000001,
      "text": "every almost everyone in congress of both parties but and you know i think the case is very clear the"
    },
    {
      "id": 1084,
      "start": 6353.280000000001,
      "end": 6361.280000000001,
      "text": "counter arguments against it are i'll politely call them fishy um uh and yet it doesn't happen and we"
    },
    {
      "id": 1085,
      "start": 6361.280000000001,
      "end": 6367.4400000000005,
      "text": "sell the chips because there's there's so much money there's so much money riding on it um and you know"
    },
    {
      "id": 1086,
      "start": 6367.44,
      "end": 6373.28,
      "text": "the the that money wants to be made and and in that case in my opinion that's a bad thing um and but"
    },
    {
      "id": 1087,
      "start": 6373.28,
      "end": 6379.679999999999,
      "text": "but it also it also applies when when it's a good thing and and so i i don't think that if we're talking"
    },
    {
      "id": 1088,
      "start": 6379.679999999999,
      "end": 6389.36,
      "text": "about drugs and benefits of the technology i i i am not as worried about those benefits being hampered"
    },
    {
      "id": 1089,
      "start": 6389.36,
      "end": 6395.5199999999995,
      "text": "in the developed world i am a little worried about them going too slow and i as i said i do think we should"
    },
    {
      "id": 1090,
      "start": 6395.52,
      "end": 6402.0,
      "text": "work to speed the approval process in the fda i do think we should fight against these chatbot bills"
    },
    {
      "id": 1091,
      "start": 6402.0,
      "end": 6408.080000000001,
      "text": "that you're describing right described individually i'm against them i think they're stupid um but i"
    },
    {
      "id": 1092,
      "start": 6408.080000000001,
      "end": 6413.6,
      "text": "actually think the bigger worry is a developing world um where we don't have functioning markets"
    },
    {
      "id": 1093,
      "start": 6413.6,
      "end": 6419.360000000001,
      "text": "where um you know we often can't build on the technology that that we've had i worry more that"
    },
    {
      "id": 1094,
      "start": 6419.360000000001,
      "end": 6423.68,
      "text": "those folks will get left behind and i worry that even if the cures are developed you know maybe"
    },
    {
      "id": 1095,
      "start": 6423.68,
      "end": 6427.92,
      "text": "there's someone in rural mississippi who who doesn't get it as well right that's a that's a"
    },
    {
      "id": 1096,
      "start": 6427.92,
      "end": 6432.400000000001,
      "text": "that's a kind of smaller version of the thing the concern we have in the in the developing world and"
    },
    {
      "id": 1097,
      "start": 6432.400000000001,
      "end": 6438.08,
      "text": "so the things we've been doing are you know you know we work with you know we work with you know"
    },
    {
      "id": 1098,
      "start": 6438.08,
      "end": 6446.240000000001,
      "text": "philanthropists right you know we work with folks um who you know who you know deliver you know medicine"
    },
    {
      "id": 1099,
      "start": 6446.24,
      "end": 6452.88,
      "text": "and health interventions to you know to to developing world the sub-saharan africa you know india latin"
    },
    {
      "id": 1100,
      "start": 6452.88,
      "end": 6459.04,
      "text": "america you know uh you know other other developing parts of the world that's the thing i think that"
    },
    {
      "id": 1101,
      "start": 6459.04,
      "end": 6465.2,
      "text": "won't happen on its own you mentioned expert controls yeah why can't us and china both have a country of"
    },
    {
      "id": 1102,
      "start": 6465.2,
      "end": 6471.76,
      "text": "geniuses why why can't you know why won't it happen or why no like why shouldn't it happen why shouldn't it"
    },
    {
      "id": 1103,
      "start": 6471.76,
      "end": 6479.280000000001,
      "text": "happen um you know i think i think if this does happen um you know then then we kind of have a"
    },
    {
      "id": 1104,
      "start": 6480.72,
      "end": 6484.72,
      "text": "well we could have a few situations if we have like an offense dominant situation we could have"
    },
    {
      "id": 1105,
      "start": 6484.72,
      "end": 6488.88,
      "text": "a situation like nuclear weapons but like more dangerous right where it's like um you know kind"
    },
    {
      "id": 1106,
      "start": 6488.88,
      "end": 6495.68,
      "text": "of kind of either side could could easily destroy everything um we could also have a world where it's"
    },
    {
      "id": 1107,
      "start": 6495.68,
      "end": 6500.400000000001,
      "text": "kind of it's unstable like the nuclear equilibrium is stable right because it's you know it's like"
    },
    {
      "id": 1108,
      "start": 6500.4,
      "end": 6506.16,
      "text": "deterrence but let's say there were uncertainty about like if the two ais fought which ai would"
    },
    {
      "id": 1109,
      "start": 6506.16,
      "end": 6511.5199999999995,
      "text": "win um that could create instability right you often have conflict when the two sides have a"
    },
    {
      "id": 1110,
      "start": 6511.5199999999995,
      "end": 6516.4,
      "text": "different assessment of their likelihood of winning right if one side is like oh yeah there's a 90"
    },
    {
      "id": 1111,
      "start": 6516.4,
      "end": 6521.36,
      "text": "chance i'll win and the other side's like there's a 90 chance i'll win then then then a fight is much"
    },
    {
      "id": 1112,
      "start": 6521.36,
      "end": 6526.4,
      "text": "more likely um they can't both be right but they can both think that but this is like a fully general"
    },
    {
      "id": 1113,
      "start": 6526.4,
      "end": 6531.36,
      "text": "argument against the diffusion of ai technology which it may which is that's the implication of"
    },
    {
      "id": 1114,
      "start": 6531.36,
      "end": 6536.32,
      "text": "this world let me let let me just go on because i think we will get diffusion eventually the other"
    },
    {
      "id": 1115,
      "start": 6536.32,
      "end": 6542.16,
      "text": "concern i have is that people the governments will oppress their own people with ai and and and so um"
    },
    {
      "id": 1116,
      "start": 6542.96,
      "end": 6547.28,
      "text": "you know i'm i'm just i'm worried about some world where you have a country that's already uh you"
    },
    {
      "id": 1117,
      "start": 6547.28,
      "end": 6553.2,
      "text": "know kind of uh uh uh you know uh uh you know there's there's a government that kind of kind of"
    },
    {
      "id": 1118,
      "start": 6553.2,
      "end": 6560.0,
      "text": "already um you know is is kind of kind of building a you know a tech high-tech authoritarian state um"
    },
    {
      "id": 1119,
      "start": 6560.0,
      "end": 6563.76,
      "text": "and to be clear this is about the government this is not about the people like people we need to find"
    },
    {
      "id": 1120,
      "start": 6563.76,
      "end": 6570.0,
      "text": "a way for people everywhere to benefit um my worry here is about governments um so yeah my you know my"
    },
    {
      "id": 1121,
      "start": 6570.0,
      "end": 6575.28,
      "text": "my worry is if the world gets carved up into two pieces one of those two pieces could be authoritarian"
    },
    {
      "id": 1122,
      "start": 6575.28,
      "end": 6581.679999999999,
      "text": "or totalitarian in a way that's very difficult to displace um now will will governments eventually get"
    },
    {
      "id": 1123,
      "start": 6581.68,
      "end": 6586.400000000001,
      "text": "powerful ai and and you know there's risk of authoritarianism yes will governments eventually"
    },
    {
      "id": 1124,
      "start": 6586.400000000001,
      "end": 6594.4800000000005,
      "text": "get powerful ai and there's risk of um uh you know of of kind of bad bad bad equilibria yes i think"
    },
    {
      "id": 1125,
      "start": 6594.4800000000005,
      "end": 6600.240000000001,
      "text": "both things but the initial conditions matter right you know at some point we're neat we're going to"
    },
    {
      "id": 1126,
      "start": 6600.240000000001,
      "end": 6606.240000000001,
      "text": "need to set up the rules of the road i'm not saying that one country either the united states or a"
    },
    {
      "id": 1127,
      "start": 6606.240000000001,
      "end": 6610.56,
      "text": "coalition of democracies which i think would be a better setup although it requires more international"
    },
    {
      "id": 1128,
      "start": 6610.56,
      "end": 6615.68,
      "text": "cooperation than we currently seem to want to make um but you know i don't i don't think a coalition of"
    },
    {
      "id": 1129,
      "start": 6615.68,
      "end": 6620.96,
      "text": "democracies or or certainly one country should just say these are the rules of the road there's going"
    },
    {
      "id": 1130,
      "start": 6620.96,
      "end": 6627.200000000001,
      "text": "to be some negotiation right the world is going to have to grapple with this and what i would like is"
    },
    {
      "id": 1131,
      "start": 6627.200000000001,
      "end": 6634.320000000001,
      "text": "that the the the you know the democratic nations of the world those with you know who are close whose"
    },
    {
      "id": 1132,
      "start": 6634.320000000001,
      "end": 6640.400000000001,
      "text": "governments have represent closer to pro-human values are are holding the stronger hand then have"
    },
    {
      "id": 1133,
      "start": 6640.4,
      "end": 6645.5199999999995,
      "text": "have have more leverage when the rules of the road are set and and so i'm very concerned about"
    },
    {
      "id": 1134,
      "start": 6645.5199999999995,
      "end": 6650.799999999999,
      "text": "that initial condition um i was really listening to an interview from three years ago and one of"
    },
    {
      "id": 1135,
      "start": 6650.799999999999,
      "end": 6655.12,
      "text": "the ways it aged poorly is that i kept asking questions assuming there's going to be some key"
    },
    {
      "id": 1136,
      "start": 6655.759999999999,
      "end": 6660.719999999999,
      "text": "fulcrum moment two to three years from now when in fact being that far out it just seems like"
    },
    {
      "id": 1137,
      "start": 6660.719999999999,
      "end": 6666.16,
      "text": "progress continues ai improves ai is more diffused and people use it for more things it seems like"
    },
    {
      "id": 1138,
      "start": 6666.16,
      "end": 6670.32,
      "text": "you're imagining a world in the future where the countries get together and here's the rules of"
    },
    {
      "id": 1139,
      "start": 6670.32,
      "end": 6674.4,
      "text": "the road and here's the leverage we have here's the leverage you have when it seems like on current"
    },
    {
      "id": 1140,
      "start": 6674.4,
      "end": 6679.84,
      "text": "trajectory everybody will have more ai um some of that ai will be used by authoritarian countries"
    },
    {
      "id": 1141,
      "start": 6679.84,
      "end": 6684.16,
      "text": "some of that within the authoritarian countries will be raised by private actors versus state actors"
    },
    {
      "id": 1142,
      "start": 6684.16,
      "end": 6688.639999999999,
      "text": "it's not clear who will benefit more it's always unpredictable to tell in advance you know it seems"
    },
    {
      "id": 1143,
      "start": 6688.639999999999,
      "end": 6693.84,
      "text": "like the internet privileged authoritarian countries more than you would have expected um and maybe the ai will"
    },
    {
      "id": 1144,
      "start": 6693.84,
      "end": 6700.400000000001,
      "text": "be the opposite way around um so i i want to better understand what you're imagining here yeah yeah"
    },
    {
      "id": 1145,
      "start": 6700.400000000001,
      "end": 6705.76,
      "text": "so so just to be precise about it i think the exponential of the underlying technology will"
    },
    {
      "id": 1146,
      "start": 6705.76,
      "end": 6710.400000000001,
      "text": "continue as it has before right the models get smarter and smarter even when they get to country of"
    },
    {
      "id": 1147,
      "start": 6710.400000000001,
      "end": 6716.16,
      "text": "geniuses in a data center you know i i think you can continue to make the model smarter there's a"
    },
    {
      "id": 1148,
      "start": 6716.16,
      "end": 6722.72,
      "text": "question of like getting diminishing returns on their value in the world right how much does it"
    },
    {
      "id": 1149,
      "start": 6722.72,
      "end": 6728.400000000001,
      "text": "matter after you've already solved human biology or you know you know at some point you can do harder"
    },
    {
      "id": 1150,
      "start": 6728.400000000001,
      "end": 6733.68,
      "text": "math you can do more abstruse math problems but nothing after that matters but putting that aside"
    },
    {
      "id": 1151,
      "start": 6733.68,
      "end": 6739.68,
      "text": "i do think the the exponential will continue but there will be certain distinguished points on the"
    },
    {
      "id": 1152,
      "start": 6739.68,
      "end": 6748.0,
      "text": "exponential and companies individuals countries will reach those points at different times um and and so"
    },
    {
      "id": 1153,
      "start": 6748.0,
      "end": 6752.16,
      "text": "you know there's there's you know could there be some you know you know i talk about is a nuclear"
    },
    {
      "id": 1154,
      "start": 6752.16,
      "end": 6757.599999999999,
      "text": "deterrent still in adolescence of technology is a nuclear deterrent still stable uh in the world of"
    },
    {
      "id": 1155,
      "start": 6757.599999999999,
      "end": 6762.32,
      "text": "of a i don't know but that's that's an example of like one thing we've taken for granted that like"
    },
    {
      "id": 1156,
      "start": 6762.32,
      "end": 6767.28,
      "text": "the technology could reach such a level that it's no longer like you know we can no longer be certain"
    },
    {
      "id": 1157,
      "start": 6767.28,
      "end": 6772.32,
      "text": "of it at least um uh you know think of think of others you know they're they're they're you know"
    },
    {
      "id": 1158,
      "start": 6772.32,
      "end": 6776.88,
      "text": "they're they're kind of points where if you if you reach a certain point you maybe you have"
    },
    {
      "id": 1159,
      "start": 6776.88,
      "end": 6782.400000000001,
      "text": "offensive cyber dominance and like every every computer system is transparent to you after that"
    },
    {
      "id": 1160,
      "start": 6782.400000000001,
      "end": 6788.88,
      "text": "um unless the other side has it has a kind of equivalent defense so i don't know what the critical"
    },
    {
      "id": 1161,
      "start": 6788.88,
      "end": 6794.32,
      "text": "moment is or if there's a single critical moment but i think there will be either a critical moment"
    },
    {
      "id": 1162,
      "start": 6794.32,
      "end": 6803.2,
      "text": "a small number of critical moments or some critical window where it's like ai is ai confers some"
    },
    {
      "id": 1163,
      "start": 6803.2,
      "end": 6811.5199999999995,
      "text": "large advantage from the perspective of national security and one country or coalition has reached"
    },
    {
      "id": 1164,
      "start": 6811.5199999999995,
      "end": 6816.639999999999,
      "text": "it before others that that you know that that you know i'm not advocating that they're just like okay"
    },
    {
      "id": 1165,
      "start": 6816.639999999999,
      "end": 6821.84,
      "text": "we're in charge now that's not that's not how that's not how i think about it you know that there's"
    },
    {
      "id": 1166,
      "start": 6821.84,
      "end": 6826.72,
      "text": "always the the other side is catching up there's extreme actions you're not willing to take and and"
    },
    {
      "id": 1167,
      "start": 6826.72,
      "end": 6833.68,
      "text": "it's not right to take you know to take complete um to take complete control anyway but but at the"
    },
    {
      "id": 1168,
      "start": 6833.68,
      "end": 6837.92,
      "text": "point that that happens i think people are going to understand that the world has changed and there"
    },
    {
      "id": 1169,
      "start": 6837.92,
      "end": 6846.08,
      "text": "there's going to be some negotiation implicit or implicit about what what is the what is the post ai"
    },
    {
      "id": 1170,
      "start": 6846.08,
      "end": 6856.56,
      "text": "world order look like and and i think my interest is in you know making that negotiation be one in"
    },
    {
      "id": 1171,
      "start": 6856.56,
      "end": 6864.4800000000005,
      "text": "which you know classical liberal democracy has you know has a strong hand well i would understand"
    },
    {
      "id": 1172,
      "start": 6864.4800000000005,
      "end": 6869.92,
      "text": "what that better means because you say in the essay quote autocracy is simply not a form of government"
    },
    {
      "id": 1173,
      "start": 6869.92,
      "end": 6875.92,
      "text": "that people can accept in the post powerful ai age and that sounds like you're saying the ccp"
    },
    {
      "id": 1174,
      "start": 6875.92,
      "end": 6884.88,
      "text": "as an institution cannot exist after we get agi um and that seems like a like a very strong demand"
    },
    {
      "id": 1175,
      "start": 6884.88,
      "end": 6891.84,
      "text": "and it seems to imply a world where the leading lab or the leading country will be able to and by that"
    },
    {
      "id": 1176,
      "start": 6891.84,
      "end": 6898.72,
      "text": "language should get to determine how the world is governed or what kinds of governments are allowed"
    },
    {
      "id": 1177,
      "start": 6898.72,
      "end": 6907.12,
      "text": "and not allowed yeah so when when i um i believe that paragraph was i think i said something like you"
    },
    {
      "id": 1178,
      "start": 6907.12,
      "end": 6913.52,
      "text": "could take it even further and say x so i wasn't i wasn't necessarily endorsing that that that i wasn't"
    },
    {
      "id": 1179,
      "start": 6913.52,
      "end": 6918.240000000001,
      "text": "necessarily endorsing that view i you know i was saying like here's if first you know here's a"
    },
    {
      "id": 1180,
      "start": 6918.240000000001,
      "end": 6922.160000000001,
      "text": "weaker thing that i believe but you know i think i you know i think i said you know we have to worry"
    },
    {
      "id": 1181,
      "start": 6922.160000000001,
      "end": 6926.4800000000005,
      "text": "a lot about authoritarians and you know we should try and you know kind of kind of check them and"
    },
    {
      "id": 1182,
      "start": 6926.4800000000005,
      "end": 6931.92,
      "text": "limit their power like you could take this kind of further much more interventionist view that says like"
    },
    {
      "id": 1183,
      "start": 6932.4800000000005,
      "end": 6938.080000000001,
      "text": "authoritarian countries with ai are these you know the the you know these kind of self-fulfilling"
    },
    {
      "id": 1184,
      "start": 6938.080000000001,
      "end": 6942.320000000001,
      "text": "cycles that you can't that are very hard to displace and so you just need to get rid of them"
    },
    {
      "id": 1185,
      "start": 6942.32,
      "end": 6946.24,
      "text": "from from the beginning that that has exactly all the problems you say which is you know"
    },
    {
      "id": 1186,
      "start": 6947.44,
      "end": 6951.679999999999,
      "text": "you know if you were to make a commitment to overthrowing every authoritarian country i mean"
    },
    {
      "id": 1187,
      "start": 6951.679999999999,
      "end": 6956.0,
      "text": "then they would take a bunch of actions now that like you know that that could could lead to"
    },
    {
      "id": 1188,
      "start": 6956.0,
      "end": 6963.04,
      "text": "instability so that that may or you know that that just that just may not be possible but the point i was"
    },
    {
      "id": 1189,
      "start": 6963.04,
      "end": 6971.759999999999,
      "text": "making that i do endorse is that it is it is quite possible that you know today you know the view or at"
    },
    {
      "id": 1190,
      "start": 6971.76,
      "end": 6976.56,
      "text": "least my view or the view in most the western world is is democracy is a better form of government than"
    },
    {
      "id": 1191,
      "start": 6976.56,
      "end": 6983.360000000001,
      "text": "authoritarianism but it's not like if a country's authoritarian we don't react the way we reacted if"
    },
    {
      "id": 1192,
      "start": 6983.360000000001,
      "end": 6989.12,
      "text": "they committed a genocide or something right and and i'm i guess what i'm saying is i'm a little worried"
    },
    {
      "id": 1193,
      "start": 6989.12,
      "end": 6995.76,
      "text": "that in the age of agi authoritarianism will have a different meaning it will be a graver thing um and and"
    },
    {
      "id": 1194,
      "start": 6995.76,
      "end": 7001.04,
      "text": "we have to decide one way or another how how how to deal with that and the interventionist view is one"
    },
    {
      "id": 1195,
      "start": 7001.04,
      "end": 7008.8,
      "text": "possible view i was exploring such views um you know uh it may end up being the right view it may end up"
    },
    {
      "id": 1196,
      "start": 7008.8,
      "end": 7013.84,
      "text": "being too extreme to be the right view but i do have hope and one piece of hope i have is"
    },
    {
      "id": 1197,
      "start": 7013.84,
      "end": 7024.64,
      "text": "there there is we have seen that as new technologies are invented forms of government become obsolete i"
    },
    {
      "id": 1198,
      "start": 7024.64,
      "end": 7031.360000000001,
      "text": "i mentioned this in adolescence of technology where i said you know like feudalism was basically you know"
    },
    {
      "id": 1199,
      "start": 7031.360000000001,
      "end": 7038.56,
      "text": "like a form of government right and and then when when we invented industrialization feudalism was no"
    },
    {
      "id": 1200,
      "start": 7038.56,
      "end": 7043.76,
      "text": "longer sustainable no longer made sense why is that hope why couldn't that imply that democracy is no"
    },
    {
      "id": 1201,
      "start": 7043.76,
      "end": 7049.360000000001,
      "text": "longer going to be well a competitive system it could right it could go it could go either way"
    },
    {
      "id": 1202,
      "start": 7049.360000000001,
      "end": 7058.0,
      "text": "right but but i actually so i these problems with authoritarianism right that the problems of"
    },
    {
      "id": 1203,
      "start": 7058.0,
      "end": 7066.0,
      "text": "authoritarianism get deeper i just i wonder if that's an indicator of other problems that authoritarianism"
    },
    {
      "id": 1204,
      "start": 7066.0,
      "end": 7073.68,
      "text": "will have right in other words people become because authoritarianism becomes worse people are"
    },
    {
      "id": 1205,
      "start": 7073.68,
      "end": 7079.4400000000005,
      "text": "more afraid of authoritarianism they work harder to stop it it's it's more of a kid like you have"
    },
    {
      "id": 1206,
      "start": 7079.4400000000005,
      "end": 7087.68,
      "text": "to think in terms of total equilibrium right um i just wonder if it will motivate new ways of thinking"
    },
    {
      "id": 1207,
      "start": 7087.68,
      "end": 7094.0,
      "text": "about with with with the new technology how to preserve and protect freedom and and uh even more"
    },
    {
      "id": 1208,
      "start": 7094.0,
      "end": 7102.08,
      "text": "optimistically will it lead to a collective reckoning and you know a a a kind of a more emphatic"
    },
    {
      "id": 1209,
      "start": 7102.08,
      "end": 7108.72,
      "text": "realization of how important some of the things we take as individual rights are right a more emphatic"
    },
    {
      "id": 1210,
      "start": 7108.72,
      "end": 7114.32,
      "text": "realization that we just we really can't give these away there's there we've seen there's no other way"
    },
    {
      "id": 1211,
      "start": 7114.32,
      "end": 7123.44,
      "text": "to live that actually works um i i i i am actually i am actually hopeful that i i guess one way to say it"
    },
    {
      "id": 1212,
      "start": 7123.44,
      "end": 7129.92,
      "text": "it sounds too idealistic but i actually believe it could be the case is that is that dictatorships become"
    },
    {
      "id": 1213,
      "start": 7129.92,
      "end": 7135.92,
      "text": "morally obsolete they become morally unworkable forms of government um and that and that and that the the"
    },
    {
      "id": 1214,
      "start": 7135.92,
      "end": 7142.24,
      "text": "the the the crisis that that creates is is is sufficient to force us to find another way um"
    },
    {
      "id": 1215,
      "start": 7143.4400000000005,
      "end": 7146.64,
      "text": "i think there is genuinely a tough question here which i'm not sure how you resolve"
    },
    {
      "id": 1216,
      "start": 7147.52,
      "end": 7152.88,
      "text": "and we've had to come out one way or another on it through history right so with china in the 70s"
    },
    {
      "id": 1217,
      "start": 7152.88,
      "end": 7158.08,
      "text": "and 80s we decided even though it's an authoritarian system we will engage with it i think in retrospect"
    },
    {
      "id": 1218,
      "start": 7158.08,
      "end": 7162.16,
      "text": "that was the right call because in a state authoritarian system but a billion plus people"
    },
    {
      "id": 1219,
      "start": 7162.16,
      "end": 7166.4,
      "text": "are much wealthier and better off than they would have otherwise been um and it's not clear that it"
    },
    {
      "id": 1220,
      "start": 7166.4,
      "end": 7170.64,
      "text": "would have stopped being an authoritarian country otherwise you can just look at north korea uh as an"
    },
    {
      "id": 1221,
      "start": 7170.64,
      "end": 7176.32,
      "text": "example of that right and i don't know if that takes that much that much intelligence to remain an"
    },
    {
      "id": 1222,
      "start": 7176.32,
      "end": 7182.72,
      "text": "authoritarian country that continues to coalesce its own power as you can just imagine a north korea with"
    },
    {
      "id": 1223,
      "start": 7182.72,
      "end": 7187.92,
      "text": "an ai that's much worse than everybody else's but still enough to keep power and and and so in general"
    },
    {
      "id": 1224,
      "start": 7187.92,
      "end": 7193.36,
      "text": "it seems like should we just have this attitude of the benefits of ai will in the form of all"
    },
    {
      "id": 1225,
      "start": 7193.36,
      "end": 7198.96,
      "text": "these empowerments of humanity and health and so forth will be big and and historically we have"
    },
    {
      "id": 1226,
      "start": 7198.96,
      "end": 7203.76,
      "text": "decided it's good to spread the benefits of technology widely even with even to people whose"
    },
    {
      "id": 1227,
      "start": 7203.76,
      "end": 7207.28,
      "text": "governments are authoritarian and i think i guess it is a tough question how to think about it with"
    },
    {
      "id": 1228,
      "start": 7207.28,
      "end": 7213.36,
      "text": "ai but um historically we have said yes this is a positive sum world and it's still worth diffusing"
    },
    {
      "id": 1229,
      "start": 7213.36,
      "end": 7218.4,
      "text": "technology yeah so so there are a number of choices we have i you know i think framing this as"
    },
    {
      "id": 1230,
      "start": 7219.04,
      "end": 7225.599999999999,
      "text": "a kind of government to government decision and you know in in national security terms that's like one"
    },
    {
      "id": 1231,
      "start": 7225.599999999999,
      "end": 7229.92,
      "text": "lens but there are a lot of other lenses like you could imagine a world where you know we produce all"
    },
    {
      "id": 1232,
      "start": 7229.92,
      "end": 7235.679999999999,
      "text": "these cures to diseases and like the you know the the cures to diseases are fine to sell to authoritarian"
    },
    {
      "id": 1233,
      "start": 7235.679999999999,
      "end": 7240.719999999999,
      "text": "countries the data centers just aren't right the chips and the data centers just aren't um and and"
    },
    {
      "id": 1234,
      "start": 7240.72,
      "end": 7247.360000000001,
      "text": "the ai industry itself um uh you know like like another possibility is and and i think folks should"
    },
    {
      "id": 1235,
      "start": 7247.360000000001,
      "end": 7253.04,
      "text": "think about this like you know could there be developments we can make either that naturally"
    },
    {
      "id": 1236,
      "start": 7253.04,
      "end": 7260.0,
      "text": "happened as a result of ai or that we could make happen by building technology on ai could we create an"
    },
    {
      "id": 1237,
      "start": 7260.0,
      "end": 7266.8,
      "text": "equilibrium where where it becomes infeasible for authoritarian countries to deny their people"
    },
    {
      "id": 1238,
      "start": 7266.8,
      "end": 7272.08,
      "text": "kind of private use of the benefits of the technology um uh you know are there are there are"
    },
    {
      "id": 1239,
      "start": 7272.08,
      "end": 7277.360000000001,
      "text": "there are there equilibria where we can kind of give everyone an authoritarian country their own ai"
    },
    {
      "id": 1240,
      "start": 7277.360000000001,
      "end": 7282.16,
      "text": "model that kind of you know you know like defends themselves from surveillance and there isn't a way"
    },
    {
      "id": 1241,
      "start": 7282.16,
      "end": 7287.52,
      "text": "for the authoritarian country to like crack crack down on this while while retaining power i don't know"
    },
    {
      "id": 1242,
      "start": 7287.52,
      "end": 7292.4800000000005,
      "text": "that that sounds to me like if that went far enough it would be it would be a reason why authoritarian"
    },
    {
      "id": 1243,
      "start": 7292.48,
      "end": 7297.599999999999,
      "text": "countries would disintegrate from the inside um but but maybe there's a middle world where like there's"
    },
    {
      "id": 1244,
      "start": 7297.599999999999,
      "end": 7303.2,
      "text": "an equilibrium where if they want to hold on to power the authoritarians can't deny kind of individualized"
    },
    {
      "id": 1245,
      "start": 7303.2,
      "end": 7308.719999999999,
      "text": "access access to the technology but i actually do have a hope for the for the um for the for the more"
    },
    {
      "id": 1246,
      "start": 7308.719999999999,
      "end": 7314.5599999999995,
      "text": "radical version which is you know is it possible that the technology might inherently have properties"
    },
    {
      "id": 1247,
      "start": 7314.5599999999995,
      "end": 7321.04,
      "text": "or that by building on it in certain ways we could create properties um that that that have this kind of"
    },
    {
      "id": 1248,
      "start": 7321.04,
      "end": 7326.96,
      "text": "dissolving effect on authoritarian structures now we we hoped originally right we think about back to"
    },
    {
      "id": 1249,
      "start": 7326.96,
      "end": 7332.16,
      "text": "the beginning of the obama administration we thought originally that that you know social media and and"
    },
    {
      "id": 1250,
      "start": 7332.16,
      "end": 7337.76,
      "text": "the internet would have that property turns out not to but but i i don't know what what if we could"
    },
    {
      "id": 1251,
      "start": 7337.76,
      "end": 7342.0,
      "text": "uh what if we could try again with with the knowledge of how many things could go wrong and that this"
    },
    {
      "id": 1252,
      "start": 7342.0,
      "end": 7346.64,
      "text": "is a different technology i don't know that it would work but it's worth a try yeah i i think it's just"
    },
    {
      "id": 1253,
      "start": 7347.2,
      "end": 7350.64,
      "text": "it's very unpredictable like there's first principles reasons why authoritarianism"
    },
    {
      "id": 1254,
      "start": 7350.64,
      "end": 7355.4400000000005,
      "text": "it's all it's all very unpredictable i i don't think i mean we got it we we just got to we kind"
    },
    {
      "id": 1255,
      "start": 7355.4400000000005,
      "end": 7360.240000000001,
      "text": "of we got to recognize the problem and then we got to come up with 10 things we can try and we got"
    },
    {
      "id": 1256,
      "start": 7360.240000000001,
      "end": 7365.200000000001,
      "text": "to try those and then assess whether they're working or which ones are working if any and then try new"
    },
    {
      "id": 1257,
      "start": 7365.200000000001,
      "end": 7370.64,
      "text": "ones if the old ones aren't but i guess what that nets out to today is you say we will not sell data"
    },
    {
      "id": 1258,
      "start": 7370.64,
      "end": 7376.96,
      "text": "centers or sorry chips and then the ability to make chips to china and so in some sense you are denying"
    },
    {
      "id": 1259,
      "start": 7377.6,
      "end": 7381.68,
      "text": "there would be some benefits to that's right the chinese economy chinese people etc because we're"
    },
    {
      "id": 1260,
      "start": 7381.68,
      "end": 7385.44,
      "text": "doing that and then there'd also be benefits to the american economy because it's a positive"
    },
    {
      "id": 1261,
      "start": 7385.44,
      "end": 7388.88,
      "text": "sum world we could trade they could have their country data centers doing one thing we could have"
    },
    {
      "id": 1262,
      "start": 7388.88,
      "end": 7397.2,
      "text": "ours doing another and already we you're saying it's not worth that positive sum uh stipend to"
    },
    {
      "id": 1263,
      "start": 7398.0,
      "end": 7403.52,
      "text": "empower those countries what i would say is that you know we are we are about to be in a world where"
    },
    {
      "id": 1264,
      "start": 7404.160000000001,
      "end": 7408.96,
      "text": "growth and economic value will come very easily if right if we're able to build these powerful ai"
    },
    {
      "id": 1265,
      "start": 7408.96,
      "end": 7415.040000000001,
      "text": "models growth and economic value will come very easily what will not come easily is distribution of"
    },
    {
      "id": 1266,
      "start": 7415.040000000001,
      "end": 7421.92,
      "text": "benefits distribution of wealth political freedom um you know these are the things that are going to be"
    },
    {
      "id": 1267,
      "start": 7421.92,
      "end": 7429.120000000001,
      "text": "hard to achieve and so when i think about policy i think i think that the technology in the market"
    },
    {
      "id": 1268,
      "start": 7429.12,
      "end": 7434.24,
      "text": "will deliver all the fundamental benefits you know almost almost faster than we can take them"
    },
    {
      "id": 1269,
      "start": 7434.24,
      "end": 7440.5599999999995,
      "text": "um uh and and that these questions about about distribution and political freedom and rights are"
    },
    {
      "id": 1270,
      "start": 7440.5599999999995,
      "end": 7445.2,
      "text": "are are the ones that that will actually matter and that policy should focus on okay so speaking of"
    },
    {
      "id": 1271,
      "start": 7445.2,
      "end": 7452.48,
      "text": "distribution as you're mentioning we have developing countries and um in many cases catch-up growth has"
    },
    {
      "id": 1272,
      "start": 7452.48,
      "end": 7456.64,
      "text": "been weaker than we would have hoped for but when catch-up growth does happen it's fundamentally because"
    },
    {
      "id": 1273,
      "start": 7456.64,
      "end": 7460.72,
      "text": "they have underutilized labor and we can bring the capital and know-how from developed countries to"
    },
    {
      "id": 1274,
      "start": 7460.72,
      "end": 7466.8,
      "text": "these countries and then they can grow quite rapidly yes obviously in a world where labor is no longer"
    },
    {
      "id": 1275,
      "start": 7466.8,
      "end": 7472.4800000000005,
      "text": "the constraining factor this mechanism no longer works it's just the hope basically to rely on"
    },
    {
      "id": 1276,
      "start": 7472.4800000000005,
      "end": 7476.240000000001,
      "text": "philanthropy from the people who immediately get wealthy from ai or from the countries that get"
    },
    {
      "id": 1277,
      "start": 7476.240000000001,
      "end": 7481.4400000000005,
      "text": "wealthy from ai what is the i mean i mean philanthropy should obviously play some role as it has"
    },
    {
      "id": 1278,
      "start": 7481.44,
      "end": 7487.28,
      "text": "the the you know as it has as in the past but i think growth is always growth is always better and"
    },
    {
      "id": 1279,
      "start": 7487.28,
      "end": 7492.4,
      "text": "stronger if we can make it endogenous yeah so you know what are the relevant industries in like in"
    },
    {
      "id": 1280,
      "start": 7492.4,
      "end": 7497.44,
      "text": "like in like in like in like an ai driven world look there's lots of stuff you know like there's you"
    },
    {
      "id": 1281,
      "start": 7497.44,
      "end": 7500.879999999999,
      "text": "know i said i said we shouldn't build data centers in china but there's no reason we shouldn't build"
    },
    {
      "id": 1282,
      "start": 7500.879999999999,
      "end": 7505.5199999999995,
      "text": "data centers in africa right um in fact i think it'd be great to build data centers in africa"
    },
    {
      "id": 1283,
      "start": 7505.5199999999995,
      "end": 7509.679999999999,
      "text": "um you know as long as they're not owned by china we should we should build we should build data"
    },
    {
      "id": 1284,
      "start": 7509.68,
      "end": 7514.4800000000005,
      "text": "centers in africa i think that's a that's that's i think that's a great thing to do um you know"
    },
    {
      "id": 1285,
      "start": 7514.4800000000005,
      "end": 7519.92,
      "text": "we should also build you know there's no reason we can't build you know a pharmaceutical industry"
    },
    {
      "id": 1286,
      "start": 7519.92,
      "end": 7526.400000000001,
      "text": "that's like ai driven like you know the the if ai is accelerating accelerating drug discovery then"
    },
    {
      "id": 1287,
      "start": 7526.400000000001,
      "end": 7530.08,
      "text": "you know there will be a bunch of biotech startups like let's make sure some of those happen in the"
    },
    {
      "id": 1288,
      "start": 7530.08,
      "end": 7534.400000000001,
      "text": "developing world and certainly during the transition i mean we can talk about the point where humans"
    },
    {
      "id": 1289,
      "start": 7534.400000000001,
      "end": 7538.88,
      "text": "have no role but but humans will have still have some role in starting up these companies and"
    },
    {
      "id": 1290,
      "start": 7538.88,
      "end": 7543.28,
      "text": "supervising supervising the ai models so let's make sure some of those humans are humans in the"
    },
    {
      "id": 1291,
      "start": 7543.28,
      "end": 7548.08,
      "text": "developing world so that fast growth can happen there as well you guys recently announced quad is"
    },
    {
      "id": 1292,
      "start": 7548.08,
      "end": 7551.68,
      "text": "going to have a constitution that's aligned to a set of values and not necessarily just the end user"
    },
    {
      "id": 1293,
      "start": 7552.4800000000005,
      "end": 7557.04,
      "text": "and there's a world you can imagine where if it is aligned to the end user it preserves the balance"
    },
    {
      "id": 1294,
      "start": 7557.04,
      "end": 7561.04,
      "text": "of power we have in the world today because everybody gets to have their own ai that's advocating for"
    },
    {
      "id": 1295,
      "start": 7561.04,
      "end": 7565.76,
      "text": "them and so the ratio of bad actors to good actors stays constant it seems to work out for our world today"
    },
    {
      "id": 1296,
      "start": 7565.76,
      "end": 7573.360000000001,
      "text": "um why is it better not to do that but to have a specific set of values that the ai should carry"
    },
    {
      "id": 1297,
      "start": 7573.360000000001,
      "end": 7579.68,
      "text": "forward uh yeah so i'm not sure i'd quite draw the distinction in that way um there there may be two"
    },
    {
      "id": 1298,
      "start": 7579.68,
      "end": 7586.16,
      "text": "relevant distinctions here which are i think you're talking about a mix of the two like one is should we"
    },
    {
      "id": 1299,
      "start": 7586.16,
      "end": 7593.04,
      "text": "give the model a set of instructions about do this and versus don't do this and the other you know versus"
    },
    {
      "id": 1300,
      "start": 7593.04,
      "end": 7599.6,
      "text": "should we give the model a set of principles for you know for kind of how to act um and and and there"
    },
    {
      "id": 1301,
      "start": 7599.6,
      "end": 7606.8,
      "text": "it's it's you know it's it you know it's it's just it's kind of purely a practical and empirical thing"
    },
    {
      "id": 1302,
      "start": 7606.8,
      "end": 7612.8,
      "text": "that we've observed that by teaching the model principles getting it to learn from principles"
    },
    {
      "id": 1303,
      "start": 7612.8,
      "end": 7618.8,
      "text": "its behavior is more consistent it's easier to cover edge cases and the model is more likely to do what"
    },
    {
      "id": 1304,
      "start": 7618.8,
      "end": 7623.84,
      "text": "people want it to do in other words if you know if you're like you know don't tell people how to hot"
    },
    {
      "id": 1305,
      "start": 7623.84,
      "end": 7630.24,
      "text": "wire a car don't speak in korean don't you know you know just you know if you give it a list of rules"
    },
    {
      "id": 1306,
      "start": 7630.24,
      "end": 7635.360000000001,
      "text": "it doesn't really understand the rules and it's kind of hard to generalize from them um you know if it's"
    },
    {
      "id": 1307,
      "start": 7635.360000000001,
      "end": 7640.96,
      "text": "just kind of a like you know list of do's and don'ts whereas if you give it principles and then you"
    },
    {
      "id": 1308,
      "start": 7640.96,
      "end": 7645.4400000000005,
      "text": "know it has some hard guard rails like don't make biological weapons but overall you're trying to"
    },
    {
      "id": 1309,
      "start": 7645.44,
      "end": 7652.16,
      "text": "understand what it should be aiming to do how it should be aiming to operate so just from a practical"
    },
    {
      "id": 1310,
      "start": 7652.16,
      "end": 7656.48,
      "text": "perspective that turns out to be just a more effective way to train the model that's one piece"
    },
    {
      "id": 1311,
      "start": 7656.48,
      "end": 7661.36,
      "text": "of it so that you know it's the kind of rules versus principles trade-off then there's another thing"
    },
    {
      "id": 1312,
      "start": 7661.36,
      "end": 7667.839999999999,
      "text": "you're talking about which is kind of like the courage ability versus um like you know i would say"
    },
    {
      "id": 1313,
      "start": 7667.839999999999,
      "end": 7673.36,
      "text": "kind of intrinsic motivation trade-off which is like how much should the model be a kind of"
    },
    {
      "id": 1314,
      "start": 7673.36,
      "end": 7679.759999999999,
      "text": "i don't know like a a skin suit or something where you know you know you know you just kind of you"
    },
    {
      "id": 1315,
      "start": 7679.759999999999,
      "end": 7684.799999999999,
      "text": "know it just kind of directly follows the instructions that are given to it by whoever is giving it those"
    },
    {
      "id": 1316,
      "start": 7684.799999999999,
      "end": 7690.639999999999,
      "text": "instructions um versus how much should the model have an inherent set of values and and go off and"
    },
    {
      "id": 1317,
      "start": 7690.639999999999,
      "end": 7698.719999999999,
      "text": "do things on its own um and and and and and there i i would actually say everything about the model is"
    },
    {
      "id": 1318,
      "start": 7698.72,
      "end": 7703.4400000000005,
      "text": "actually closer to the direction of of like you know it should mostly do what people want it should"
    },
    {
      "id": 1319,
      "start": 7703.4400000000005,
      "end": 7707.92,
      "text": "mostly follow these we're not trying to build something that like you know goes off and runs"
    },
    {
      "id": 1320,
      "start": 7707.92,
      "end": 7713.6,
      "text": "the world on its own we're actually pretty far on the corrigible side now now what we do say is there"
    },
    {
      "id": 1321,
      "start": 7713.6,
      "end": 7719.12,
      "text": "are certain things that the model won't do right that it's like you know that that that i think we say"
    },
    {
      "id": 1322,
      "start": 7719.12,
      "end": 7724.08,
      "text": "it in various ways in the constitution that under normal circumstances if someone asks the model to do a"
    },
    {
      "id": 1323,
      "start": 7724.08,
      "end": 7730.32,
      "text": "task you should do that task that that should be the default um but if you've asked it to do something"
    },
    {
      "id": 1324,
      "start": 7730.32,
      "end": 7738.96,
      "text": "dangerous or if you've you know if you've um asked it to um you know uh uh to kind of harm someone else"
    },
    {
      "id": 1325,
      "start": 7738.96,
      "end": 7743.5199999999995,
      "text": "um then the model is unwilling to do that so i i actually think of it as like a mostly"
    },
    {
      "id": 1326,
      "start": 7744.24,
      "end": 7750.72,
      "text": "a mostly corrigible model that has some limits but those limits are based on principles yeah i mean then"
    },
    {
      "id": 1327,
      "start": 7750.72,
      "end": 7755.04,
      "text": "the fundamental question is how are those principles determined and this is not a special question for"
    },
    {
      "id": 1328,
      "start": 7755.04,
      "end": 7761.360000000001,
      "text": "anthropic this would be a question for any company but um uh because you have been the ones to actually"
    },
    {
      "id": 1329,
      "start": 7762.0,
      "end": 7767.2,
      "text": "write down the principles i get to ask you this question normally a constitution is like you write"
    },
    {
      "id": 1330,
      "start": 7767.2,
      "end": 7772.72,
      "text": "it down it's set in stone and there's a process of updating it and changing it and so forth in this"
    },
    {
      "id": 1331,
      "start": 7772.72,
      "end": 7779.280000000001,
      "text": "case it seems like a document that people in anthropic write that can be changed at any time that guides"
    },
    {
      "id": 1332,
      "start": 7779.28,
      "end": 7785.84,
      "text": "the behavior of systems are going to be the basis of a lot of economic activity what is the how do you"
    },
    {
      "id": 1333,
      "start": 7785.84,
      "end": 7793.12,
      "text": "think about how those principles should be set yes um so i think there's there's two there's maybe three"
    },
    {
      "id": 1334,
      "start": 7794.719999999999,
      "end": 7799.679999999999,
      "text": "three kind of sizes of loop here right three ways to iterate one is you can iterate we iterate within"
    },
    {
      "id": 1335,
      "start": 7799.679999999999,
      "end": 7804.24,
      "text": "anthropic we train the model we're not happy with it and we kind of change the constitution and i think"
    },
    {
      "id": 1336,
      "start": 7804.24,
      "end": 7809.36,
      "text": "that's good to do um and you know putting out publicly you know making updates to the constitution"
    },
    {
      "id": 1337,
      "start": 7809.36,
      "end": 7812.48,
      "text": "every once in a while saying here's a new constitution right i think that's good to do"
    },
    {
      "id": 1338,
      "start": 7812.48,
      "end": 7817.2,
      "text": "because people can comment on it the second level of loop is different companies will have different"
    },
    {
      "id": 1339,
      "start": 7817.2,
      "end": 7822.88,
      "text": "constitutions um and you know i think it's useful for like anthropic puts out a constitution and you"
    },
    {
      "id": 1340,
      "start": 7822.88,
      "end": 7828.8,
      "text": "know the gemini model puts out a constitution and you know other companies put out a constitution and"
    },
    {
      "id": 1341,
      "start": 7828.8,
      "end": 7835.04,
      "text": "then they can kind of look at them compare outside observers can critique and say this this i like"
    },
    {
      "id": 1342,
      "start": 7835.04,
      "end": 7840.0,
      "text": "this one this thing from this constitution and this thing for that constitution and and then kind of"
    },
    {
      "id": 1343,
      "start": 7840.0,
      "end": 7845.92,
      "text": "that that creates some kind of you know soft incentive and feedback for all the companies to like take the"
    },
    {
      "id": 1344,
      "start": 7845.92,
      "end": 7851.4400000000005,
      "text": "best of each elements and improve then i think there's a third loop which is you know society beyond the"
    },
    {
      "id": 1345,
      "start": 7851.4400000000005,
      "end": 7857.4400000000005,
      "text": "ai companies and beyond just those who kind of you know who who comment on the constitutions without"
    },
    {
      "id": 1346,
      "start": 7857.44,
      "end": 7862.4,
      "text": "hard power and and there you know we've done some experiments like you know a couple years ago we"
    },
    {
      "id": 1347,
      "start": 7862.4,
      "end": 7867.599999999999,
      "text": "did an experiment with i think it was called the collective intelligence project to like um you"
    },
    {
      "id": 1348,
      "start": 7867.599999999999,
      "end": 7873.839999999999,
      "text": "know to to basically pull people and ask them what should be in our ai constitution um uh and and you know"
    },
    {
      "id": 1349,
      "start": 7874.799999999999,
      "end": 7878.96,
      "text": "i think at the time we incorporated some of those changes and so you could imagine with the new"
    },
    {
      "id": 1350,
      "start": 7878.96,
      "end": 7883.839999999999,
      "text": "approach we've taken to the constitution doing something like that it's a little harder because it's"
    },
    {
      "id": 1351,
      "start": 7883.839999999999,
      "end": 7887.28,
      "text": "like that was actually an easier approach to take when the constitution was like a list of"
    },
    {
      "id": 1352,
      "start": 7887.28,
      "end": 7892.24,
      "text": "do's and don'ts um at the level of principles it has to have a certain amount of coherence"
    },
    {
      "id": 1353,
      "start": 7892.24,
      "end": 7897.36,
      "text": "um but but you could you could still imagine getting views from a wide variety of people"
    },
    {
      "id": 1354,
      "start": 7897.36,
      "end": 7902.16,
      "text": "and i think you could also imagine and this is like a crazy idea but hey you know this whole interview"
    },
    {
      "id": 1355,
      "start": 7902.16,
      "end": 7907.84,
      "text": "is about crazy ideas right so um uh you know you could even imagine systems of of kind of"
    },
    {
      "id": 1356,
      "start": 7907.84,
      "end": 7912.88,
      "text": "representative government having having input right like you know i wouldn't i wouldn't do this"
    },
    {
      "id": 1357,
      "start": 7912.88,
      "end": 7917.2,
      "text": "today because the legislative process is so slow like this is exactly why i think we should be"
    },
    {
      "id": 1358,
      "start": 7917.2,
      "end": 7921.12,
      "text": "careful about the legislative process and ai regulation but there's no reason you couldn't"
    },
    {
      "id": 1359,
      "start": 7921.12,
      "end": 7927.52,
      "text": "in principle say like you know all ai you know all ai models have to have a constitution that starts with"
    },
    {
      "id": 1360,
      "start": 7927.52,
      "end": 7933.12,
      "text": "like these things and then like you can append you can append other things after it but like there has to"
    },
    {
      "id": 1361,
      "start": 7933.12,
      "end": 7938.8,
      "text": "be this special section that like takes precedence i wouldn't do that that's too rigid that that sounds"
    },
    {
      "id": 1362,
      "start": 7938.8,
      "end": 7944.400000000001,
      "text": "um you know that that that that sounds kind of overly prescriptive in a way that i think overly"
    },
    {
      "id": 1363,
      "start": 7944.400000000001,
      "end": 7948.88,
      "text": "aggressive legislation is but like that is the thing you could you know like like that is that"
    },
    {
      "id": 1364,
      "start": 7948.88,
      "end": 7953.4400000000005,
      "text": "that is the thing you could try to do is is there some much less heavy-handed version of that maybe"
    },
    {
      "id": 1365,
      "start": 7954.08,
      "end": 7960.4800000000005,
      "text": "i really like control loop too um where obviously this is not how constitutions of actual governments"
    },
    {
      "id": 1366,
      "start": 7960.4800000000005,
      "end": 7966.56,
      "text": "do or should work where there's not this vague sense in which the supreme court will feel out how"
    },
    {
      "id": 1367,
      "start": 7966.56,
      "end": 7970.0,
      "text": "people are feeling and what are the vibes and then update the of the constitution accordingly so"
    },
    {
      "id": 1368,
      "start": 7970.0,
      "end": 7976.160000000001,
      "text": "there's yeah with actual governments there's a more procedural process yeah exactly but you actually"
    },
    {
      "id": 1369,
      "start": 7976.160000000001,
      "end": 7982.88,
      "text": "have a vision of competition between constitutions which is actually very reminiscent of how um some"
    },
    {
      "id": 1370,
      "start": 7983.52,
      "end": 7987.52,
      "text": "libertarian charter cities people used to talk about what an archipelago of different kinds of"
    },
    {
      "id": 1371,
      "start": 7987.52,
      "end": 7991.120000000001,
      "text": "governments would look like and then there would be selection among them of who could operate the most"
    },
    {
      "id": 1372,
      "start": 7991.120000000001,
      "end": 7995.92,
      "text": "effectively yeah in which place people would be the happiest and in a sense you're actually"
    },
    {
      "id": 1373,
      "start": 7996.8,
      "end": 8001.6,
      "text": "yeah there's this vision i'm i'm kind of recreating that yeah yeah like the utopia of archipelago you"
    },
    {
      "id": 1374,
      "start": 8001.6,
      "end": 8007.120000000001,
      "text": "know again you know i think i think that vision has has you know if things to recommend it and things"
    },
    {
      "id": 1375,
      "start": 8007.120000000001,
      "end": 8011.4400000000005,
      "text": "that things that things that will kind of kind of go wrong with it you know i think i think it's a i"
    },
    {
      "id": 1376,
      "start": 8011.4400000000005,
      "end": 8015.280000000001,
      "text": "think it's an interesting in some ways compelling vision but also things will go wrong with it that"
    },
    {
      "id": 1377,
      "start": 8015.280000000001,
      "end": 8022.4800000000005,
      "text": "you hadn't that you hadn't imagined so you know i i like loop two as well but i i i feel like the whole"
    },
    {
      "id": 1378,
      "start": 8022.48,
      "end": 8028.16,
      "text": "thing has got to be some some mix of loops one two and three and it's it's a matter of the proportions"
    },
    {
      "id": 1379,
      "start": 8028.16,
      "end": 8035.12,
      "text": "right i i think that's got to be the the answer um when somebody eventually writes the equivalent of"
    },
    {
      "id": 1380,
      "start": 8035.12,
      "end": 8041.5199999999995,
      "text": "the making of the atomic bomb for this era what is the thing that will be hardest to glean from the"
    },
    {
      "id": 1381,
      "start": 8041.5199999999995,
      "end": 8048.16,
      "text": "historical record that they're most likely to miss i think a few things one is at every moment of this"
    },
    {
      "id": 1382,
      "start": 8048.16,
      "end": 8053.5199999999995,
      "text": "exponential the extent to which the world outside it didn't understand it this is this is a bias"
    },
    {
      "id": 1383,
      "start": 8053.5199999999995,
      "end": 8058.88,
      "text": "that's often present in history where anything that actually happened looks inevitable in retrospect"
    },
    {
      "id": 1384,
      "start": 8058.88,
      "end": 8065.76,
      "text": "and and so you know i i think when people when people look back it will be hard for them to put"
    },
    {
      "id": 1385,
      "start": 8065.76,
      "end": 8073.36,
      "text": "themselves in the place of people who were actually making a bet on this thing to happen that"
    },
    {
      "id": 1386,
      "start": 8073.36,
      "end": 8078.16,
      "text": "that wasn't inevitable that we had these arguments like the arguments that you know that i make for"
    },
    {
      "id": 1387,
      "start": 8078.16,
      "end": 8085.839999999999,
      "text": "scaling or that continual learning will be solved um uh uh you know that that you know some of us"
    },
    {
      "id": 1388,
      "start": 8085.839999999999,
      "end": 8091.44,
      "text": "internally in our heads put a high probability on this happening but but it's like there's there's a"
    },
    {
      "id": 1389,
      "start": 8091.44,
      "end": 8097.5199999999995,
      "text": "world outside us that's not that's not acting on that it's not kind of not acting on that at all um uh"
    },
    {
      "id": 1390,
      "start": 8097.52,
      "end": 8104.56,
      "text": "and and and i think i think the the weirdness of it um i i i think unfortunately like the insularity of"
    },
    {
      "id": 1391,
      "start": 8104.56,
      "end": 8110.88,
      "text": "it like you know if if we're one year or two years away from it happening like the average person on the"
    },
    {
      "id": 1392,
      "start": 8110.88,
      "end": 8115.040000000001,
      "text": "street has no idea and that's one of the things i'm trying to change like with the memos with talking"
    },
    {
      "id": 1393,
      "start": 8115.040000000001,
      "end": 8121.92,
      "text": "to policy makers but like i don't know i think i i think that's just a that's just like a crazy that's"
    },
    {
      "id": 1394,
      "start": 8121.92,
      "end": 8128.88,
      "text": "just like a crazy thing yeah um finally i would say and and this probably applies to almost all"
    },
    {
      "id": 1395,
      "start": 8128.88,
      "end": 8135.12,
      "text": "historical moments of crisis um how absolutely fast it was happening how everything was happening all"
    },
    {
      "id": 1396,
      "start": 8135.12,
      "end": 8141.52,
      "text": "at once and so decisions that you might think you know were kind of carefully calculated well actually"
    },
    {
      "id": 1397,
      "start": 8141.52,
      "end": 8145.84,
      "text": "you have to make that decision and then you have to make 30 other decisions on the on the same day"
    },
    {
      "id": 1398,
      "start": 8145.84,
      "end": 8149.4400000000005,
      "text": "because it's all happening so fast and and you don't even know which decisions are going to"
    },
    {
      "id": 1399,
      "start": 8149.44,
      "end": 8155.28,
      "text": "turn out to be consequential so you know one of my one of my i guess worries although it's also an"
    },
    {
      "id": 1400,
      "start": 8155.28,
      "end": 8161.44,
      "text": "insight into into you know into kind of what's happening is that you know some very critical"
    },
    {
      "id": 1401,
      "start": 8161.44,
      "end": 8166.4,
      "text": "decision will be will be some decision that you know someone just comes into my office and is like"
    },
    {
      "id": 1402,
      "start": 8166.4,
      "end": 8171.839999999999,
      "text": "dario you have two minutes like you know should we should we do you know should we do thing thing a"
    },
    {
      "id": 1403,
      "start": 8171.839999999999,
      "end": 8177.759999999999,
      "text": "or thing b on this like you know someone gives me this random you know half page half page memo and"
    },
    {
      "id": 1404,
      "start": 8177.76,
      "end": 8183.04,
      "text": "is like should we should we do a or b and i'm like i don't know i have to eat lunch let's do b and and"
    },
    {
      "id": 1405,
      "start": 8183.04,
      "end": 8189.04,
      "text": "you know that ends up being the most consequential thing ever so final question uh it seems like you"
    },
    {
      "id": 1406,
      "start": 8189.04,
      "end": 8196.0,
      "text": "have there's not tech ceos who are usually writing 50 page memos every few months and it seems like you"
    },
    {
      "id": 1407,
      "start": 8196.0,
      "end": 8203.12,
      "text": "have managed to build a rule for yourself and a company around you which is compatible with this more"
    },
    {
      "id": 1408,
      "start": 8203.92,
      "end": 8210.400000000001,
      "text": "intellectual type role as ceo and i want to understand how you construct that and how"
    },
    {
      "id": 1409,
      "start": 8210.400000000001,
      "end": 8215.04,
      "text": "like how does that work to be you just go away for a couple weeks and then you tell your company this"
    },
    {
      "id": 1410,
      "start": 8215.04,
      "end": 8218.960000000001,
      "text": "is the memo like here's what we're doing it's also reported you write a bunch of these internally yeah"
    },
    {
      "id": 1411,
      "start": 8218.960000000001,
      "end": 8223.84,
      "text": "so i mean for this particular one you know i wrote it over winter break um uh so there was the time you"
    },
    {
      "id": 1412,
      "start": 8223.84,
      "end": 8228.720000000001,
      "text": "know and i was having a hard time finding the time to actually find it to actually write it but i"
    },
    {
      "id": 1413,
      "start": 8228.72,
      "end": 8232.64,
      "text": "actually think about this in a broader way um i actually think it relates to the culture of the"
    },
    {
      "id": 1414,
      "start": 8232.64,
      "end": 8238.96,
      "text": "company so i probably spend a third maybe 40 of my time making sure the culture of anthropic is good"
    },
    {
      "id": 1415,
      "start": 8238.96,
      "end": 8244.88,
      "text": "as anthropic has gotten larger it's it's gotten harder to just you know get involved in like you"
    },
    {
      "id": 1416,
      "start": 8244.88,
      "end": 8249.439999999999,
      "text": "know directly involved in like the training of the models the launch of the models the building of the"
    },
    {
      "id": 1417,
      "start": 8249.439999999999,
      "end": 8254.24,
      "text": "products like it's 2500 people it's like you know there's just you know i have certain instincts but"
    },
    {
      "id": 1418,
      "start": 8254.24,
      "end": 8259.119999999999,
      "text": "like there's only you know i i it's very difficult to get in to get to get involved in every single"
    },
    {
      "id": 1419,
      "start": 8259.119999999999,
      "end": 8265.6,
      "text": "detail you know i like i i try as much as possible but one thing that's very leveraged is making sure"
    },
    {
      "id": 1420,
      "start": 8265.6,
      "end": 8270.8,
      "text": "anthropic is a good place to work people like working there everyone thinks of themselves as"
    },
    {
      "id": 1421,
      "start": 8270.8,
      "end": 8275.44,
      "text": "team members everyone works together instead of against each other and you know we've seen as some"
    },
    {
      "id": 1422,
      "start": 8275.44,
      "end": 8279.92,
      "text": "of the other ai companies have grown without naming any names you know we're starting to see"
    },
    {
      "id": 1423,
      "start": 8280.64,
      "end": 8284.08,
      "text": "decoherence and people fighting each other and you know i would argue there was even"
    },
    {
      "id": 1424,
      "start": 8284.08,
      "end": 8288.72,
      "text": "a lot of that from the beginning but but you know that it's it's gotten worse but i i think we've"
    },
    {
      "id": 1425,
      "start": 8288.72,
      "end": 8296.8,
      "text": "done an extraordinarily good job even if not perfect of holding the company together making everyone"
    },
    {
      "id": 1426,
      "start": 8296.8,
      "end": 8301.68,
      "text": "feel the mission that we're sincere about the mission and that you know everyone has faith that"
    },
    {
      "id": 1427,
      "start": 8301.68,
      "end": 8305.84,
      "text": "everyone else there is working for the right reason that we're a team that people aren't trying"
    },
    {
      "id": 1428,
      "start": 8305.84,
      "end": 8310.64,
      "text": "to get ahead at each other's expense or backstab each other which again i think happens a lot at some"
    },
    {
      "id": 1429,
      "start": 8310.64,
      "end": 8315.84,
      "text": "of the other places um and and how do you make that the case i mean it's a lot of things you know"
    },
    {
      "id": 1430,
      "start": 8315.84,
      "end": 8321.279999999999,
      "text": "it's me it's it's it's daniela who you know runs the company day to day it's the co-founders it's the"
    },
    {
      "id": 1431,
      "start": 8321.279999999999,
      "end": 8326.56,
      "text": "other people we hire it's the environment we try to create but i think an important thing in the culture"
    },
    {
      "id": 1432,
      "start": 8326.56,
      "end": 8332.96,
      "text": "is i some and just you know the the you know the other leaders as well but especially me"
    },
    {
      "id": 1433,
      "start": 8333.68,
      "end": 8340.24,
      "text": "have to articulate what the company is about why it's doing what it's doing what its strategy is"
    },
    {
      "id": 1434,
      "start": 8340.24,
      "end": 8347.439999999999,
      "text": "what its values are what its mission is and what it stands for and um you know when you get to 2500"
    },
    {
      "id": 1435,
      "start": 8347.439999999999,
      "end": 8352.64,
      "text": "people you can't do that person by person you have to write or you have to speak to the whole company this"
    },
    {
      "id": 1436,
      "start": 8352.64,
      "end": 8357.599999999999,
      "text": "is why i get up in front of the whole company every two weeks and speak for an hour it's actually i"
    },
    {
      "id": 1437,
      "start": 8357.599999999999,
      "end": 8363.439999999999,
      "text": "mean i wouldn't say i write essays internally i do two things one i write this thing called dvq"
    },
    {
      "id": 1438,
      "start": 8363.439999999999,
      "end": 8369.199999999999,
      "text": "dario vision quest um uh i wasn't the one who named it that that's the name it received and it's one of"
    },
    {
      "id": 1439,
      "start": 8369.199999999999,
      "end": 8373.199999999999,
      "text": "these names that i kind of i tried to fight it because it made it sound like i was like going off and"
    },
    {
      "id": 1440,
      "start": 8373.199999999999,
      "end": 8378.72,
      "text": "smoking peyote or something um uh but but the name just stuck um so i get up in front of the company"
    },
    {
      "id": 1441,
      "start": 8378.72,
      "end": 8385.039999999999,
      "text": "every two weeks i have like a three or four page document and i just kind of talk through like three"
    },
    {
      "id": 1442,
      "start": 8385.039999999999,
      "end": 8390.0,
      "text": "or four different topics about what's going on internally the you know the the models we're"
    },
    {
      "id": 1443,
      "start": 8390.0,
      "end": 8395.359999999999,
      "text": "producing the products the outside industry the world as a whole as it relates to ai and"
    },
    {
      "id": 1444,
      "start": 8395.359999999999,
      "end": 8401.439999999999,
      "text": "geopolitically in general you know just some mix of that and i just go through very very honestly i"
    },
    {
      "id": 1445,
      "start": 8401.439999999999,
      "end": 8405.76,
      "text": "just go through and i just i just say you know this this is what i'm thinking this is what"
    },
    {
      "id": 1446,
      "start": 8405.76,
      "end": 8411.76,
      "text": "anthropic leadership is thinking and then i answer questions and and that direct connection i think"
    },
    {
      "id": 1447,
      "start": 8411.76,
      "end": 8416.48,
      "text": "has a lot of value that is hard to achieve when you're passing things down the chain you know"
    },
    {
      "id": 1448,
      "start": 8416.48,
      "end": 8423.12,
      "text": "six six levels deep um uh and you know a large fraction of the company comes comes to attend either"
    },
    {
      "id": 1449,
      "start": 8423.12,
      "end": 8428.800000000001,
      "text": "either in person or um either in person or virtually and it you know it really means that you can"
    },
    {
      "id": 1450,
      "start": 8428.800000000001,
      "end": 8433.2,
      "text": "communicate a lot and then the other thing i do is i just you know i have a channel in slack where i"
    },
    {
      "id": 1451,
      "start": 8433.2,
      "end": 8439.12,
      "text": "just write a bunch of things and comment a lot um and often that's in response to you know just"
    },
    {
      "id": 1452,
      "start": 8439.12,
      "end": 8445.68,
      "text": "things i'm seeing at the company or questions people ask or like you know we do internal surveys"
    },
    {
      "id": 1453,
      "start": 8445.68,
      "end": 8450.16,
      "text": "and there are things people are concerned about and so i'll write them up and i'm like i'm you know"
    },
    {
      "id": 1454,
      "start": 8450.16,
      "end": 8455.76,
      "text": "i'm i'm i'm just i'm very honest about these things you know i just i just say them very directly"
    },
    {
      "id": 1455,
      "start": 8455.76,
      "end": 8461.28,
      "text": "and the point is to get a reputation of telling the company the truth about what's happening to call"
    },
    {
      "id": 1456,
      "start": 8461.28,
      "end": 8468.16,
      "text": "things what they are to acknowledge problems to avoid the sort of corpo speak the kind of defensive"
    },
    {
      "id": 1457,
      "start": 8468.16,
      "end": 8474.16,
      "text": "communication that often is necessary in public because you know the world is very large and full"
    },
    {
      "id": 1458,
      "start": 8474.16,
      "end": 8480.800000000001,
      "text": "of people who are you know interpreting things in bad faith um but you know if you have a company of"
    },
    {
      "id": 1459,
      "start": 8480.800000000001,
      "end": 8485.92,
      "text": "people who you trust and we try to hire people that we trust then then you know you can you can you"
    },
    {
      "id": 1460,
      "start": 8485.92,
      "end": 8491.92,
      "text": "can you know you can you can really just be entirely unfiltered um and uh you know i think i think that's"
    },
    {
      "id": 1461,
      "start": 8491.92,
      "end": 8496.88,
      "text": "an enormous strength of the company it makes it a better place to work it makes people more you know"
    },
    {
      "id": 1462,
      "start": 8496.88,
      "end": 8500.24,
      "text": "more of the sum of their parts and increases likelihood that we accomplish the mission because"
    },
    {
      "id": 1463,
      "start": 8500.24,
      "end": 8504.4,
      "text": "everyone is on the same page about the mission and everyone is debating and discussing how best to"
    },
    {
      "id": 1464,
      "start": 8504.4,
      "end": 8510.08,
      "text": "accomplish the mission well in lieu of an external dario vision quest we have this interview this"
    },
    {
      "id": 1465,
      "start": 8510.08,
      "end": 8514.8,
      "text": "this interview is a little like that uh this isn't fun dario thanks for doing it yeah thank you"
    },
    {
      "id": 1466,
      "start": 8514.8,
      "end": 8520.24,
      "text": "dhwarkash hey everybody i hope you enjoyed that episode if you did the most helpful thing you can do"
    },
    {
      "id": 1467,
      "start": 8520.24,
      "end": 8525.92,
      "text": "is just share it with other people who you think might enjoy it it's also helpful if you leave a rating"
    },
    {
      "id": 1468,
      "start": 8525.92,
      "end": 8531.36,
      "text": "or a comment on whatever platform you're listening on if you're interested in sponsoring the podcast you"
    },
    {
      "id": 1469,
      "start": 8531.36,
      "end": 8547.28,
      "text": "can reach out at dhwarkash.com advertise otherwise i'll see you on the next one"
    }
  ]
}