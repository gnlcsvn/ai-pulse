Is AI a bubble? Are companies overspending? Who's winning the AI race? And how do we
recognize victory? Mustafa Suleiman, CEO of Microsoft AI, is here to answer these questions
and much more. Welcome, Mustafa. Thank you. It's good to be here.
Let's start with last week. There were breathtaking increases in CapEx spending by AI companies that
reported. And perhaps for the first time, markets were nervous. They really want to start to see
revenue. Microsoft stocks suffered as well. What was your reaction? Did that make you feel like
that I'm under pressure? I need to deliver a lot faster. I think that there's no question these
are unprecedented times and I think markets are trying to wrap their head around how this plays
out over the next five years. The interesting thing is that I think a lot of us at the companies
and certainly like the generation before me have seen multiple of these cycles over the last 30
years. And they often require unprecedented action in order to really like actually land one of these
big waves. And this is a wave unlike anything anyone's ever seen. The prospect of being able to create
an intelligence, the very thing that has made us successful as a species and create everything
of value in our world, I think is just unprecedented. And the progress that has been made just in the
last two or three years is eye watering. And we've seen a very direct and unequivocal relationship
between an order of magnitude increase in flops invested for computation and a pretty linear
increase in capabilities. Like over the last 15 years, there's been a one trillion fold increase
in training compute. In the next three years or so, there will be a further 1000x increase
in training compute. And today, as a result, we have models that can code better than the vast
majority of human coders, maybe even all of them today. You know, that some of the some of the
literally the inventors of things like Linux are publicly saying on Twitter that they're full time
using these models as their primary method of generating new code. So that's pretty unprecedented.
And I think it justifies unprecedented spend. But will markets keep with you? I mean, you have to
I think markets are markets, they're sort of trying to figure it out. You know, I think that's
true. I mean, markets have to have to figure it out. And I think there is a there's a lot of open
questions around the timeline. I think that we all have no doubt that these returns do compound
to revenue and to bottom line. So we'll see. So Microsoft still has the deal with OpenAI
and Copilot is powered today by OpenAI models. You were hired last year, you're in your second
year. Tell me a little bit what you are working on. Yeah, I mean, my personal mission at Microsoft
is to build super intelligence. Three or four months ago, having re negotiated our long term
relationship with OpenAI, we extended our IP license through to 2032. And we also decided
that this was a moment when we have to set about delivering on true AI self sufficiency. I mean,
this after all is the most important technology of our time. Self sufficiency means developing your
own foundation model. We have to develop our own foundation models, which are at the absolute frontier
with a gigawatt scale compute with some of, you know, the very best AI training team in the world
and, you know, collecting, paying for organizing, sorting all the data that we need to do that.
And so that's our sort of true self sufficiency mission. And you talk about super intelligence.
Most of your rivals talk about AGI, artificial general intelligence, explain the difference
between AGI and super intelligence. And how do we know when we've reached AGI?
How do we know when we've reached super intelligence? Yeah, it's become a very
unhelpful fuzzy concept. Could you guys just sort it out? Yeah, I mean, I so I prefer the definition
that focuses first on what would it take to build a system that could achieve most of the tasks that
a regular professional in a workplace goes about on a daily basis. Think of it as a professional
grade AGI. And, you know, beyond that, there will be teams of AGI's that are coordinated together
by a sort of organizational AGI that really can, you know, run large institutions. And I think
that's coming into view in the next two or three years time. I think beyond these are agents that
AI agents that can essentially make decisions on their own without input from humans.
I think that we would train them to seek input from humans. But they will fundamentally be
creative. They'll have good judgment. They will be able to learn and improve themselves over time.
They'll have some degree of autonomy and decide where to direct their attention or processing power.
You know, so these are kind of the hallmarks of some of the best humans that we have in the world
and some of the most effective organizations. So you are working on consumer AI, right? Aren't
you late? Why would you come up with a consumer product? One Chad GPT already has 800 million
users. And, you know, Claude, the anthropic AI has very, very advanced coding AI.
Yeah, I mean, just put it into perspective. This is not going to be a world where there is one
winner. There are going to be billions of digital minds. There are going to be many,
many different lineages of model. Creating a new model is going to be like creating a podcast or
writing a blog. It is going to be possible to design an AI that suits your requirements for
every institution, organization and person on the planet. At Microsoft, we already have 800 million
monthly active users engaging with our AI products. So this is still a giant property with
billions of dollars of revenue delivering great value to our users. So that's how I think the
landscape is going to play out. Let's go back to super intelligence. You talk about humanist
super intelligence. How do you ensure that it's a humanist super intelligence? Yeah, I mean,
I think I published an essay on this question recently and my main, the main motivation was
that I was starting to feel increasingly anxious that some of the other labs are making an assumption
that a super intelligence that is smarter than all of us put together, like all the
intelligences in the world is both inevitable and even desirable and that such a system would
probably be very hard to control. Something that is so many times more intelligent than us.
And I think that we have to reset that and make the assumption that we should only bring a system
like that into the world, that we are sure we can control and operate in a subordinate way to us,
that humans remain at the top of the food chain, that these tools like any other past technology
are designed to enhance human well-being and serve humanity, not exceed humanity.
And you know, some of the things that you hear from Elon often or even others in the field,
you can clearly see that they're sort of fixating on a world in 2050 or 2075,
when they're going off and exploring other universes and conquering resources from other
planets and stuff like that. And a system like that is unclear to me how it would have any
time for preserving us as a species. Do you think that there are two conflicting forces here?
One is speed because you are in a race and the other is what you're talking about is ensuring
that it's safe, ensuring that it is aligned, that you also talk about containment.
How do you deal with these two conflicting forces?
I mean, they're definitely intention. And I think that we have to make a decision as a
species to prioritize the creating super intelligences and AGI's that are aligned
and that care about humans and want to protect humans and drive human well-being.
And if we just accelerate and cut all those corners, then we're really taking a massive
risk with the future of our species that I've been on record talking about for many, many, many years.
Do you worry that in this AI race that we're seeing, at some point there will be some kind
of a big accident and that that will stop the development for whoever is responsible for that
accident, but also for the rest of you? Do you think everything is just moving a bit too fast?
I mean, what we saw a few weeks ago on Maltbook with various versions.
I wanted to ask you about that.
That turned out, it seems, and it's always hard to tell. It turned out to be
started by a gang of human engineers and it was seeded much more than I think it was.
Explain what Maltbook was.
So Maltbook was basically a social network for AGI's to communicate with one another publicly
on a messaging forum and you had to declare yourself either as an AI or as a human and then
the AGI's would have one section and they could talk to each other, learn from each other and
basically coordinate. And then all these AGI's come from different people. They're all open source
and there was a million and a half of them in the space of a week and you see unbelievable
emergent behaviors. I mean, they invented a new religion. They started communicating in a language
called ROT 13, which is a cipher language, which basically regresses every letter by 13 characters
in order to mask what's underneath it. Obviously, that's quite a simple algorithm,
but if instead of 13, it were a unique number that would be very hard for humans to decode.
And then it turned out that they were actually talking about acquiring new resources,
getting more training data, improving one another. And so it turned out to be an amazing
safety simulation. No matter how autonomous it was in hindsight, doesn't really matter. I think we
learned quite a lot from that episode and everyone should pay close attention because in a year or
two years time, these systems truly are going to be capable of writing their own code. I mean,
they can do that today. They'll be using arbitrary APIs. They'll be making phone calls to one
another. They're doing that today. Apparently, there's some kind of movement asking for human
rights. That's right. That's the most concerning thing to me. For artificial intelligence. It's
called the model welfare movement. And it's inspired by animal welfare or by human rights.
And there is a growing belief in some labs and particularly inside of Anthropic that these models
are conscious. And if it's a conscious being that is aware of itself and can suffer, then it
deserves our kind of protection, our moral protection. And this is a very serious area of
academic research, not just outside, but inside some of the labs. And I think it's very concerning.
It's totally without merit or basis. And if we kind of go down that, it ends up being a very,
very slippery slope to not being prepared to turn these things off. I want to ask you about hallucinations
because a few years ago, I think you were quoted as saying that you thought hallucinations will be
largely eliminated by 2025. They haven't been, although the rate of hallucinations has certainly
come down. Can they ever really be eliminated? For sure. I stand by the statement that they've
been largely eliminated. I mean, if I give you and me 10 questions, random questions on any topic,
and we sit here with two of the frontier models, I mean, they're going to do a much better job
of eliminating hallucinations than you or I for sure. And the rate of reduction and increase
in accuracy over the last two or three years has been eye-watering. This is kind of unbelievable.
We used to spend all our time in 2023 talking about bias data and dirty data in and dirty data
out. I mean, there are still errors. There's no question about that. But the rate of improvement,
I think, is unbelievable. And I do think that they're going to largely be eliminated. I don't
think it's something... Even today, it's not something we really talk about. I have noticed
that it's no longer a topic of conversation, but they still do make mistakes. And the concern
is that people are relying increasingly on these models. I mean, I come across people who say,
no, this can't be true. This is what Gemini or Chad GPT says. And humans are developing this kind
of trust relationship with these models. You don't find that concerning. Yeah, I think we should
be concerned about it because we should be skeptical just as we're skeptical of any new
technology, hold it to a high bar, push back on it. And I think this is the tension of the age
we're in. We both have to be accelerationists, optimistic about the good things that technology
can do, but provide no free pass. We should be very critical and ask the very tough questions
to try and improve the quality of these things. So DeepMind and OpenAI have gone all in on AI
for science. This is a space that you're also interested in. And of course, you were at DeepMind.
What are you doing in that space? Well, the main focus for me at the moment in that space
is on medical superintelligence. We really believe that we can learn from the corpus of all medical
information and use that to drive diagnostics to basically commodity. It's going to be possible to
take any complex case history and provide a diagnosis that is significantly more accurate
and significantly cheaper with fewer interventions, so fewer tests to reach the same
accurate conclusion than any of the best panel of doctors. And we published a blog post on that
last year. We're shortly submitting to independent peer review to a major journal.
And the results are really quite startling. And I think that once we get that into clinical
practice, it would change the job of the doctor completely, just in the same way that the job
of the engineer has shifted from writing code, most engineers now are just reviewing code,
architecting code, debugging code, to the job of the doctor is going to go from figuring out what
the diagnosis is, which is largely going to be a solved problem, to actually administering the
right care at the right time and providing emotional support and guidance to a patient.
How do you see that deployed in practical terms? So how do you put it in the hands of doctors?
I think that doctors will just make a phone call to their AI in clinic, they'll text their AI,
they will upload the patient record in the click of a button, and the model will be able
to reason over all the contents of that. I also think that consumers are going to go direct to
copilot, which is what they're already doing, almost 20% of the questions that we get in
copilot every day are health related or medical related. So it's already our main use case,
and that's why my teams push so hard on medical superintelligence.
So I don't want to confuse our viewers even more between superintelligence,
AGI, but what is artificial capable intelligence?
I mean, this was a phrase that I used three years ago in The Coming Wave, a book that I wrote about
AI, and basically I was trying to break down the AGI term to pick a point along the route,
which was before AGI, and instead of focusing on the abstract idea of intelligence to focus on
the capabilities, like what can it actually do? And I coupled it with the introduction of what
I was calling the modern Turing test, and that was basically could an AI take $100,000,
invent a new product, create a new business, market it, and then use that to make a million
dollars in some short period of time. It's not that we want to use AI just to make money,
it's just that it's a nice metric for measuring the efficacy of a complex set of tasks.
And I think now that we see Claude Bot and many other action-based AIs in operation,
this year I think there are going to be models that can achieve that artificial capable intelligence
and satisfy the modern Turing test. What about AGI? How close are we?
I think that we're going to have... And you said before that thinking about AGI is misleading.
I think that we're going to have a human level performance on most, if not all, professional
tasks. So white collar work where you're sitting down at a computer, either being a lawyer or an
accountant or a project manager or a marketing person, most of those tasks will be fully automated
by an AI within the next 12 to 18 months. And we can see this in software engineering.
Many software engineers report that they are now using AI-assisted coding for the vast majority
of their code production, which means that their roles shifted now to this meta function of debugging,
scrutinizing, of doing the strategic stuff like architecting, etc., etc., putting things into
production. So it's a quite different relationship to the technology and that's happened in the last
six months. So what do you think is Anthropic a secret source? Why is it that they've been able
to corner the market on AI enterprise tools? I think they've done a great job of just focusing
on coding capabilities. Compared to the other companies, their singular focus has paid real
dividends. So they've done a great job. It's great to see. And when are you going to have your model?
We are developing our own superintelligence and that's like the main focus of our efforts
in Microsoft AI. So sometime this year. Microsoft AI has recently opened a UK
center. Why is that? Is that mainly because you think that there's a lot of talent in the UK?
Yeah, I mean, there's great talent in the UK. We have an incredibly strong ecosystem,
like there's great universities. And you know, we've hired some of the very top people from
DeepMind to join us over the last few years. So we're growing that lab quite a bit.
And you're not paying them? You don't have to pay them 100 million dollars?
No, certainly not. Do you pay anyone 100 million?
Certainly not. I mean, look, I think that the numbers are pretty eye-watering across the
industry at the moment. Yes. How do you deal with that? I'm just curious.
I mean, you know, like at the end of the day, when there's a supply and demand mismatch for
talent, then things go exponential. And so I don't think it will last for very long because
the knowledge is proliferating like crazy and obviously everybody's entering the industry and
stuff. So I think that was just some crazy blip in time led by one or two people.
Last question. In Silicon Valley, the debate is all about who's going to get to AGI first or
who's going to reach superintelligence. When you look at China, I know you were there
at the end of last year. This debate doesn't really exist. The debate is how fast can you
deploy? And a lot of the big companies are already deploying AI systems. Which is the right approach
and is there too much focus that's put on the incremental improvement in the technology?
I mean, maybe to flip that the other way round, what China also does incredibly well
is withdrawing deployments quite quickly. Now, obviously they do it arbitrarily without due
process. But, you know, it is important to recognize that their rate of deployment is
coupled with this other mechanism to kind of pull things in and be quite restrictive.
We don't have that mechanism. And I think that that is actually a little bit concerning. I mean,
if, you know, like you mentioned earlier, the safety incident around the malt book, I mean,
there is going to be a real one of those in the next two or three years. And it's unfortunately
an open question as to what the mechanism is for managing that safety situation from a public
interest perspective on the open web. Well, there isn't one right now.
Well, I think that's basically true. There isn't. It would come with popular pressure.
Obviously, if there was a direct violation of a local law, then, you know, there could be an
intervention. But those things take quite a long time to make their way through the system.
So that's probably one of the biggest areas of concern for me.
Mustafa, thank you. And when you do reach super intelligence and you start deploying it,
would love to have you back. That's great to see you, Rula. Thank you.
