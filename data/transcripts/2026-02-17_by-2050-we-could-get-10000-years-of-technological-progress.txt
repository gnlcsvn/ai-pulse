If you look at public communications from at least OpenAI, Anthropic, and Google DeepMind, in all of their stated safety plans, you see this element of as AIs get better and better, they're going to incorporate the AIs themselves into their safety plans more and more. How to create a setup where we use control techniques and alignment techniques and interpretability to the point where we feel good about relying on their outputs is a crucial step to figure out. Because it either bottlenecks our progress, because we're checking on everything all the time and slowing things down, or it doesn't bottleneck our progress, but we hand the AIs the power to take over. Today, I'm speaking with Ajayi Khotra. Ajayi is a senior advisor at OpenPhilanthropy, where in 2024, she led their technical AI safety grantmaking. More generally, she's been doing AI-related research and strategy since 2018, and has become very influential in AI circles for her work on timelines, capability evaluations, and threat modeling. Thanks so much for coming back on the show, Ajayi. Thank you so much for having me. So, doing this interview gave me a chance to go back and listen to the interview that we recorded, I guess, two and a half years ago. And I have to say, you were very on the ball. There was a lot of issues that came up in that conversation that you were bringing to people's attention that I think in the subsequent two and a half years seem like a much, much bigger deal now. You talked about meters evaluating autonomous capabilities, a line of research that's gone on to become super influential, very widely read, I think, in policy circles. You talked about using probes to monitor and shut down dangerous conversations, something that's pretty standard practice, and maybe one of the potentially most useful outputs from mechanistic interpretability. You talked about the importance of using chain of thought and scratch pads to monitor what AIs are doing and why. It's still probably the dominant technique. You talked about the growing situational awareness of AI models and the resulting possibility of deceptive alignment, something that's now a completely mainstream topic. You talked about how when you train models to not engage in bad behavior, they don't necessarily just learn to become honest. They also learn to just hide their misbehavior better, something that I guess research has kind of borne out, does really happen, and is a big concern. You talked about how you expected models to get schemier as they get smarter, especially once we inserted reinforcement learning back into the mix, something that's definitely happened. And you talked a bunch about sycophancy, how you thought models might end up just flattering people rather than giving accurate information because that's kind of something that we enjoy. So I feel like, I mean, you didn't come up with all these ideas or anything like that, but I think you were ahead of the curve, and maybe we'll get some ahead of the curve ideas in this interview as well. Hopefully. Thank you. So you think that a key driver of disagreements about kind of everything to do with AI is people's different views on how likely AGI is to speed up science and technology and, I guess, physical infrastructure and manufacturing. Why is that? Yeah. So I think a thing that I've been noticing as the concept of AGI has become more and more mainstream is that it's also become more and more watered down. So, like, last year I was on a panel about the future of AI at DealBook in New York, and it was me and one or two other folks who kind of think about things from a safety perspective and then a number of, like, sort of venture capitalists and, like, technologists. And the moderator asked at the very beginning of the panel how likely it was, like, whether we thought it was more likely than not that by 2030 we would get AGI, defined as AIs that can do everything humans can do. And, like, seven or eight hands went up, not including mine, because my timelines are somewhat longer than that. But then he asked a follow-up question a couple questions later about whether you thought, whether we thought that AI would create more jobs or destroy more jobs over the following 10 years. So 2030 was five years, and seven out of 10 people thought that we would have AGI by 2030, but then it turned out that eight out of 10 people, not including me, thought that AI would create more jobs than it destroyed over the next 10 years. And I was a little confused. I was like, why is it that you think we will have AI that can do absolutely everything that the best human experts can do in five years, but will actually end up creating more jobs than it destroys in the following 10 years? Like, what's happening? It seems like there's a certain tension in this view. And when I poked some people later in the panel about that seeming tension, I think they, like, really quickly backed off and they said, you know, oh, like, what does AGI really mean? The moderator had defined it as this very extreme thing. But they were like, you know, we kind of already have AGI. People keep moving the goalposts. We keep making, like, cool new products. And, like, people aren't accepting that it's AGI. And they, like, aspire to something higher. And I thought that was funny because, like, you know, the old school, singulitarian, futurist definition of AGI is this very extreme thing. But I think, like, VCs have an instinct to sort of call something AGI that is, like, GPT-5 is AGI or, like, something just much milder. And so I think this creates a situation where people feel like they've gotten a lot of evidence that AGI doesn't, isn't a very big deal and, like, doesn't change much. Because, like, we already have AGI or we're going to have it next year or we got it, like, two years ago. And, like, you know, look around us, nothing much is changing. But I feel like, and so I think there's this expectation where, like, whether or not we get AGI in the next few years, people, a lot of people, like, are starting to not really, like, care about that question. They still expect the next 25 years or the next 50 years to play out kind of like the last 25 years or the last 50 years where, you know, there's a lot of technological change between 2000 and 2025. But it's like a moderate amount of change. And they kind of expect that in 2050 there will be a similar amount of change as there was between 2000 and 2025. Even if they think that we're going to get AGI in 2030. They think AGI is just, like, what's going to drive that sort of continued mild improvement. Whereas I think that there's a pretty good chance that by 2050, you know, the world will look as different from today as today does from, like, the hunter-gatherer era. You know, like, it's like 10,000 years of progress rather than 25 years of progress driven by AI automating all intellectual activity. Yeah. I guess you've hinted at the fact that there is an enormously wide range of views on this. But can you give us a sense of just how large the spectrum is and what the picture looks like on either end? Yeah. Yeah. So I would say on the sort of standard mainstream view, if you ask, like, a normal person on the street, like, what 2050 will look like, or if you ask, like, a standard mainstream economist, I think they would think, well, the population is a little bit bigger. We have somewhat better technologies. Maybe, like, they have a few pet technologies that they're, like, most interested in. Like, then maybe we have this one or that one. Slightly better medicine. People live slightly longer. And, yeah, it's an amount of change that's, like, extremely manageable. I think on the far extreme from there, on the other side, is, like, a view described in If Anyone Builds It, Everyone Dies. Where in that worldview, you have, at some point, probably pretty unpredictably, we sort of crack the code to extreme superintelligence. Like, we invent a technology that rather suddenly goes from being, like, you know, GPT-5 and GPT-6 and so on to being so much smarter than us that we're, like, you know, cats or, like, mice or ants compared to this thing's intelligence. And then that thing can, like, really immediately have, like, really extreme impacts on the physical world. Like, the classical sort of canonical example here being inventing nanotechnology. So, like, the ability to, like, precisely manufacture things that are, like, really, really tiny and can replicate themselves really, really quickly and can do, like, all sorts of things, you know, and can, like, move, you know, inventing, like, space probes close to the speed of light and things like that. I think there's a whole spectrum in between where, like, people think that we are going to get to a world where we have technologies approaching their physical limits. We have, like, spaceships approaching the speed of light. And we have sort of self-replicating entities that replicate as quickly as bacteria while, like, also doing useful things for us. But we're going to have to go through, like, intermediate stages before getting there. But I think, like, something that unites all of the people who are sort of AI futurists and, like, concerned about AI X-risk is that they think in the coming decades we're likely to get this level of, like, extreme technological progress driven by AI. How strong is the correlation between how much someone expects AI or AGI to speed up science research in particular and, I guess, like, physical industry as well and how likely they think it is to go poorly or how nervous they are about the whole prospect? I think it's a very strong correlation. Like, I've found often that people who, like, reasonable people who are AI accelerationists tend to think that the default course of how AI is developed and deployed in the world is very, very, very slow and gradual. And they think that we should, like, cut some red tape to make it go at a little bit more of a reasonable pace. And people who are worried about X-risk think that the default course of AI is this extremely explosive thing where it, like, you know, overturns society on all dimensions at once in, you know, maybe a year or maybe five years or maybe six months or maybe a week. And they're saying, oh, we should slow it down to take 10 years maybe. And meanwhile, you know, the sort of accelerationists think that by default, diffusing and capturing the benefits of AI will take, like, 50 years or 100 years. And they want to speed it up to take 35 years, you know. It's quite interesting. I guess, yeah, people who radically differ in their policy prescriptions might be targeting, aiming for the same level of speed, actually. Maybe they want this period to take 10 years or 20 years. That's what both of them want. But they just think it's going to, their baseline is so different. So they're pushing in completely opposite directions. What's your kind of modal expectation? What do you think is the most likely impact for it to have? I think that probably in the early 2030s, we are going to see what Ryan Greenblatt calls top human expert dominating AI, which is an AI system that can do tasks that you can do remotely from a computer better than any human expert. So it's better at remote virology tasks than the best virologists, better at remote, you know, software engineering tasks than the best software engineers, and so on for all the different domains. And by that time, I feel like probably the world has already accelerated and changed, and sort of narrower and weaker AI systems have already, like, penetrated in a bunch of places, and, like, we're looking at a pretty different world. But at that point, I think things can go much, much faster. Because I think at top human expert dominating AIs in the cognitive domain could probably use human physical labor to build, like, robotic physical actuators for themselves. That would be, like, one of the things that, whether the AIs are sort of, have already taken over and are acting on their own, or whether, like, humans are still in control of the AIs, I think that would be a goal they would have of, like, automating the physical as well. And I think I have pretty wide uncertainty on, like, exactly how hard that'll be. But whenever I check in on the field of robotics, I actually feel like robotics is, like, progressing pretty quickly. And it's taking off for the same reasons that sort of cognitive AI is taking off. It's, like, large models, lots of data, imitation, large scale, is helping robotics a lot. So I imagine that, like, you can pretty quickly, maybe within a year, maybe within a couple years, get to the point where these, like, superhuman AIs are controlling a bunch of physical actuators that allow them to sort of close the loop of, like, making more of themselves, like, doing all the work required to, like, run the factories that, like, print out the chips that then run the AIs, and, like, doing all the repair work on that, and, like, gathering the raw materials on that. So you're saying, you're expecting, in the 2030s, it won't just be that these AI models are capable of automating, you know, computer-based R&D, but they'll also be able to lead on the project of building fabricators that produce the chips that they run on. And so that's, like, another kind of positive feedback loop. Yeah. So I really recommend the post Three Types of Intelligence Explosion, that's by Tom Davidson on Forethought, where he makes the point that, like, you know, we talk a lot about the sort of promise and the danger of AIs automating AI R&D, and, like, you know, automating the process of making better AIs. But that's only one feedback loop that is required to fully close the loop of making more AIs, because we're talking about software that makes the, like, you know, transformer architecture slightly more efficient, or, like, gathers better data to train the AIs on. But AIs are also running on chips, which are printed in, like, these chip, like, you know, factories at NVIDIA, and those factories have machines that are built by other machines that are built by other machines, and, you know, ultimately go down to raw materials. And I think that something we don't talk about very much, because it'll happen afterward, is how hard it would be for the AIs to automate that entire stack, the full stack, and not just the software stack. So, I guess, the range of expectations that exist among sensible, thoughtful people who've engaged with this on how much, like, at peak, how much is AGI going to speed up economic growth? It ranges from people who say it will speed up economic growth by 0.3 percentage points, so it'll be, you know, a 15% increase or something on current rates of economic growth, and, you know, and I'd be very happy if it was that good. Yeah. People who say, at peak, the economy will be growing at 1,000% a year, or higher than that, thousands of percent a year. So, it's like 100 or 1,000 or a 10,000-fold disagreement, basically, on the likely impact that this is going to have. It's an almost, like, unfathomable degree of disagreement among people who, it's not as if they've thought about this independently and they haven't had a chance to talk. They've spoken about this, they've shared their reasons, and they don't change their mind, and they disagree by a thousand-fold impact. Yeah. I guess, and you've made it part of your, I guess, mission in life the last couple of years to have, like, really sincere, intellectually engaged, curious conversations with people across the full spectrum. Why do you think it is that this disagreement is able to be maintained? Yeah. I feel like, at the end of the day, the different parties tend to lean on two different, like, pretty sort of simple priors or simple, like, outside views that are kind of different outside views. And I would say that the party, the, like, sort of group that expects things to be a lot slower tends to lean on, well, for the last 100, 150 years in frontier economies, we've seen 2% growth. And, like, think of the technological change that has occurred over the last 100 or 150 years. You know, we, like, went from having very little, like, electricity was just an idea to, like, everywhere was electrified. You know, we had the washing machine and the, like, the television, the radio, like, all these things happened. Computers happened in this period of time. None of these show up as, like, an uptick in economic growth. And I think there's this stylized fact that mainstream economists really like to cite, which is that new technology is sort of the engine that sustains 2% growth. And in the absence of that new technology, growth would have slowed. And so they're kind of like, this is how new technologies always are. People think that they're going to lead to a productivity boom, but you never see them in the statistics. You didn't see the radio, you didn't see the television, you didn't see the computer, you didn't see the internet. And you're not going to see AI. AI might be really cool. It might be the next thing that lets us keep chugging along, you know. And that's, like, one perspective. It's an outside view they keep returning to. And also just maybe a, like, somewhat more generalized thing, which is, like, things are just always hard and slow. You know, just, like, way harder and slower than you think. You know, it's like, what was it, like, Murphy's, not Murphy's Law. Murphy's Law, because anything that can go wrong will go wrong. Yeah, anything that can go wrong will go wrong. I think this is our experience in our personal lives, that it's awfully hard to achieve things at work. Things that to other people might seem so straightforward. And they're like, why haven't you finished this yet? And you're like, well, I could give you a very long list. Or, like, Hofstadter's Law. It always takes longer than you think, even when you take Hofstadter's Law into account. Or, like, the programmer's credo. This is my favorite one. It's like, we do these things not because they are easy, but because we thought they would be easy. Like, so there's just this whole cloud of, like, you know, it's naivete to think that things can go crazy fast. If you write down a story that seems, like, perfect and unassailable for how things will be, like, super easy and fast, there's all sorts of bottlenecks and all sorts of drag factors you inevitably fail to account for in that story. It's like, that's kind of, like, that perspective. And then I think the alternative perspective leans a lot on, like, much longer-term economic history. So if you attempt to try and, like, assign reasonable, like, GDP measures to the last 10,000 years of human history, you see acceleration. So the growth rate was not always 2% per year at the frontier. 2% per year is actually blisteringly fast compared to what it was in, like, 3,000 B.C., which is, like, maybe that was, like, 0.1% per year. So the growth rate has already multiplied, like, many-folds, maybe in order of magnitude, maybe two. I think that, like, people in the slower camp tend to feel like the exercise of doing, like, long-run historical data is just, like, too fraught to rely upon. But people in both camps do agree that the Industrial Revolution happened, and the Industrial Revolution accelerated growth rates a lot. And we went from having growth rates that were well below 1% to having 2% a year growth rates. And I think that, like, yeah, people in the, like, faster camp tend to, like, lean on the long run and on models that say that the reason that we had accelerating growth in the long run was a feedback loop where more people can, like, try out more ideas and, like, discover more innovations, which then leads to food production being more efficient, which then leads to a larger supportable population. And then you can rinse and repeat, and you get, like, super exponential population growth. And then that, like, that perspective says that AIs, if you can slot in AIs to replace not just the cognitive, but the cognitive and the physical, the entire package, and close the full loop of AIs doing everything needed to make more AIs, or AIs and robots doing everything needed to make more AIs and robots, then there's no reason to think that, like, 2% is some sort of, like, physical law of the universe. They can grow as fast as, you know, their physical constraints allow them to grow, which are not necessarily the same as the constraints that keep human-driven growth at 2%. So that's the justification that they provide for their perspective in broad strokes. But why is it that even after communicating this at great length to one another, they don't kind of converge on uncertainty or saying it'll be something in the middle because there's competing factors, that they just, like, continue to be reasonably confident about quite different, I guess, narratives about how things will go? Yeah. I'm honestly not sure. I think maybe one part of it is that, so I guess, like, you know, I'm partial to the things will be crazier side of things, so I'm not sure I'll be able to, like, give a perfectly balanced account. But I feel like one thing I've noticed on the side, in terms of people who think it'll be slower, is that their worldview kind of has a built-in error theory of people who think things will go faster. So the, like, worldview is not just things will keep ticking along, but everyone thinks there will always be, like, some big new revolution that makes things go. Everyone's always expected to speed up and they've been wrong almost every time. And they've always been wrong. So there's that dynamic, which is, like, it's a, like, you know, from their point of view, I think it's totally reasonable. It's, like, kind of, like, even if there isn't some super knockdown argument in the terms of your interlocutor, where you can, like, point to a mistake that they'll accept, or even if you kind of look at the story and think it's kind of plausible, you still have this strong prior that, like, this kind of thing being, someone could have made the kind of the same argument about television, someone could have made the same argument about computers. None of these played out. So I think that's a big factor. I also think there hasn't been, like, these, you know, these are complicated ideas. There hasn't been that much dialogue. And I think there could be more. And I think there could be more dialogue that is trying to ground things in, like, near-term observations also. But, yeah, I think that's a big part of it. I think they have, like, an error theory built in that makes it so that, like, the object-level conversation about, like, okay, like, you know, here's how the AI could make the robots, and here's how the robots could bootstrap into more robots and so on. Like, that whole way of thinking doesn't feel very, like, legitimate or interesting. Or, like, they sort of have a story where, like, that type of thinking always leads to a bias towards expecting things to go faster than they actually will, because it's, like, hard for that kind of thinking to account for all the drag factors and all the bottlenecks. Whereas I think on the other side, people who think things will go faster feel like everyone is always kind of, like, blanket assuming that there are going to be bottlenecks. And then they bring up specific bottlenecks, and those specific bottlenecks, when you look into them, don't seem, you know, like, they might slow things down from some sort of absolute peak of 1,000% growth, but they're not reasons to think that 2% is where the ceiling is, or even that 10% is where the ceiling is. So they also have this kind of error theory of, like, the bottleneck subjection. So it's incredibly decision-relevant to figure out who is right here. I think, like, almost all of the parties to this conversation, if they completely changed their view, and the people who thought it was going to be 1,000% decided it was going to be 0.3%, they would probably change what they're working on, or they would think it was, like, a decisive consideration, probably, against everything that they were doing previously, and vice versa. If people came to think that there would be 1,000% speed up, then they'd probably be a whole lot more nervous and interested in different kinds of projects. So how can we potentially get more of a heads up ahead of time about which way things are going to go? I guess it seems like sharing theoretical arguments hasn't been persuasive to people. Yeah. Is there any kind of empirics that we could collect as early as possible? So one thing that I think will not address all of this, but is a step in the right direction, is really characterizing how and why and if AI is speeding up software and AI R&D. So Meter came out with an Uplift RCT, which I think was the first of its kind, or at least the largest and highest quality, where they had software developers split into two groups. One group was allowed to use AI. The other group was disallowed from using AI. And they studied, you know, how quickly those developers solved issues like tasks on their to-do list. And it actually turned out that in this case, AI slowed down their performance, which I thought was interesting. I don't expect that to remain true. But I'm glad we're starting to collect this data now. And I'm glad we're starting to sort of cross-check between benchmark-style evaluations, where AIs are given a bunch of tasks and sort of scored in an automated way, and evidence we can get about actual, like, in-context, real-world speedups. So I really want to get a lot more evidence about that of all kinds, like big Uplift RCTs. It would be great if companies were into internally conducting RCTs on their own, like, rollouts of internal products to see, like, you know, our teams that get the latest AI product earlier more productive than teams that don't. Even self-report, which I think has a lot of limitations, is still something we should be gathering. So, yeah, I guess my, like, high-level formula would be look at the places where adoption has penetrated the most and start to measure speed up in, like, actual output variables. Like, I think it would be really cool if there was a solar panel manufacturing plant that had, like, really adopted AI and we, like, started to see, like, you know, how much more quickly they could manufacture solar panels or, like, how much better they could make solar panels. Yeah, is it possible to do this at the chip manufacturing level? I guess maybe that's the most difficult manufacturing that there is, more or less. So we might think that AI, you get more of an early heads up if you do something that's more straightforward, like solar panels, but would really like to be monitoring across all kinds of different manufacturing. How much difference is any of this making? I think the most important thing, or the thing I ultimately care about, is the AI stack. So chip design, chip manufacturing, manufacturing the equipment that manufactures chips, and then, of course, the software piece of it, too. The software piece is the earliest piece, but I think we should be monitoring degree of AI adoption, self-reported AI acceleration, RCTs, anything we can get our hands on for the entire stack. Because I think the, like, moment when the sort of AI futurists think things are likely to be going much, much faster sort of coincides with when AI has, like, fully automated the process of making more AI. So that's really something to watch out for. And then I think, like, but on a separate track, you also want to just be looking at the earliest power users, no matter where they are, just because you can get insight that transfers to these domains. Is there anything else we can do? I don't know. I'm really, like, curious about this. Do I understand right that last year, you put out a request for proposals? You were at OpenPhil looking to fund people who had ideas for how would we resolve this question? Yeah. So I put out a pair of requests for proposals in late 2023. One of them was on building difficult, realistic benchmarks for AI agents. So at the time, very few people were working with AI agents and only, like, a couple of agentic benchmarks had come out, including meters benchmark that I discussed on the show last time. And so I was really excited about it. It felt like it was a moment to move on from, like, giving LLMs multiple choice tests to giving them, like, real tasks, like, book me a flight or, like, you know, make this piece of software work, like, write tests, run the tests, iterate until the thing actually works. And that was, like, a very new idea at the time, but also the time was sort of right for that idea. And there were a lot of academic researchers who were excited about moving into the space. So we got a lot of applications for that arm of our request for proposals, and we funded a bunch of cool benchmarks, including, like, SciBench, which is a cyber offense benchmark that's used in a lot of, like, standard evaluations now. But then we also had this other arm, which was basically, like, types of evidence other than benchmarks, like surveys, RCTs, all the things we talked about. We got much less interest for that. And I think it just reflects that it's harder to think of, like, good ways to measure things outside of benchmarks, even though everyone agrees benchmarks have major weaknesses and consistently overestimate real-world performance because benchmarks are sort of, like, clean and contained, and the real world is messy and open-ended. But one thing that I'm excited about that came out of the second RFP is that Forecasting Research Institute is running this panel called LEAP, which is the Longitudinal Experts on AI panel, where they just take, like, 100 or 200 AI experts, economists, and superforecasters and have them answer a bunch of granular questions about where AI is going to be in the next six months, in the next year, in the next five years. Both, like, benchmark scores, but also things like, you know, will companies report that they're, like, slowing down hiring because of AI? Or, like, will an AI be able to, like, plan an event in the real world? Or, like, these kinds of things. So I'm very excited about that. And I think, honestly, like, having people make subjective predictions, explain how those predictions are connected to their, like, longer-run worldviews, and then, like, check over time who's right might be the, like, most flexible tool we have. So I'm very excited to see where LEAP goes. But I think it is, like, it is challenging to get indicators that are clearly early warnings so that we can actually, like, do something about it if the people who are more concerned are right, but that are also, like, clearly valid and, like, not easy to dismiss on the other side as just, like, not realistic enough to matter. So as part of this, you've been thinking about, I guess one way that this could really go wrong is if the companies that are developing cutting-edge AI may know, they may begin to see themselves internally how much it's helping them and that perhaps it's speeding them up enormously. But they may not decide not to share that information with the rest of the world. Yeah, and they may decide not to release those products. Like, if there's one company that's well ahead of the others, then, like, in AI 2027, it was sort of depicted that the company that was ahead in the AI race was so far ahead of its competitors that it could afford to just keep its best stuff internal and, like, only release sort of less good products to the rest of the world. It could afford it in the sense of it didn't need to make money by selling the product? Like, its competitors were far enough behind that they couldn't, like, undercut it or compete with it by, like, releasing a better product. Like, in the story, the, like, the company in the lead, Open Mind, is basically just, like, releasing products that are slightly better than the, like, state-of-the-art of its competitors. They're saying they're so far ahead that they can just choose to always, basically, have their product be somewhat better. They could just release whatever level of their own internal machine would be the best to the external world. Yeah. Okay. But I guess it would be, yeah, it would be unfortunate if there are people who do know this, but the broader world doesn't get a heads up. And so, you know, we could have known six months or a year earlier in what direction things were going, but that was kept secret. I mean, I guess maybe for the leading AI company, they'd prefer to keep it secret. But for the rest of it, I suppose we'd probably prefer that the government has some idea what's going on. Yeah. So you've been thinking about what sort of transparency requirements could be put in place that would require people, require the companies to release information that would give the rest of us clues as to where things are going. What sort of transparency requirements could those be? Yeah. So I think there's a whole spectrum of evidence about AI capabilities where on the one hand, the sort of easiest to test but the least informative is benchmark results. And companies do release benchmark results when they release models right now. So they say, you know, Claude Opus 4 was released and they have a model card that says, like, you know, it has this score on this, like, hacking benchmark. It has this score on the software engineering benchmark and so on as part of a report about whether it's dangerous. Or GPT-5 had the same thing. I think that that's great that they do that. But in my ideal world, they would release their highest internal benchmark score at some calendar time cadence. So every three months, they would say, we've achieved this level score on this hacking benchmark, this level score on software engineering benchmark, this score on an autonomy benchmark. And that's because, as you said, danger could manifest from purely internal deployment. Because if they have an AI agent that's sufficiently good at AI R&D, they could use that to go much faster internally. And then other capabilities and therefore other risks might come online much faster than, like, people were previously expecting. So it's not ideal to have your, like, report card for the model come out when you release it to the public. Unless there's some sort of guarantee that you're not sitting on a product that's, like, sufficient, like, that's substantially more powerful than the public product. So maybe it's fine to, like, release it, release your, like, model card and system card along with the product if you also separately have a guarantee that you won't have too much of a gap between the internal and the external. So that's, like, on the, like, end of things that are currently discussed. It's, like, kind of how I would tweak information that's currently reported to be, like, somewhat more helpful for this concern. But then there's a bunch of other stuff that is not currently reported that I think ideally it would be really great to know. Stuff like how much and how are they using AI systems internally? So one thing I'm very interested in is so companies will sometimes report kind of to brag about, like, the percentage of lines of code that are written by their AI systems. Various CEOs have said, like, internally 90% of our lines of code are written by AIs and things like that. I think it would be great to have systematic reporting of those kinds of metrics, but those metrics aren't the, like, ideal metric I'd be interested in. So, like, one thing I'm interested in is, like, what fraction of pull requests to your internal code base were mostly written by AI and mostly reviewed by AI. So AI is, like, humans are, like, not involved for the most part in, like, both sides of this equation. And I'd be very interested in watching that number climb up because I think it's an indication both of AI capabilities and of, like, how much deference they're giving to AIs. And eventually, if things are going to go crazy fast, the AIs have to be doing most things, including most, like, management and approval and review because if humans have to do that stuff, then things can only go so fast, you know? So I really want to track how much, like, higher-level decision-making authority is being given to the AIs in practice inside the companies. Yeah, I think there are probably a bunch of other things that we could, like, send basically as a survey. Like, how much do you use AIs for this type of thing, for that type of thing? Like, how much speed-up do you get from, you know, subjectively do you think you get? If you're running any internal RCTs, I would, of course, love to know the results of that. What about just requirements that inasmuch as they're training future generations of AI models, they have to reveal to at least, like, some people in the government, like, how they're performing on, like, normal evals of capability so they can kind of see the line going up even if they're not releasing it as products for whatever reason. And if the line starts, you know, if the benchmarks start curving upwards, like, far above previous expectations, then that could lead them to sound the alarm. Yeah, I think that is a good thing to do. But I'm sort of, I sort of don't think that just benchmarks alone will actually lead anyone to sound the alarm because we just, like, the thing with benchmarks is that they saturate. Yeah, but they always have that S-curve shape. They always have the S-curve shape. And the benchmarks we have right now are harder than the previous generation of benchmarks. But it's still far from the case that, like, I feel confident that if your AI gets 100% score on all these benchmarks, then it's, like, a threat to the world and it could take over the world. I still think the benchmarks we have right now are, like, well below that. So what's probably going to happen is that these benchmarks are going to get saturated, then there's going to be a next generation of benchmarks people make. And then those benchmarks are going to tick up and then get saturated. So I think we need some kind of real-world measure before we can start sounding the alarm. And then the ultimate real-world measure is actually just observed productivity, right? Like, if they are seeing internally that they're discovering insights, like, faster than they were before, then that's a very, like, late but also very clear signal. And that's the point at which they should definitely sound the alarm and, like, we should sort of know what's happening. So, yeah. Yeah. How has this idea been received by the companies? I mean, on the one hand, it seems like transparency requirements is the regulatory instrument that the companies have objected to the least. It's the one that they've been most willing to tolerate. On the other hand, the whole message of this is we don't trust you to share information with the rest of the world and we think that you might screw us over, basically, by rushing ahead and, like, deliberately concealing that. I could imagine that that could be a little bit offensive to them, or at least if that is their plan, then they probably want to find some excuse for not having this kind of oversight. Yeah, I think that the response just tends to differ based on the, like, actual information that's being asked for. So, benchmark scores, they already release. Like I said, they release it at the point of releasing a product, which I think is fine for now, but I would like to move it to a regime where they release benchmark scores at some sort of fixed cadence, even if they don't have a product release. benchmark scores are not considered, like, sensitive information, but this other stuff that I think is a lot more informative on the margin is much more fraught, right? They don't necessarily want to share with the world, like, you know, the rate at which they're gaining, like, algorithmic insights, because you want to maintain some mystery about that for competitive reasons. Like, it's risky for you if it's a little bit too fast, because then, like, I don't know, competitors will start paying more attention to you and, like, trying to copy you and trying to find out what's going on. It's also risky for you if it's, like, too slow, because then that's kind of embarrassing. Investors lose heart. Yeah, investors lose heart. And another thing I didn't mention earlier is that I would really like them to be reporting their most concerning misalignment-related safety incidents. So, like, has it ever been the case that in real-life use within the company the model lied about something important and covered up the logs? Like, I really want to know that. But then, of course, it's clear that reporting that is very embarrassing to companies. So one, like, thing that might help here is that there are a number of companies now, so perhaps they could report their dis- you know, they could report their individual data data to some sort of third-party aggregator that then reports out, like, an anonymized, like, overall industry aggregate score. But I don't think that solves all the issues because there are few enough of them that, like, people would be able to guess. So I think there's a lot of, like, competitive challenges and IP sensitivity challenges and, like, just PR challenges to overcome here with some of the more, like, penetrating internal information. But I think it's, like, important enough to the public interest that we should try and find a way to, like, navigate that. Yeah. So it's not unusual for government agencies to be able to basically demand commercially sensitive information from companies for regulatory or governance purposes. I actually worked at one when I was in the Australian government. I was at the Productivity Commission which had, like, extraordinary subpoena powers to basically demand almost any documents from any company in the country. Like, I rarely use power, but, and it wasn't the only agency that had that capability. And what kinds of things would you ask them? I mean, well, I never actually saw this power being used. It was a kind of, I guess people were proud of the fact that we had that authority. Yeah. But I think you would usually do it for, like, competition reasons, trying to tell whether companies are colluding potentially or whether there's, like, an insufficient degree of, like, market competition and there would be a reason to intervene. And I would imagine almost certainly that there's government agencies in the U.S. that have a similar remit. Yeah. And so if they actually could keep that kind of information secret, then maybe the companies would be more happy to share it with people who are specialized, basically, in reading this, like, comprehending this data and figuring out what to do with it. Yeah. I think that could be a solution, but I'm a little skeptical. So I think that releasing this information publicly is probably a lot better than releasing it just to a government body, basically because, you know, we're, like, building the plane of, like, AI safety research, like, as we're flying it. And it's not like there's a box-checking exercise that any kind of government agency that's, like, often understaffed, especially with, like, technical staff, could do. It's more like we want this information out there in the open, and then we want people to do, like, some involved analyses of it. And, like, our sense of what information we even want is probably going to be, like, shifting over time, and it'll probably go better if there's, like, a robust kind of external scientific conversation about, like, what indicators we want to see and what that would mean and, like, when we should trigger alarm. And if that's all being routed through governments with, like, 10 people or, like, even 50 people who have to deal with it, I think it's, like, it would be very hard for them to interpret the evidence, like, quickly enough and well enough and be confident enough to sound the alarm and then have people actually listen to them. Like, if I imagine sounding the alarm on something like the intelligence explosion, I kind of picture it having to be, like, a society-wide conversation, kind of, like, sounding the alarm about COVID or, like, something I have in my mind is, like, when Joe Biden had that disastrous debate performance that led to, like, weeks of conversation that ultimately led to him being removed from the ticket. It would have been very hard, I think, for a small, narrow group of people sort of entrusted with the authority to, like, make the same thing happen. Because I guess you want common knowledge and you want lots of attention focused on the issue as well as just some technocrats being aware. As well as the opportunity for a bunch of technical experts who may not be paying that much attention now because, like, maybe they think this stuff is all science fiction to jump in at that moment and, like, offer their their takes. And I think it would be very powerful if someone like Arvind Narayanan who's, like, known for being very skeptical of these stories actually looked at the data and changed his mind and said, oh, yeah, like, this is happening now and it's dangerous. And it's very hard to get those kinds of common knowledge dynamics if everything is, like, just sent to governments. That said, of course, like, I think sending things to governments is better than, like, not sending it anywhere. So, like, I also think that's good. So, in as much as the plan A would be we want them to be sharing this information such that anyone in the public can find out, I guess they'll probably resist this to any legislation imposing this to some extent. And I guess for partially legitimate reasons that it is probably going to be frustrating for them, how high on the list of, like, in as much as people are trying to set priorities for what sort of asks do you make and which sort of fights do you pick, would this be, like, very high on the list for you? I think I laid out a whole spectrum of ideal kind of, like, information sharing practices and I don't think going all or nothing on that whole package is, like, a top priority fight to pick but I think sort of the algorithm of thinking really hard about what pieces of information we would want to know in order to know for ourselves if the intelligence explosion was happening and sort of getting the, like, highest value items on that list or the, like, biggest bang for buck items on that list to me feels very high. And I think that's the, like, strategy that people working on AI safety-related legislation have landed on. So, like, the RAISE Act in New York and SB53 in California are both, like, quite transparency-oriented and both oriented around, for example, like, whistleblower protections which are, which are, like, an important sort of policy plank underlying transparency. Do you think that information about an emerging intelligence explosion might just leak out to the public anyway because staff at the companies would feel uncomfortable with that proceeding in secret? I think that's very plausible. I still think that information that leaks in the form of, like, rumors in San Francisco, like, tech bro parties doesn't have the ability to impact policy and, like, decision-making all the way in D.C. or London or Brussels in the same way as information that is just sort of clearly unrefuted and very salient and sort of official. So, I mean, I think that the AI safety scene in the Bay Area has benefited from having, like, close social ties to people who work at AI companies, getting a sense of, like, what might be coming around the corner. But that's not something you can just, that's not something that you can use to really, like, pull an alarm or, like, advocate for very costly actions. So, I think it, like, isn't really enough. We need more. So, let's imagine that, via whatever mechanism, society does get a heads up, that we are starting to see the early stages of an intelligence explosion. What would we do with that heads up? Yeah. So, I think one just extremely important factor is at that point in time, how good are AI systems at everything besides AI R&D? So, the alarm has sounded and AI, we learned that AI has, like, fully or almost fully automated R&D at the leading AI lab, perhaps all the AI labs. This is causing those labs to go way faster than they were going with, like, mostly human-driven progress in the previous era. So, at that point in time, whatever AI progress you thought was going to be made by default in the next 10 years or the next 20 years or the next 30 years might be made in a year or two or even six months based on how much, depending on how much AI is speeding everything up. So, you know, at this stage, AIs might not be that dangerous, but we might be about to move very quickly through the point in time where they're not so dangerous to the point in time where they have, you know, sort of godlike abilities. And I think that what we want to do as a society, if we gain confidence that we're sort of at the starting point of this intelligence explosion is to redirect as much of that AI labor as we can from further AI R&D to things that could help protect us from future generations of AIs, both in terms of AI takeover risk and also in terms of a wide range of other problems that might be created for society by increasingly powerful AI. AI. And at that point, it's not, it's still not in the sort of narrow selfish interests of whichever company is in the lead to do that because if they were to slow down unilaterally, then like someone behind them could catch up. But hopefully if we have, if the alarm has sounded and we have like a clear picture of, you know, we have six months or 12 months or 18 months until radical superintelligence, then this might be like a window of opportunity to coordinate to get, to like use AIs for protective activities instead of further AI capability acceleration. So the challenge we have is AI is becoming much smarter very quickly and we feel very nervous about that. But I guess and the opportunity that's created is that, well, we have AIs like a lot more labor and we have like much smarter potential researchers than we did before. So why don't we turn that new resource towards solving this problem that I guess at the moment we don't really know how to fix. It's a little bit, I guess, I think some people who are not too worried about AI, they look at society as a whole or they look at history and they say, well, technology has enabled us to do all kinds of more destructive things but we don't particularly feel like we're in a more precarious situation now or at much greater personal risk now than in 1900 or in 1800 because advances in destructive technology have been offset by advances in safety increasing technology and on balance probably things have gotten safer and so the idea is, well, can we potentially, it's like going to be a vertiginous time but perhaps we could pull off the same trick in this crunch time period. Yeah, and I think that like a lot of people who are more concerned about AI risk are very like dismissive of this plan. It's just like, it sort of sounds like a crazy plan. It's like really flying by the seat of your pants like expecting the thing that's creating the problem to solve the problem but in a sense like I do think humanity has repeatedly used sort of general purpose technologies that both like created problems to solve those problems like, you know, automobiles, something as mundane as that. Like, you know, cars created the opportunity for there to be carjackings and for there to be drive-by shootings and for like, you know, it empowered bad actors in various ways. But of course, like, you know, if the police and law enforcement have cars as well, like that is, that is a balance. Like it's not like you're, you know, when you imagine a future with some crazy new advanced technology and you imagine all the problems it creates, it can be hard to, like with the same level of detail and fidelity, imagine all the responses to those problems that are also enabled by that technology. And so, you know, you could imagine someone worrying about the rise of like fast vehicles and like neglecting to think about how the fast vehicles would have like, you know, all the ways, all the ways that like they cause bad things could be sort of like kept in check by people using vehicles for law enforcement and similar. And similarly with computers, you know, you can, you can hack things with computers, computers, but computers also enable you to do a lot of automated monitoring for that kind of hack and like automated vulnerability discovery. Yeah, different kinds of law enforcement. Like you couldn't, you couldn't imagine a police force not using computers. So I do think the basic principle is sound that like if you're worried about problems created by technology, one of the first things on your mind should be how can you use whatever that new technology is to solve those problems. And, but, but, you know, I, I think that this is an especially narrow window to get this right. And you're not imagining cars creating like broad-based rapid acceleration of, of all sorts of new technologies and potentially like just a 12-month window or two-year window or six-year window before everything goes totally crazy. So I do think that it's important to not blow through that window to like monitor as we're approaching it and to monitor how long we have. But, but yeah, I think I'm fundamentally fairly optimistic about trying to use early transformative AI systems, like early systems that automate a lot of things to automate the process of controlling and aligning and managing risks from the next generation of systems who then like automate the process of managing those risks from the generation after and so on. Yeah, it's interesting that you say that this approach has often, often been dismissed because I feel, I feel it's very in vogue now. I hear about this proposal every, every couple of days as someone, someone presents it or I read something about it in one guise or another. Yeah. Um, I guess one reason why years in the past it might have felt unpopular is people were mostly focused on the issue of misaligned AI. They were concerned about an AI that has it in for you and would like to take over if it had the opportunity. And that's maybe the worst application of this out of all of them because there you're, you're asking the AI to align itself but you don't know whether it's assisting you or trying to undermine you. And so, I mean, you could try to make that work. People have suggested proposals where you could try to get useful, honest work out of an AI that is not, that doesn't want to help you. Um, but it's a lot easier to see how you potentially solve problems other than alignment. Like if you assume, well, the alignment part, we feel like we've got a good handle on. But there's a huge list of other problems that are being created during the intelligence explosion. Like the fact that AI now, if people get access to it, could invent other kinds of destructive technologies that we don't yet have good countermeasures for. In that case, it's just clear how well the AI could just help you figure out what the countermeasures ought to be. So I don't think that I agree with this. So I do think misalignment, the prospect that these early AIs, these early transformative AIs are misaligned is a huge obstacle to this plan that needs to be shored up and handled and specifically addressed. And I don't think that it necessarily bites harder for getting the AIs to do alignment research than for getting the AIs to do anything else helpful. Because if they have it out for you, they don't necessarily want to like help you shore up your civilization's defenses. So if you're imagining trying to get a hardened, misaligned AI to help you with biodefense, if it's misaligned and it, you know, for example, wants the option of like threatening you with a bioweapon in its arsenal in the future, it would similarly have an incentive to do a bad job at that as it would to do a bad job at alignment research. So in general, I think there's this, there's one big concern, which is will the AIs that we're trying to use at that point in time have motivations that give them incentives to undermine the work we're trying to get them to do? And I think they certainly would have incentives to undermine alignment research if they were misaligned, but I think they would also have incentives to undermine like efforts to make ourselves more rational and thoughtful like AI for epistemics. Because if we're more rational and thoughtful, then maybe we'll realize they're probably misaligned and that would be bad for them. They would also have incentive to undermine our like defact style like defensive efforts because that would make it harder for them to take over. That makes sense. I think the distinction I was drawing is for people who thought that the alignment problem was extremely hard to solve and we were like way off track to solving it, the idea of getting the AI to solve the problem is kind of self-contradictory because like, well, I wouldn't believe, I wouldn't trust the AI at all. Anything that it proposed I would assume was sabotaging us. If you're on the side of thinking, well, the alignment problem is actually the easier part of things, I think that that's a relatively straightforward technical problem that we are on track to solve, but there's this like laundry list of 10 other issues. It's like then very obvious like, but we'll have the brilliant AGI, so why don't we just use that to solve all the other things? And also, I'm inclined to trust it and believe it. Yeah. So I do think that if you are not worried about alignment at this early stage, everything becomes easier. It becomes an even more attractive, you know, strategy and path. But I think the canonical using AI for AI safety or using AI for defense plan does imagine that we're not sure at the beginning that they're aligned. We may not be like highly confident that they're like extremely misaligned and like fully power seeking and like looking to take over at every opportunity, but we're not imagining that we know with confidence we can trust them. So figuring out how to create a setup where we use control techniques and alignment techniques and interpretability and like whatever other tool at our disposal to get to the point where we feel good about relying on their outputs is like a crucial step to figure out because it either like bottlenecks our progress because we're checking on everything all the time and slowing things down or it doesn't bottleneck our progress, but we like hand the AIs the power to take over. So which kind of specific problems arising from the intelligence explosion are you envisaging wanting to get the AGI to help us out with? Yeah, so one obvious one is just AI alignment. How can we ensure that either these AIs that we're using to help us right now or future generations of AIs that they help us create and future generations that those AIs help us to create, how can we ensure that that whole chain is motivated to help humans and is honest and is like basically doing what we say and steerable? And that is sort of the foundation of everything else. But then there are also other things that are not really about AIs at all that are just about broad societal defenses. So if we think that the advent of extremely powerful AI will like create a flood of new like cyber vulnerabilities that are quickly discovered in like a bunch of critical systems like weapons systems and the power grid and so on, can we preemptively use those same AIs that are good at finding those vulnerabilities to find and patch them before bad actors can use the AIs to find them? Another thing is biodefense. So you had my colleague, Andrew, on your podcast recently that talked about his ambitious plan to, you know, rapidly scale up like detection of novel pathogens, rapidly scale up medical countermeasures when they're detected and rapidly scale up the manufacturing of like PPE and clean rooms and things like that. If we have AI systems that are good at like, you know, that kind of research problem and also maybe we have at that point robots, so a lot of that manufacturing itself can be automated and can go a lot faster than if humans had to do that stuff, that would be like a big boon to biodefense. And then there's some somewhat more speculative things along the lines of like, you can think of this as a kind of defense, like you can think of it as like a psychological defense maybe, but there's stuff around can we use AIs to make our collective decision making a lot smarter, a lot wiser, a lot better? Can we make it so that we're better at finding truth together? Can we make it so that we're better at coming to like compromise policy solutions that leave like lots of people happy? How do you ensure that advances in AI doesn't lead to a war between the US and China, that kind of thing? Or even that too, but even more mundanely, stuff like over the last 10, 15 years, social media has led to like a degradation of political discourse. Could AI tools help you just kind of like find the policy from among the vast space of possible policies policies that like a large number of people actually like and can like credibly put trust in and so on. So I interviewed Will McCaskill and Tom Davidson from Forethought earlier in the year and they have, the organization has a long list of what they call grand challenges, which they suspect all of them are probably amenable to this kind of AGI labor during crunch time. I think other ones are like ensuring that society doesn't end up locked into particular values kind of prematurely and like cuts off our ability for further reflection and changing our mind. The potential use of AI or AGI in as much as it's very steerable and follows instructions to be used in kind of power grabs by the people who are operating it. I guess the space governance, this question of if we actually do start to be able to use resources in space, how would we share them? How would we divide them such that, in particular, such that there's not conflict ahead of time because people anticipate that once you start grabbing resources in space, you're on track to become overwhelmingly dominant. Yeah, there's epistemic disruption, which you mentioned. I guess new competitive pressures kind of concerns that you can end up in a sort of Malthusian situation if you have competition between many different AIs and possibly some others that are missing here. But there's, I guess, many other, like, I guess we don't know which of these are going to loom large at the time. Some of them might feel like they've kind of been addressed or perhaps that we were hallucinating issues that aren't so severe. But yeah, there's many different ways that we could potentially apply it. Yeah, I agree. I think all of those problems that Tom and Will highlighted seem like real problems to me. I think maybe my approach would be to, from our current vantage point, lump a lot of that under AI for helping us think better and helping us, like, find solutions that we're mutually happy with. So it's like AI for coordination, compromise, negotiation, truth-seeking, that cluster of things. Because I think, like, you know, something like the question of space governance, like, how do we divide up the resources of space if, like, there are some existing factions that have an existing distribution of power? No one really wants the sort of destruction that comes from everybody racing as hard as possible to get there first. But there's, like, a complicated space of, like, negotiated options beyond that. And I think AIs could potentially help a lot with that sort of thing. So you said in your notes that you think this approach is basically what all of the frontier AI companies say. this is their safety plan, more or less. Is that right? Yeah, I would think so. I think if you look at public communications from at least OpenAI, Anthropic, and Google DeepMind, this sort of jumps out more or less in these different cases. But in all of their, like, stated safety plans, you see this element of as AIs get better and better, they're going to incorporate the AIs themselves into their safety plans more and more. And I think some are more explicit than others about expecting some sort of, like, specific crunch time that occurs when AI is, like, rapidly accelerating AI R&D. But everybody is picturing AIs playing a heavy role in the safety of future AIs. Yeah. What assumptions are necessary for this approach to make sense? Or what kinds of setups could actually just make it a bad plan? Yeah. I think fundamentally, you need it to be the case that there exists a window of opportunity where, like, before AIs are uncontrollably powerful or have created, like, unacceptable levels of risk, where they are, like, really capable and, like, really change the game for, like, AI safety research. And that there's some meaningful window of time where you can notice as you're approaching it. And even by default, without, like, crazy slowdown, it lasts at least six months or lasts a year. If you think instead that once your AI sort of hits upon some generality threshold, it, like, within a matter of days or weeks becomes crazy super intelligent, this plan doesn't work. Because, like, you know, you wouldn't even notice probably before it's too late. So, and then I think there can also be unlucky orderings of capabilities where this plan wouldn't work. Where you could have AIs that are, like, really specifically good at AI R&D and they're really not good at anything else. Not even AI safety research that's very similar to AI R&D. They're just, like, extremely good at AI R&D. Maybe the only thing they're good at is making it so that future generations of AIs have better sample efficiency and can learn new things more efficiently. Then you could have a period of six months or a year where you know this is happening and you have these AIs. But you're still sort of hurtling towards a highly general super intelligence without being able to use these AIs for anything else necessarily because they're just not good at anything else. There's something that's a bit self-contradictory about that because an AI that can, it's, like, extremely smart but all it can do is improve the sample efficiency of the next model is in a sense, like, not very troubling but in itself because it doesn't have, like, general capabilities. That kind of model isn't going to be able to take over or invent other technologies. It's only at the point that it has the broader capabilities, the broader agency that it actually is able to make problems. But I guess you're saying you could have a long lead up where that's all that it can do and then at the last stage. Yeah, and then at the last stage it might be going, it might go back to the first scenario I talked about where it's like, oh, the narrow AIs that are just, like, savants at AI R&D hit upon an algorithm in almost like a blind search, like, almost like if you imagine AlphaFold, like, it is brilliant at, like, figuring out how proteins fold but isn't, yeah, it isn't, like, broadly aware. Like, you could imagine such AIs or, like, an algorithmic search process hitting upon an architecture or, like, a training strategy that then can go foom really quickly. And so in this lead up you're like, yep, AI is accelerating AI R&D. It's crunch time. We have six months left. We have three months left. But, like, these AIs are not the AIs that you can use for anything useful. Yeah, I guess many of the problems that would like it to help with issues are, like, social issues, political issues, philosophical issues in some cases. What do you think is the chances that AI, I mean, the companies, I think, they're working harder to make them good at coding and to make them good at AI research than any other particular thing. And I guess those are more concrete, measurable problems than solving philosophical questions. So it seems like it is really a live risk that, unfortunately, the balance of capabilities will end up being pretty disadvantageous for this plan. Yeah, I think that the further afield you go from work that looks like doing ML research and doing software engineering, the greater a penalty they'll probably be. The AIs, like, currently are much better at, you know, helping my friends who do ML research all day than me where I do, you know, weird thinking and, like, go on these kinds of podcasts and, like, write emails to people making, like, grant decisions and stuff like that. It's much worse at that stuff. You can see already that it's got, like, a very specialized skill profile. Fortunately, I do think that at least AI safety, there's a big chunk of AI safety research that does look very similar to ML research. And I do think, like, you know, my friends who are getting, like, big speed-ups from AI are safety researchers and they're doing the kinds of work, control, alignment, et cetera, that I think will be, like, some of the most important things you want these AIs to be helping with at the very beginning. But, yeah, stuff like AI for epistemics, AI for moral philosophy, you know, AI for negotiation, AI for policy design, all that stuff just may not be that good, doesn't necessarily have to be good by default, and that's, like, a big concern of the plan. I guess another worry would be that the AI models end up being able to cause trouble before they end up being capable enough to figure out solutions. Like, I guess, like, I know a classic case there would be, imagine that we put a lot of effort into, I guess it would be a bit stupid to do this, but we put a lot of effort into training an AI model that's extremely good at developing new viruses or new bacteria, basically changing diseases to make them worse. I mean, there are people who are using AI to develop new viruses. I guess they're using it to develop medical treatments, but that sort of stuff can then be repurposed for other things. But if that sort of highly specialized model arrives first before you end up with a model that has a sufficient understanding of all the society and biology and medicine to figure out what the good countermeasures are, then we need a different approach than this one. Yeah, and in general, I think of, like, AIs doing defensive labor as a prediction about the world that you want to, like, try and be thinking about as you make your plans, it's not a guarantee. And in many cases, the answer will be to specialize now in doing the kinds of things that might be hardest for the AIs to do then. And I think stuff like building a bunch of physical infrastructure to, like, stockpile a bunch of PPEs and vaccines and things like that is a prime candidate for something that just inherently takes a long lead time and that the AIs might not be that advantaged at at the point that they're good at doing the, like, scary things that it's meant to protect against. Yeah, that was going to be another concern of mine that, in as much as the AIs are very helpful, you might imagine that they're very helpful at the, like, idea generation or the strategizing stage, but they might still be quite bad at, like, actually running a business or actually figuring out how to do all of the manufacturing. So if they come up with, they could come up with a great strategy for countervailing new bioweapons where they're like, here's the widget that you should use. Go and make 10 billion of them are like, can you help us with that? It's like, no, I'm not very good at that. Good luck. Yeah, I think that in general, you should expect AIs to be much better at things that there are tighter feedback loops on, where you can recognize success after a short period of time. And that's why they're, like, that's one of the reasons why they're really, really good at coding, because you can just, like, train them on this, like, very hard-to-fake signal of, like, did the code run after you, like, did whatever you did with it. And in general, I think, like, idea generation versus actually executing on, like, a one-year plan has some of this element of, like, you can read a white paper and be like, huh, yeah, that's pretty good. And, like, you can push the thumbs-up button and, like, generate an AI that's, like, pretty good at generating white papers that you think are, like, you know, neat and, like, probably would work. But it's, like, much harder to train the AI to, like, run the team of, like, thousands of humans and robots that are, like, actually executing on the plan. Why is the crunch time aspect or, you know, the intelligence explosion taking off actually even relevant to when we would want to start doing this? Because you might just think if AI can help us do research or do work to solve any of these problems, then as soon as it's able to do that, we want to do it. Like, whether or not an intelligence explosion is kicking off or not. To some extent, that's right. I think the reason that I focus so much on the intelligence explosion is twofold. One is because at that point, I think we might have a pretty short clock to figure out a bunch of stuff. And, you know, the default trajectory might look like 12 months to extremely powerful, uncontrollable superintelligence that can easily take over the world. So it kind of changes our calculus of, like, you might, you want to, like, focus on, like, very short-term things rather than things that have long lead times, at least at crunch time, if not before. The other thing is, I think crunch time can help alleviate some of the challenges we've been talking about with AIs not being good at the full spectrum of things we want them to be good at. Because, sort of, by definition, at that point, AIs are really good at further AI R&D. And one of the things we could do with AIs that are good at AI R&D, at least in most cases, is to try and direct their AI R&D towards, like, filling out the skill profile of AIs and getting them to be good at some of the types of things that we want them to be good at that they aren't so good at right now. And so, at that point, you might have, like, just much more capability at your disposal. And it might be, like, much more worth putting in the effort to try and, like, fine-tune and scaffold and do all these other things to make your AI that's good at moral philosophy or your AI that's good at biodefence. So you're thinking about this strategy not just as a description, I guess, what other organizations potentially should work on or as a description of what the AI companies are already planning to do. But also, I guess, because you think maybe this should influence what open philanthropy plans to do over the coming years and potentially that, like, open philanthropy's best play might be to have billions of dollars waiting at this, at this relevant crunch time and then disperse them incredibly quickly buying a whole lot of compute to get AIs to solve these problems. Yeah, I mean, just like how right now, you know, 80% plus of our grant money goes to salaries to pay humans to think about stuff and do research and do policy analysis and advocacy and all these other things, you know, so too, in a few years, it might be the case that AIs are better than most of our human grantees and our money should mostly be going to buying API credits or renting GPU time to get the AIs to do, like, a similar distribution of activities. So an alternative approach to this would be that at the point that we get a heads up that we think an intelligence explosion is beginning to take place, we do everything we can to pause at that stage, to slow down, basically to arrest that process so that rather than having to, like, rush to, in three or six months, get the AIs to fix all of these issues, we buy ourselves a bunch more time. Why not adopt that as the primary approach instead? Yeah, so I think that the plan I described is compatible with pausing at an intelligence explosion, like, right at the brink of an intelligence explosion. In fact, I would hope that we do that because I think by default having 12 months to get everything in order is just not enough time. But I think of it as doing two things. One is making the pause less binary. So if you think of the default path as almost 100% of AI labor goes into further rounds of making AIs better and making more AIs and making more chips and so on, and you think of a pause or a stop as 0% of AI labor is going in, of the world's AI labor is going in towards those activities, I think there's a whole spectrum between 0% and 100%. And then I think of it as doing another thing, which is, it's sort of answering the question of what you do in the pause, which is like, you do all this protective stuff and you have these AIs around to do it with. And you might think, like, like once you have that frame of like making the pause less binary and thinking really hard about what you do during a pause, I think you might often end up thinking, oh, it's worth going a little bit further with AI capabilities because, you know, especially if we tilt the capabilities in a certain direction, we might at the end of that get AIs that are much better than they are right now at biodefense while still not being uncontrollable, still not being that scary. And you can imagine a bunch of little pauses and little redirections and so on during that whole period. And I would hope that like at some point in the period we do activities like policy coordination and so on that cause us to have longer in this sweet spot of AIs that are powerful enough to help with a lot of stuff but not so powerful. They're like, you know, we've already lost the game. So, yeah, we should probably clarify that although you think this is among our best bets, in an ideal world, do you think that we would go substantially slower through all of this? Because, you know, as good a plan as this might be, we'll really be white-knuckling it and not be confident that it's necessarily going to work. Yeah. So I think that, you know, if a really clear early warning sign triggers that we are about to enter into this intelligence explosion, fast takeoff space where we go in the space of 12 months from, you know, AI R&D automation to vastly superhuman AI, then I would vote for at that time shifting that trajectory to be 10 times longer or even longer than that and trying to make that transition as a society in 10 years instead of one year or 20 years instead of one year, I still wouldn't, and this is maybe a bit of a quibble, I still wouldn't advocate for pausing and then, like, hanging out for 10 years and then unpausing because I actually think that, like, slowly inching our way up is better than, like, pause then unpause and then having a jump. But, yeah, I would, like, going back to what we said about, like, how your default expectations of trajectories influence what you think should happen. I think the default is going through this in, like, one year and I would certainly rather it be 10 or 15 or 20 years. But I think that this, like, the frame of using AIs to solve our problems applies regardless of whether you're, sort of, white-knuckling it in one year or, like, maybe eking out an extra two months or if you manage to get the, like, consensus and the common knowledge that allows the world to step through it in 10 years. Yeah, I guess, in as much as we're slowing down to do something, this is a big part of the thing that we're slowing down to do. Yeah. So this is a big part of the company's plan for technical alignment. If this doesn't work out, yeah, why do you think it's most likely to have failed for them? I think that it's probably, if it fails, it's probably most likely to fail because they just didn't actually do a big redirection from using AIs for further AI capabilities to putting a lot of energy towards using them for AI safety because, you know, they say this is their plan but they don't really have any quantitative claims about, like, at that stage what fraction of their AI labor or their human labor for that matter is going to go towards the safety versus the further acceleration. And they'll be facing tremendous pressure at that point from their competitors to stay ahead. And so my guess is that unless they have like just much more robust commitments than they have right now, they probably just won't be directing that much of their AI labor. And so if they have a hundred thousand really smart human equivalents, maybe only like a hundred of them are working on AI safety, which is maybe still like more than they had before in human labor, but not that much compared to like how quickly things are going. It's just saying unless they have really strong commitments, but I guess other mechanisms would be that it's legally required. At this point, the government basically insists that most of the compute go towards this, or at least like most of it is not going towards a request of self-improvement. Or I guess if the companies could reach some sort of agreement where they're saying, well, we would all like to spend more of our compute on this kind of thing. So we're going to have some, I guess, contract where we're going to spend like 50% of all of our compute and then we like don't lose relative position in particular. Yeah, I mean, I think that particular contract is probably going to run into big antitrust issues. Might be a little illegal, but maybe we could carve out an exception to antitrust with this one. I guess a different mechanism, in as much as the government is taking a massive interest, they could help to try to coordinate this one way or another. I think it's a bit tough. This is not the kind of thing it's like super easy to make laws about because it's really not a box checking exercise. Like what do you actually, when you like write the legislation that like half the compute must be spent on safety rather than capabilities, like what do you count as safety research? And like how are you enforcing this? Like do you have like auditors in there being like, what are you working on? What are you working on to like all the team leads in the companies and like, you know, checking off that they have that it's 50% safety? I could imagine stuff like that. I think it would require like extremely technically deep regulators that like we just don't really have right now, I think. I thought that you might say that the most likely reason for this to fail was that it just turned out that alignment is incredibly hard. You get egregious misalignment even at like relatively low levels of intelligence and we don't really figure out how to fix that early enough to get useful work out of them. Yeah, I think that's a possibility. I don't think it's the most likely way it fails. On my views, I think the most likely way it fails is that they don't go super hard on it. But I think it's also plausible that they're just trying to get the AIs to help with alignment and the AIs are just like misaligned and the control procedures and other things are like ineffective and so they just deliberately only help with further AI R&D and don't help with alignment and safety and biodefense and like all these other things you'd want them to help with. I would hope that at that stage the transparency regime is strong enough that that fact is broadcast really widely and then that could inspire like a change in policy that causes us to slow down. But then in that world it's a bad world even if we do slow down a lot because we're just on our own. We have to like do this stuff without the AI's help because we can't get them to help us. But I'm actually like reasonably bullish about control techniques getting early AIs that are not super galaxy brain super intelligences to be helpful for a range of stuff that they're good at. I guess another way that they could end up actually just not making that much of an effort is if the window is relatively brief and it just takes a long time to get projects off the ground and they haven't really planned this ahead so they end up debating it back and forth and then by the time they've figured out that they actually do want to do this. I mean I suppose it's like nominally in these various papers but I wonder whether they actually are thinking ahead about how this would feel and whether they'll be have the decision making capability to decide to redirect enormous resources towards this other effort. Yeah. I do think anything that requires a large corporation to be super discontinuous in something it's doing is like facing big headwinds as a plan so I would hope that they're sort of smoothly increasing the amount of internal inference compute that is going towards safety as the AIs get better and better so that the jump doesn't have to be huge at that final stage and that is something that if we could elicit like honest reports without creating like perverse incentives that's something I'd want to know about like how much I mean how many how much human labor is going to safety versus capabilities and how much internal AI inference is going to safety versus capabilities how much fine-tuning effort is going to safety versus capabilities and I think I think they have like a much better shot if they're if they're stepping it up over time on some kind of schedule. Okay. So that's the AI companies who I guess we're imagining would mostly be focused on this strategy for AI technical alignment but you've been thinking about this more in the context of open philanthropy and like what niche it could fill. What would open philanthropy need to do if this was you know dumping billions of dollars onto this plan but became its mainland strategy? Yeah. I think that for now the biggest thing we need to do is very similar to the biggest thing I think society needs to do for preparing for the intelligence explosion which is really trying to like track where we're at right now in terms of how useful AIs are for the work that we do and the work our grantees do. I think pushing ourselves to automate ourselves and to pushing our grantees to automate themselves and like tracking you know how good is AI at the stuff Forethought does? How good is AI at the stuff that Redwood Research or Apollo does? How good is AI at the stuff that our policy grantees do? And I think that is just like one thing is just like just socializing within ourselves like that hey like it's a it's probably a big deal when the AIs start to get really good at any given like good thing we're funding and once we start to see signs of life there we should be like prepared to potentially go really big on that and like you said earlier I do think crunch time isn't like 100% a special thing like we absolutely shouldn't be like waiting until crunch time to do anything at all it's just the prediction that like crunch time is the point when a lot of things that were hard to automate before become easier to automate so if there are some if it turns out for example that like AI is really good at math research which I think is plausible then maybe we should be trying to deliberately shift our technical grant making towards more mathy kinds of technical grant making because that is an area where you can like churn a lot more like that's just so much more tractable so I think just having a function that is looking out for these things and is maybe just like poking OpenPhil and OpenPhil's grantees to like consider shifting their work towards more easily automatable things like consider repeatedly testing whether their work can be automated is a big thing and then I think I could imagine down the line something like even just having separate accounting for like the rest of our grant making versus grant making that is going towards paying for AIs for our grantees like you know we already pay for like ChatGPT Pro subscriptions and like ChatGPT API credits for tons and tons of grantees I think just making it a bit more salient in our minds like what fraction of our giving is going towards that and do we endorse its size and do we is there like any place where we should be going bigger and are we on track is the percentage climbing the way we like think it should be does that seem in line with like the way AI capabilities are climbing are we on track to you know if we think crunch time is going to start in six years are we on track to have inference compute be like a large fraction of our spending at that time if I think about this kind of psychologically I could imagine you know if I was leading Open Philanthropy or I guess I was one of the donors being advised and we did have these transparency requirements and we did start getting a sense that an intelligence explosion might be kicking off I could imagine dithering for a long time rather than deciding to commit billions of dollars towards this because there's only a particular amount of money there's only a particular size of endowment and I think I would be like very scared that it's we'll be going too early or this is a bad idea or we're going to have egg on our face afterwards because it will turn out there were some early signs of intelligence explosion but it's not really going to work out and then we've like spent 10 billion dollars and we have nothing left to show for it that would be you'd feel really bad if you made that mistake does that sound like a plausible way for things to go oh totally I mean I think this is a very natural institutional I think even even beyond just being scared of making a mistake on this front it's just that organizations have particular ways they do things and there's like processes and right now Open Phil's process for grant making looks like usually someone fairly junior gets an opportunity come across their desk either through one of our open calls or through some contact they have and that junior person pulls together some materials to convince their manager it's a good fit and then that manager sort of convinces someone higher up that it's a good fit and you can have two layers or three layers or sometimes four layers of information cascading up the decision making process that we have in place as an org and then it's approved and it's just like if the right thing to do is to spend a billion dollars on like some particular strain of work that's like super automatable it just like that isn't even like you wouldn't trust some random junior person to make that call you need to you might need to have just a different process for that and like you need to like and I don't know what that process would look like but I think that would be like one thing to figure out I guess for this sort of incredible scaling of funding and effort to take place it would have to be you're going to be incredibly bottlenecked on people or there won't be like that many more people involved so it would have to be the AIs not just doing the object level work but also deciding what problems to work on yeah helping us with our decision making like managing the project and overseeing other AIs basically just taking up the entire org hierarchy so that's the picture that you're envisaging yeah so I think there's two possibilities here one possibility is that by the time it's the right move to dump a bunch of money on crunch time AI labor OpenFill itself has already been largely automated and that's actually like an easy world because in that world we just have a visceral sense that AIs are really helpful because they like we've like you know maybe we've slowed down our junior hiring and like all our program associates are AIs right now and like you know we are totally transformed as an organization so the like evidence like the conviction to pull the trigger might be easier to achieve and then actually we have a bunch of labor so maybe we have like a thousand people on the like AI team instead of like 45 that we have now and they can like you know figure out all this stuff much more quickly but I think the like concerning possibility is actually there's jaggedness where maybe AI is extremely good at math and maybe AI is extremely good at technical AI safety and like certain specific kinds of manufacturing that could be really useful for like a PPE play but it's not that like we haven't automated ourselves it's not that good at doing our jobs because like there wasn't much of that stuff in the training data we're just not like well set up to absorb AI labor yeah it makes horrible mistakes in a way that like you can put it in a setup in like software or manufacturing where you catch those mistakes but it's harder you need humans to do that on the open fill side so we're not very automated we don't have a visceral sense of you know it's time now like this is the moment like AIs are really really good we gotta go big but it's still the right thing to do to like pour a bunch of money into AI labor on these like few verticals that are like heavily automated I think we've maybe actually been burying the lead a little bit here on what the biggest challenge is for an external group like open fill to implement this plan which is will you even be given access to the very best models that are being trained and I guess at this crunch time when there's a crunch on demand for compute will you actually have enough computer chips so will anyone be willing to sell to you for you to do this kind of work can you go into that yeah so I think there's there's two challenges here to getting access to enough labor as an external group one is whether they will just even sell to you so like I said earlier in AI 2027 and a lot of stories of the intelligence explosion you get to a point where one company has pulled far enough ahead of its competitors that it keeps its internal best systems to itself and only releases systems that are like considerably worse than its internal frontier that are just good enough to be like ahead of its competitors released products and there can be a growing gap in like how intelligent the best internal systems are and how intelligent the best externally accessible systems are and the AI company may deliberately choose not to sell to willing customers because they want to keep their secrets to themselves another possibility is they might be willing to sell to you but the price just might be way too steep because the opportunity cost of using that compute to like sell to you to do whatever you want to do with it is training further more powerful AIs and they might be willing to pay quite a lot for that so I think both are challenges the second one is in some sense more straightforward to address which is you try to like hedge against this possibility by having some portion of your portfolio like really exposed to compute prices and hope that you know maybe that looks like in the extreme case just having GPUs yourself that you know in peacetime you just rent out to other people doing commercial activity with it but then during crunch time you redirect to doing AI labor although in that case you'll have to furthermore figure out how to get the latest AI chips like the latest AI models onto those chips that you own so you might have to cut deals to make that happen but also like in less extreme cases you might just purchase a bunch of NVIDIA or purchase a bunch of liquid public stocks that are exposed to AI to make it more likely that you can afford AI capabilities at the time so there could be a huge run up in the price of GPUs or compute at this time but you can partly hedge against that possibility by having most of your investments be in NVIDIA or other companies that sell GPUs so that if their price goes up you benefit on the investment side and that helps to offset the increasing price okay and then on the software side there's a question of whether you have access to the very best models that are being trained I guess on the one hand there's this story you can imagine where the companies are very close together the models are roughly the same margins are very low they're very keen to put out models as soon as possible in order to remain competitive I guess on the other hand you could have one leader that's starting to keep things all secret do you have a particular take on which of these scenarios you think is more likely to come about yeah I think that at least at the beginning part of crunch time like when the AIs are just starting to automate a lot of AI R&D my bet is that things will at that point be relatively commercial relatively open the leading few companies are within you know a month of each other in their capability frontier or maybe it's like hard to say who's in the lead because like one company specializes in like one aspect like you know their model is like a little spiky on like pre-training and another company's model is a little spiky on software engineering or something like that and I think that the reason I think that is basically just because it's kind of what like a naive econ 101 model would predict would happen it seems like these companies don't have big moats and it also seems like what we've seen happen over the last few years it kind of describes the present day more or less it describes the present day and that's a change from a few years ago where I do think open AI had like way more of a lead and it seemed more plausible that there would be like a monopoly or a duopoly but there are reasons to push in the other direction which is basically that if you have a super exponential feedback loop you have a bunch of actors that are growing at an increasingly rapid rate like first at 2% then at 4% then at 8% and they don't interact with one another you do get a winner take all dynamic where if they're growing on the same growth curve but one gets there gets to a particular milestone first the bat leader gets more and more and more powerful and wealthy relative to the laggards this is in contrast to exponential growth where if everyone is growing at 2% forever then the like ratios between more and less wealthy nations or companies stay fixed so there is a reason to think that specifically around the time of the intelligence explosion gaps will begin to grow again but I think probably around the start yeah it will most likely be the case that like you can buy AI labor if you can afford it you can buy API credits you can go on chatgpt.com and then I think I have a lot of uncertainty about how it evolves from there yeah what do you think is the chance that the leading company will try to keep the level of yeah the level that they're reaching secret I think it depends a lot on the competition landscape they face so basically if the other companies are really far behind then I think there's a pretty strong incentive and reason to keep your capabilities secret because you give up like sort of quarterly profits but maybe you don't care about that because you're running on investment money anyway and if you can get your AI to help you make better AI to help you make better AI and so on you could emerge with like super intelligence that might give you a power that rivals nation states or like the ability to just like decisively control how the future goes and that might be like very attractive to a sort of power seeking company I do think it does involve foregoing short term profits though which means that if competitors are close at your heels and your investors are breathing down your neck to like deliver quarterly earnings it'll be like hard you can't go and tell all of your investors oh don't worry we have like we have a super intelligence because I think then word will get out well and then also they your plan is to screw over the investors in this case your plan is to create a super intelligence not to pay them back so create a super intelligence and take over the world maybe like they won't like that there's like a mismatch in incentives between the investors and the CEO and the CEO is sort of being a bad agent to their principle so basically like the more things look like an efficient competitive market with very little slack the more the leading company will be sort of forced to provide access to the rest of us to what extent do you imagine the companies would be enthusiastically bought in on assisting with this plan because so this strategy is their predominant approach to AI technical safety I think even the optimists agree that there are other issues the society's gonna have to deal with in fact they say this all the time the ladies of the companies that we're gonna need a new social contract it's gonna upend everything yeah it's gonna be a big deal I imagine that in as much as they're nervous about the effects that the technology is gonna have they'll be very happy if someone came to them with a like pre-prepared plan for how here's how we're gonna deploy all of this compute in order to solve all these other problems yeah I think it's unclear I think there are certainly they have some incentive to be into this but the two sort of alternative uses of AI labor that might be more attractive to them are like one power seeking for themselves just like you know building up an enormous AI lead over everyone else and then sort of bursting onto the scene with an incredible amount of power and like the ability to like challenge like the US government or like nation states might be attractive to some people I think that would be like a very evil strategy to pursue but it's definitely in the water the other thing is more mundane it's just using these AIs to make normal goods and services to make the products and the media content and the other services that people most want to pay money for in a short term sense it's very similar to how right now we don't spend a huge fraction of society's GDP on bio defense and cyber defense and these other things and moral philosophy it's just like that's not what people want to pay for and AI is like another it's just a thing that accelerates the creation of products and services people want to pay for and this isn't very high on the list I guess most people are not looking to become dictator of the world or to take on huge amounts of power but I guess the kinds of people who end up leading very risky technology projects are not typical people they're like somewhat more ambitious than the typical so I suppose we can't potentially rule that out as a possibility yeah so a possible challenge would be that even if you have an enormous amount of compute there might just be only so fast that you can go because you require some sort of sequential steps so there's some step that is just like bottlenecked in time like you have to do I guess people talk about things where you have to do an experiment that just actually takes a certain amount of time to play out but more generally at least with LLMs for example they produce like one token after another and having twice as much compute doesn't necessarily allow you to basically complete an answer twice as fast without limit how much is that an issue here and as much as we're trying to solve problems in like a very short calendar time yeah I think that that is likely to come up especially for physical defenses like manufacturing PPE or scaling up the ability to rapidly create medical countermeasures and then also for social and policy things so I can imagine that AIs could be very helpful in figuring out what kind of agreement between the U.S. and China would be like mutually beneficial and how we could enforce it but the way human decision making works still probably requires humans from the U.S. and China to come together and like talk about it you know have a like you know a conference or convening and come to a decision that they ratify and they feel good about and that could be a bottleneck yeah are there any other examples of similar bottlenecks I guess in terms of solving theoretical problems I suppose you can speed things up enormously by having like many many different instances of the same model like try to brainstorm different solutions and then have them evaluate one another and that allows you to kind of have many different efforts in parallel but it's also I do think for deep theoretical problems you can speed things up by having efforts going in parallel but the right solution that's out there somewhere involves like multiple leaps where like it's hard to think of the next insight without having the foundation of the earlier insight so really even if you have a hundred AIs working in parallel what will happen is that one of them comes up with the first step of the insight and then everyone is working on in parallel and finding the next insight but you still need to go three or four steps in so what sort of stuff do we need to be doing in advance I guess like for example setting up planning meetings ahead of time for like diplomats between the US and China we basically need to do that at the very early stage in anticipation that eventually we might have a deal that they might want to ratify I guess that sounds a bit crazy but are there other examples of things that you need to do before this all kicks off yeah I think that in general you want to be thinking about what would the AIs at the time be like most comparatively disadvantaged in they'll have like all these advantages over us they'll understand the situation but much better at that point in time than we do now they'll be able to think faster move faster and so on but I think what we can contribute now would be things that just inherently take a long lead time to set up so that might include physical infrastructure like the bio infrastructure that my colleague Andrew is working on building out it might also include just social consensus like I think it takes some amount of time for an idea to be socialized in society to have it as an accessible concept that maybe we should try and create some sort of treaty between the US and China to allow AI to progress somewhat slower than it might naturally and use a bunch of AI compute to solve all these problems I think that that kind of thing takes years to become something that's in people's toolkit in the water such that they actually think to have the AIs like go down that path and like figure out the details of that so what should people be doing if they think that this kind of makes sense or it's something that they'd want to contribute to are there other organizations that should similarly be sort of planning ahead and thinking about how this might look for them or could individuals be thinking about how they could contribute to I guess adopting this approach for their own particular projects yeah so in terms of other organizations I think it would be especially great for government entities to be thinking about adopting AI I know that there's just a number of random little types of red tape that make it harder for governments to adopt AIs than for anyone in industry to adopt AIs and I think we might end up in a situation where like you know the regulatees the like industry people have like fast cars and the regulators have like horses and buggies because of this like differential adoption gap and I think just more broadly if you if your company is not already going like maximally hard on adopting AI for your personal use case and you work on defenses AI safety you know moral philosophy all these good things it's probably worth like having a team that's just kind of on the lookout for like how could you like sort of adopt AI as soon as it becomes like actually useful for you let's talk a bit about the career journey that you've been on since since we last did an interview two and a half years ago I guess back then you were doing general AI research and strategy for open philanthropy this is in 2023 and then in 2024 you started leading the AI technical grant making and then I guess towards the end of that year you decided to take four months off and take a sabbatical yeah tell us about all of that yeah so I think that before I had been at OpenPhil for more than six years before I made my first grant I was involved in like some grant making conversations earlier but the first grant I actually led on was like somewhere in mid or late 2023 and I had joined OpenPhil in 2016 so it was kind of interesting like my work at OpenPhil in some sense if you kind of if you just took the outside view and said you know this is a philanthropy that's giving away money my work there was like very strange because it was kind of thinking about these heady topics and then like writing these like long reports that I published on less wrong about them and I always felt a little like oh maybe I should dip into grant making because that is like our core product in some sense it's what we do but I had always been sort of drawn away by like deeper intellectual projects so even though I like always vaguely had the thought that I should do grant making it never really happened for me until actually I think the thing that pushed me head first into grant making was the FTX collapse so actually sorry my first grant must have been in 22 instead of 23 because at that point there were hundreds and hundreds of people who had been promised grants by the FTX foundation where their grant wasn't going to go through or they were worried it was going to be clawed back or it was partially not going through an open fill sort of put out this emergency call for proposals for people who had been affected by the crash and I had I had some thoughts and takes on technical research and also just the organization needed help like surge capacity for this sort of emergency influx of grant making so in a matter of maybe six weeks or so I made like 50 different grants after not having made any grants at all and that was a really interesting experience and I discovered there were elements of it I really liked but there were also there was just like something about the way you made grants where you just really couldn't dig into any particular thing very much especially in the context of something like the FTX emergency you just had to be like making these decisions really quickly but I felt like I had thoughts about how grant making could be done with like more at least in the technical AI safety space could be done with like more inside view justification for the like research directions we were funding than we had previously and so in early mid 2023 I sort of tried to go down that path sorry so in 2022 you did this huge burst of grant making I guess trying to help a bunch of refugees from the FTX foundation basically but then you thought I guess you would have noticed that there's probably no overarching strategy behind all the grants you were making and you were like we need to have a bigger picture idea of what we're actually trying to push on and why yeah so I was focused on grants to technical researchers so these are often academics sometimes AI safety nonprofits and they seemed like reasonable research bets but I felt unsatisfied and I think this is going to be a theme of me and my career I felt unsatisfied about how the theory of change hadn't been really ground out and spelled out as to how this type of interpretability research would lead to this type of technique or ability we have and then that could fit! into a plan! to! an AI take over in this way or similarly for any of the other research streams we were funding and this had been actually the big thing that deterred me from getting involved in OpenPhil's technical AI safety grant making for a long time even though I was one of the few people on staff that thought about technical AI safety outside of that team it was because in the end it seemed like most grant decisions in this 2015 to 2022 period turned on like heuristics about this person's a cool researcher and they care about AI safety which is like totally reasonable but I think I wanted to like have more of a story for like and this line of research is addressing this critical problem and like you know this is why we think it's plausibly likely to succeed and this is what it would mean if it succeeded and we never really like had that kind of like very built out strategy because it's like very hard it's a lot to invest in building out a strategy like that but you know having been thrown head first into grant making with the FTX crisis I because all the people who had worked on that portfolio had left by that point some to go to FTX foundation actually and so like it was this portfolio that had been somewhat orphaned within the organization and it was clearly a very important thing and I was like oh maybe we could approach it in this kind of novel way for us in this area to really try and form our own inside views about the priorities of different technical research directions and really connect how it would address the problems we most cared about It sounds like you find it unpleasant or anxiety inducing to make grants where you don't have a deep understanding of what the money is not so much what the money is being spent on but you don't have a personal opinion about whether it's likely to bear fruit is that right yeah or like I think it's a bit nebulous what the standard is that I hold myself to but I think for my research projects when I think about timelines or I think about how AI could lead to take over or how quickly could the world change if we had AGI I think I can often with months of effort get to the point where I can anticipate and have a reasonable response to and a reasonable back and forth with a very wide range of intelligent criticisms for why my conclusion might be totally wrong and totally off base I feel like I know what the skeptics that are more do-me than me will say and I know what the skeptics that are less do-me than me will say and I could have an intelligent conversation that goes for a long while with either side and that is a standard I aspired to get to with why we supported certain grants and I could do that with some of our grants but I wanted the program to get to the point where like if somebody came to me and said like you know isn't interpretability just actually like hasn't seen much success over the last four years what do you make of that I wanted to have like I wanted to kind of be at reflective equilibrium on my answers to questions like that and wanted to be able to like say something that went a bit beyond like yes but like you know outside view we should support a range of things and that that is something that I think emotionally like is unsatisfying to me if it's like a big element of my work yeah it's maybe worth explaining why it is that OpenPhil doesn't aspire to get to that level of confidence with most of its grants why is that I think it just takes a long time I think there's two things it just takes a lot of effort and then the other thing is that even if you put in that effort you don't want to fully back your own inside view and then I think I wouldn't endorse that either and so it like it's this one-two punch where it's just like developing your views about exactly how interpretability or adversarial robustness or control or corrigibility fits into everything is a ton of work you have to talk to a ton of people you have to write up a bunch of stuff and in the meantime you're not making you're not getting money out the door while you're doing all this stuff right and then having done all this stuff like where are going to end up in a place where there are reasonable views on both sides and like it's a complicated issue we probably want to hedge our bets and defer to different people with different amounts of the pot and so on and so I think people have a reaction that's very reasonably like we're going to end up in a place where we've thought it through it was a lot of work it's still very uncertain we still want to spread our bets so why not just get to! I think I have sympathy for that hopefully I represented that perspective reasonably well but I just feel like in my life in my experience like having done the homework like really qualitatively changes like the details of the decisions you make in ways that I think can be really high impact like one thing that I am able to do having like gone through the whole rigmarole of forming views is work with researchers to find the most awesome version of their idea by the lights of my goals and pitch them on that and co-create grant opportunities and I think there's just something that I maybe won't be great at defending but I just feel like there are other nebulous benefits beyond that and I really like operating that way so in 2024 you actually took on responsibility for this whole portfolio but I guess your personal philosophy of how to operate is somewhat in tension with how OpenPhil as a whole is tending to operate or just in tension with in the short term making a large volume of grants right so what did you end up doing in the role so I think I ended up pursuing a compromise where one thing that just comes with the territory of this role is that there were there have been grantees that we made grants to in the past that are up for renewal and like part of the responsibility of being the person in charge of this program area is that you investigate those renewals and make decisions about whether we should keep the grantees on follow what an open fill canonical decision making process would be there and so I tried to pursue kind of a barbell strategy for a while where on the one hand there were either renewals or people who knew us who reached out to us to ask us to consider grants where I wouldn't hold myself to the standard of really on the technical merits understanding and defending the proposal but would lean more on heuristics like this person seems aligned with the goal of reducing AI takeover risk this person has a broadly good research track record and so on and try to make those grants relatively quickly but then I would also be trying to develop a different funding program or some grants that I really wanted to bet on where I would try and hold myself to that standard and try and write down why I thought this was a good thing to pursue and it turned out that the second thing basically turned into making a bet in late 23 to mid 24 of AI agent capability benchmarks and other ways of gaining evidence about AI's impact on the world agents so sort of the stuff we were talking about earlier where you're trying to get an early heads up about whether the AI's are going to be effective agents I guess 2023 we were really unsure how that was going to go it seemed like agents in general have been a bit disappointing or it hasn't progressed as much as I expected or probably as you expected! open fill has done technical safety requests for proposals before but this was by far the narrowest and most sort of deeply justified technical RFP that we had put out at that time where I was like we are looking for benchmarks that test agents not just models that are chatbots and these are the properties we think a really great benchmark would have and these are examples of benchmarks we think are good and not so good and we had a whole application form that was in some sense sort of guiding people to trying to elicit the information about their benchmark that we thought would be most important for determining whether or not it was really informative and mostly this was just be way more realistic have way harder tasks than existing benchmarks even if you were pretty happy with how that turned out it was like like you would expect a lot of effort poured into like one sort of direction and you know if you were if you were skeptical of this like sort of high effort sort of approach to grant making there would be this like you could argue that like you know I could have just like put in way less effort funded like you like 10 different areas picking up the low hanging fruit in all those areas so I guess halfway through 2024 you started feeling pretty burnt out or like you wanted to take a bit of a break why was that yeah I think throughout this so right around when I switched from doing mostly research to doing grant making and especially when I was like trying to ramp up this program area that had this more like inside view more understanding oriented approach to AI safety research Holden who had been running the AI team up to that point decided to step away and left the organization and he was my manager and I think that I had a working relationship with Holden that involved a lot of arguing and discussing about the substance of what I was working on and when he left leadership was stretched more thin because someone in leadership was gone and I think the people who remained in the leadership team didn't have as much context and fluency with all this AI stuff as Holden did so when I wrote up this big memo being like we should do AI safety grant making in a more understanding oriented way and we should develop inside views and here's why I think that would be good and I think what I wanted was for my manager or leadership to argue with me about the object level on that and for there to be some sort of shared view within the organization about how much this was a good idea or what are the pros and cons of it and how much we want to bet on it but I think that was like just kind of unrealistic given the other priorities on their plate and like given their level of context in this area so I ended up having to sort of approach it in a more transactional way with the organization it was more like rather than let's talk about whether this is a good idea it was more like well I want to! and it's like and so I felt like kind of lonely because I think and this is something I learned about myself like over the course of trying to run this program and then going on sabbatical and reflecting on it that I really like to be kind of plugged into the central brain of the organization I'm part of and I sort of didn't like I didn't feel like I had a path to do that and instead like what I had a path to do was to like stand up this thing which I tried to do but it just like felt a bit tough going and like it sounds like you're a bit on your own yeah I felt a bit on my own and I'm not a very like entrepreneurial person I think or like I'm ambitious in some ways but like it's not I just really have a high need for like constantly talking to other people and I tried to achieve that sense of team by hiring people under me to help me with this vision but I think I was not very good at hiring and management partly it was because this vision was pretty nebulous and I think I probably needed to spend more cycles working out the kinks in it by myself and really solidifying what it is and what's the realistic version of doing an understanding oriented technical AI safety program so it was very hard to hire because you had to hire for someone who really resonated with that off the bat even though it wasn't a very well-defined thing so that took a lot of energy and then I think with people I was managing I have always struggled and in this case still struggled with perfectionism in management so I have this long history of trying to get people to serve as writers who write up my ideas and it never works for me because they don't do it just the way I want it and I'm myself a pretty fast writer and so working with a writer as their editor and getting their writing output to be something I'm satisfied with often ends up taking more time than doing it myself and I found the same happened to some extent with grantmakers where at one point we had a number of people sort of spent part of their time working on the benchmarks RFP and I think it's possible that I would have just moved through the grants faster if it were just me working on it which is a bit tough I think this is a weakness or challenge a lot of new managers go through and I was going through that at the same time as feeling the feedback and engagement I got from above me was much less than it was before and I had to prove this new way of doing things and felt I thought and still think there was a lot to the arguments I was making but also it was not a wild success when I took a swing at it by myself so September last year you decided to step away and take some time away from work after 8 years of working very hard full time what did you end up doing with that time it was a mix of things I just did a lot of like life stuff like I don't know and just invested more in like I found a new group house to move into or like started a new group house so that was cool did more just like trying to take care of myself I started an exercise habit off that exercise habit now again so we'll see and then I did a lot of reflecting on why this work situation ended up being so hard for me and like also just like my journey through just like my career as a whole and like what are the patterns and when things were hard for me I also just jumped in and helped with some random projects going on so the curve conference which is a conference that kind of brings together AI skeptics and AI safety people and people on all sides of the issue of AI's impact on society that was having its first iteration while I was on sabbatical so I was able to get involved with that more and try to be helpful more than I could have been if I had a full time job which was really cool did some writing most of that writing hasn't been published but it was still good for me to do but yeah it kind of went by really fast honestly there was a lot of stuff to think about and a lot to do yeah what sorts of reflections did you have on your career so far and your motivation and what had been difficult in 2023 and 2024 yeah so I think in terms of like I want to be like an advisor and a helper to the kind of central organization and I had been that in many ways over the previous six years so the transition to being more entrepreneurial and more like I have a little startup making grants in my area and the organization is investing like money in me but not necessarily a lot of like attention and I didn't necessarily have a path to like make arguments that then influenced like stuff in a cross cutting way that was hard so I think that was interesting to learn about myself that like that's if I don't have that I will still sort of gravitate towards trying to like meddle in like everything else that's going on and if I don't have like a productive path to meddle I'll feel sad that was one big thing I think another big thing is this just how much depth do I want like I do think I really want to like I have a drive to really like get to the bottom of something or just like I'm always like thinking about the counter argument and the counter argument to the counter argument and like the stuff I liked even when I was very young like I really liked math tutoring and like I really liked math in general because you could just like dig and dig and like get to an answer and that's just inherently like a like uneasy fit with grant making or just like investing like Fox stuff yeah it's like venture capital that OpenFillers engaged in in a way yeah yeah yeah so that was also interesting to! and like I said it was like somewhat strangely for my first six or seven years at OpenFill I actually just did do like rather deep research even though we were a grant making organization I just wasn't doing grant making yeah because Holden really wanted this deep research he wanted to more deeply understand the idea yeah personally and he thought it was healthy for the organization yeah I think that's right I what like different leadership it's like probably pretty unlikely we would have gone as deep as we did into doing our own AI strategy thinking because the thought would have been like well that we should fund like a place like FHI or like now forethought to do that stuff instead of us in your notes you said that you spent a fair bit of time reflecting in this period about what it had been that you liked about effective altruism as an ecosystem and as a mentality and what things you didn't like so much about it tell us about that yeah so I guess it's been a long time since you've talked about effective altruism in the show so I'll just sort of open with what it even is which is this movement or idea that you should think explicitly and seriously and quantitatively about how you can do the most good with your career or with your money that you're donating and that different career paths and different charities you could donate to could differ by orders of magnitude and how much good they do so like if you are working on reducing climate change it could be orders of magnitude more helpful to work on researching green technologies versus to work turn off their lights more or conserve electricity more in their personal use and there's this ethos that if you're really taking this seriously and you really care about helping the world you stop and think and you do the map in the same way that if you had cancer or your spouse had cancer you would do the research and figure out what treatments had what side effects and what treatments had what first into the EA rabbit hole when I was 13 so it's been more than half my entire life that I've been extremely involved in this community this way of thinking and I think there were maybe three big things that I really liked about this approach one is just that EAs sort of challenged themselves to care about people and beings that were very different from them very far away from them in time and space so even the most like sort of quote unquote vanilla like EA cause area of global poverty the vast majority of money that goes to alleviating poverty given by individuals in rich countries goes to helping other individuals in rich countries even though money could go much much further overseas in countries where people have a much lower standard of living and the reason people donate locally is that they feel more affinity for people who are closer to them and more similar to them and EA also has a lot of strains that challenge people to extend care to animals to extend care to future generations that may live thousands of years or millions of years in the future to artificial intelligence also if if if it can be something that has consciousness and can feel pain and so on and that was really appealing to me but then there were also just like there's a way of going about doing things that was also very appealing to me which was like they were very nerdy they were very intellectual they were like really like thinking stuff through and almost like innovating methodologically on like how can we figure out which charities are better than which other charities and like there are lots of like interesting like arguments thrown around for this and they were all they were very transparent like there's just the culture of like open debate and like admitting your mistakes give well like an early sort of pillar of the early EA movement had a mistakes page on its website where it just discussed mistakes that it made they were very like honest and high integrity in like an interesting way that doesn't obviously follow from like caring about other beings more for example like give well refused to do donation matching because donation matching is usually a scam where like the big donor would have given that much anyway even if you hadn't made your donation so that whole package was really attractive to me I think it really hit a lot of psychological buttons for me at once and really felt like my people and the way I wanted to live my life so there's the being more compassionate to a wider range of beings which I guess is still the case and probably still something you like about the effective atrocious approach but there was also going into enormous intellectual depth and really debating things out and then there was also the very high integrity about honesty like not allowing any chicanery whatsoever or extremely like fastidious and like exacting level of integrity that like other movements even other pretty high integrity movements like weren't aspiring to even beyond what people are even asking you for you just like proactively say by the way did you know donation matching is a scam that's why we're not doing it even though we poor people you know it's interesting that like that was such a natural part of the early EA movement even though like you're sort of giving up on impact you know yeah it's not necessarily implied I mean I guess it's a practical question whether it is or not so I guess as things evolved you found that I guess the second one the intellectual depth was like now lacking from your job were there other things that were kind of changing that made you less enthusiastic yeah I think the so the intellectual depth was very much there in like other parts of the EA ecosystem especially AI safety and like thinking through like how exactly would you like control early transformative AI systems and things like that and like I said my heart was like always pulled towards those kinds of questions even though I worked at a grant making organization it feels like on some level you really were a more natural grant recipient rather than you should have gotten something to really go in deep on some question I think that if I had graduated college in 2022 instead of 2016 like in 2016 I graduated college I went to GiveWell and like a big part of why I went to GiveWell at the time was that they had the most intellectual depth program to upskill in ML AI safety research and then tried to join an AI safety group you know so I think I'm naturally drawn to actually doing the research in some sense so in that sense it was sort of a mundane issue that my job especially after Holden left and the demand for that kind of research evaporated a little bit at the leadership level it was like if I were I probably would have applied to join an AI safety group but then I think like there's a the third thing of just this like extremely almost comically high level of integrity that I really really liked was also like eroding over the years just as like you know when I think about why I think that like when a lot of the focus of the EA movement was convincing really smart people to donate differently being extremely like unusually high integrity was like actually just a really valuable and powerful asset so like obviously people like me and like very wealthy people that were early GiveWell donors really liked that GiveWell had a mistakes page and really liked that that whole ethos and that whole package it helped them trust that the recommendations were actually real recommendations and they weren't being spun! something and they the charity recommendation ecosystem but then like when you move away from that being your primary method of change when instead you've actually attracted quite a lot of funders and now you're trying to use that money and the talent that you've attracted to like achieve things in the world maybe things that involve like a lot of politics then the like the the being like extremely transparent can be like very challenging especially because like donors like want privacy or like if you're running a political campaign you don't want your opponents to know exactly your strategy and like you know the ways that you think you might have made mistakes like it's just like this is not how like most of the real world works you know yeah it's not the case that the world's most impactful organizations are consistently incredibly transparent or even like incredibly high integrity yeah yeah and so there was this tension between the goals which I felt like I should only care about the goals of EA there's like so sort of what EA told me and it kind of made sense to me was that like the point here is to help others as much as possible the point is not to conform to an aesthetic or like do things in a way that feels like cleanest or prettiest but at the same time I think I was like to some extent kidding myself about how much of my own motivation and my own attraction to the concept came from just the goals like just pillar one and altruism versus pillars two and three of like that intellectual depth and like intellectual creativity and this like crazy high level of like openness transparency like having absolutely nothing to hide like you know letting all comers come like I think for me like as a fact about my psychology the latter two things were actually really important for my motivation and they were sort of over time just like smaller and smaller like features of like what it was like to do EA like to try and pursue EA the environment that OpenPhil was operating and became a lot more challenging and a lot more hostile I guess that I guess for years it had been funding all kinds of AI related stuff but as AI became a much bigger industry it became apparent what sorts of concerns different different people had its work in some ways started to just clash with very large commercial interests potentially and also just alternative ideologies that had different ideas about how things ought to be regulated or how things ought to go and so you are now in a world where there are people who would sit down and think how can I fuck with open fill like what can I do to give these guys a terrible day what what did they publish that we could spread that will be embarrassing for them and in that kind of environment where people just latest round that started in 2023 of AI policy heating up open fill compromised a lot on its initial wild ambitions for transparency at the beginning there was this idea that we would publish the grants we decided not to make and explain why we decided not to make them when people came to us for grants there's a reason most organizations don't do that for our earliest two program officer hires we have a whole blog post that we wrote about their strengths and weaknesses as a candidate and alternatives we considered and how confident are we that this will work out and we stopped doing that so there was a level of transparency that's just like I still in my heart want that but it's absolutely insane and then I think the adversarial pressure that you mentioned makes it so that open fill as an organization that funds a lot of this ecosystem has a lot to lose I think if we go down a large number of helpful projects have a much harder time getting funding we have to be a lot more risk averse than many of our grantees even though those grantees are also facing an adversarial environment I think the way they many of them navigated is to fight back and explain their perspective and define themselves in the public sphere and my instinct is to do more of that and to say more and respond but it's harder to do that from open fills position for a number of reasons yeah so over the years a lot of people I guess usually critics have said that effective altruism has some things in common with religious movements to what extent have you found that to be the case and to what extent have you found that not to be the case yeah I mean I think EA aspires to be and very much succeeds at being like a lot more truth seeking than the world's religions and a lot more truth seeking than a lot of other communities and movements in the world so in that sense I think there's a disanalogy that's extremely important I do think there are like it's who really are deeply involved in the EA community it provides like a map of the good life you know it's like a vision of what it means to be good and have a good life it's sort of unlike a political movement in that it doesn't just have like a set of policy prescriptions for the world but like many religious movements it intersects with politics and like there are people who approach political questions like whether you should ban gestation crates for pigs through the lens of their commitment to EA and so it has this and it's not just like a community it's not just like a social club I think like people get solace and friendship from their local community of EAs like people do from their local church community but it is more than that it is trying to say something about like you know the sweep of the world and like your place in it and like what it means to live a good and meaningful life and it like intersects with like politics and community and a bunch of other things while not being exactly the same as it yeah I would think a key way that it's not like a religion is that has quite a functional goal or I guess that's a different aspect of it that well I guess some people like the ideas they like the blog post they don't engage with the community whatsoever and I it think people think of EA as a weird umbrella for those three things and then those three things are basically professional communities pursuing a well-defined goal but I think EA is more like a way of looking at the world and a way of thinking about the good and I think you can take an EA US policy from the perspective of like thinking about the welfare of US citizens doing like rigorous cost effectiveness analysis of like what policies actually help and don't help and a lot of people do and then I think there are like there is EA as a generator of like new cause areas that sort of could get added to the cannon and I think right now there's a bunch of fertile ground with like could EA be a force that helps society prepare for like radical change by advanced AI where AI safety is one big important thing there but there might be like a range of other issues and you might want to prioritize some of those based on like your values and like your sense of how things will play out so you wrote in your notes that at least from your personal point of view EA wasn't enough like a religion or it wasn't as much like a religion as you might personally have liked explain that I think I mean I think I'm someone that just really benefits from structure and from sort of emotional motivation reinforcement and I also just very much tend to like a little bit socially conform or I think I tend to try and achieve the ideal of my corner of the EA community is sort of like you said is just like to have a really impactful job and do a really good job at it and work a lot of hours at it and so that's like the message you get from the community and that's like what I'm trying to do but I think I personally would have liked yeah a bit more of a spiritual angle to the community colleague Joe Carlsmith's blog I think I get some of that like existential reflection about like our morality and our values and this crazy thing that so many EA's believe that in a matter of like a decade or two we might be in an utterly transformed world that might be like relative to this vantage point like utopic or dystopic and just like grappling with that and like I think you know I think if there had been like an EA church where like every Sunday like someone who's like really good and thoughtful about these issues like led a discussion spoken from the spoke about them and led a discussion about them I think that would have been like very enriching for my life and probably ultimately like made me be higher impact but that's just not how the EA community structured and it's like it's sort of deliberately not structured that way because it's like the professional community aspect of EA you really like want to not care if people believe the deepest teachings and philosophical orientation you really want to just be like if you're doing great AI safety research do great AI safety research so the incentives of a professional community pull against what I might personally want here do you think it sounds like you think while it might have been more appealing to you it's like not actually necessarily better for things to go in that direction I instead of thinking about the next google doc I need to write or the next email I need to send I would like to be spiritually marinating yeah exactly yeah yeah I guess people have a range of views but I guess it's clear why many people have not embraced that or let's have a strong division between this sort of thing which can be very stressful and it can be! dangerous and culty and there are a lot of reasons to worry about it but I do think there is just a large contingent of EAs that are like me in wanting some sort of spiritual grounding like Joe Karlsman's blog is extremely popular with hardcore EAs it's not like a generically popular blog it's like reasonably popular but it's just like there are a! number of people who probably an age thing here a little bit as well I guess I feel like when I was younger I noticed that the older people were less interested in this aspect of it and I guess now I'm in the older class and I'm like well I have my family to provide nourishment and that's like absorbing a lot of time and energy that I don't have for attending church or whatever it spiritual person so that wasn't really a niche that I needed scratched I guess earlier on I was maybe more interested in the social scene to make good friends and meet people I guess having made more friends who I think of as like minded and having a lot of common interests with that's kind of not as interesting either anymore I already got my friends and now! I actually think that I have been a religion shaped thing in my life as I age and when I think about why I think it's because when I was 20 I had unrealistic aspirations for my worldly projects I guess by that point I had already been an EA for six or seven years but I was just starting off trying to do EA and slow and just the feeling of doing my job which involves writing these Google Docs and sending these emails is just not automatically connected to my higher aspirations and there is a long grind and there's a lot of failure and so I think I have increasing demand for some separate thing that is specifically trying to reorient me mentally towards the bigger picture for me the bottom line there is that working on this stuff can be quite stressful and quite tiring and I want to completely check out and stop thinking about it and just be with people and talk about other issues the different strategies for trying to keep it manageable I think I probably want some of both now I live in a group house with a couple of little kids which is really great and I watch TV I think about other stuff in the background so I think during your sabbatical you considered going independent and becoming a writer or researcher just doing your own thing but in the end you decided to come back to open fill at least for a while why was that yeah so towards the end of the sabbatical I was planning on taking some time to start a sub stack and write about a bunch of stuff including a lot of the stuff about EA that we were discussing a lot of stuff about AI and sort of see where it went and at that time I honestly didn't have a super strong impact case for this I think I didn't think it was crazy that it would be the highest impact thing to do! But the reason I was I just wanted this and not that I could really defend that it was the highest impact thing but at that moment after having gone through this whole journey I was like maybe I have more room in my life for making a career decision on the basis of not just impact the reason I decided to stay was that basically while I was out OpenPhil was conducting a search for a new director to lead our GCR work so all our AI work and our bio risk work this was the position Holden was in when he left in 2023 and both of the top two candidates seemed really good to me and I felt like someone new coming in was like could probably really use help from someone who's not particularly running any given program area doesn't have a big team to worry about and can just help that person develop context figure out their strategy and then it could be an opportunity for me to see if I could get the feeling of plugging in again that I had been missing for a while and how did it go I think it went really well so our director of GCR is Emily Olsen who's also the president of Open Philanthropy and she's like and I've been spending what have we funded what's come of that what's the AI worldview what do we think is going to happen with AI how's that informing our strategy what are the strategies of the various sub teams and I work really well with her and it's like I actually had been lonely at Open Phil almost the entire time I had been at Open Phil even though it got worse in 2023 because while Holden was really great at giving me a lot of bandwidth and talking about object level stuff with me Holden never ran a ship where he was like I'm doing this bigger project can you help me with this piece of it and here's how it fits in Holden was always more like a research PI where I was doing my own research project and he would talk to me about it a Emily really does operate in more of an integrated way where I'm doing stuff and I know she needs to know the answer and is going to do something with it which is very cool and very novel for me as a way to work and it's something that I always thought I would want and indeed it's really great and I think she's an extremely caring and thoughtful manager for me who's I work more than I did right before I went on sabbatical and it feels less hard so you know that's just like a sign that things are working so you're trying to decide what to do next whether to stay at open fill or go into something more less meta that will allow you to go into even more depth how are using the stuff you've! learned about yourself over the last few years to inform that decision yeah so I'm talking to besides open fill which is still a top candidate I'm talking to two technical research orgs about potentially finding a fit there one is Redwood Research the other is Meter and Redwood Research works on basically futurism inspired technical AI safety research and they're best known for pioneering the AI control agenda and Meter I think of as trying to be like the world's early warning system for intelligence explosion it's like they're measuring all the different measures we want to be tracking to see if we're on the cusp of AI's rapidly accelerating AI R&D or acquiring other capabilities that let them take over both of these missions are very close to my heart they're both narrower than open fill where I could just like if I wanted just like dip my toes in absolutely everything that might help with making AI go well but then in exchange they would let me go deep in a way that I think would probably be more satisfying for me all else equal and in in terms of how I'm using what I've learned I think I think I just like and this is so cliche and it's something that if a 20 year old version of me were watching this she'd roll her eyes but yeah your extremely local environment the literal person you're reporting to matters a huge amount and the two or three people you're going to be talking to most in your job or just features like how much are you talking to people in your job versus working on your own can can just like make a transformative difference and I found it interesting to reflect on like I said all that stuff earlier about how EA has become a lot less transparent and a lot less sort of prioritizing maximal integrity at all costs and that does still bother me and actually sort of the moral foundations of EA I think of like sort of utilitarian thinking you can go down a long rabbit hole where it is very suspect in many ways and we talked about this in some previous episodes but both of those things bother me a lot more when I'm also in a working environment that's locally hard for me you know and it's not like those issues aren't issues but the salience of those kind of heady big picture things versus extremely micro things about what does it feel like when you have a one-on-one with your manager is like I think I had been underrating the mundane and the micro in how I had been thinking about my career up to now and I'm trying to do trials like I'm actually in the middle of a work trial with meter as we're filming this episode and that's what I'm paying attention to like how does the rhythm of the work feel how do the people feel yeah I guess other generalizable observations are that I mean I guess open fill environment changed over the years you were there for eight years but nine years now nine years right yeah but the kind of constraints open fill was laboring under in 2023 were very different than in 2016 and so unsurprisingly like I guess it might have been a good fit for you to start with but that doesn't necessarily mean it will be a good fit forever and also there was a leadership change at open fill the person you were reporting to changes and very often when that occurs you see some other people leave as well because they were! Yeah I think that's possible I mean it sort of was true for me in both directions like I think Holden very much was a huge part of why I wanted to work at GiveWell rather than work in a number of other potential places or do earning to give like I thought I was going to do at first and then when he left that was coincided with a difficult period for me and now with Emily it's again pretty dramatically changed what my work is and how it feels so it does seem like it's a big transformative thing and if you're in an organization where there's a leadership change I think it should probably be a trigger to think about even if you don't leave what might be different about your role and your place and what you're doing based on the different style or the constraints and strengths and weaknesses of new leadership it sounds like taking four months off was also a good call that I guess it stopped you were reasonably unhappy I guess it could have gotten worse though if you hadn't done that and it gave you breathing room to make good decisions yeah I think that's right I'm very glad that I took the sabbatical I'm also glad that I didn't leave I think a salient alternative for me at the time that I decided to take four months off was to just leave and figure out what I wanted to do next and I think it was good both for my impact and for my personal growth and satisfaction that I came back I helped Emily and now I'm doing a proper job search which at the time that I left for my sabbatical it was more like healing and reflecting and not in a focused way searching for a role Coming back to effective altruism for a bit I guess you said we basically almost don't talk about effective altruism on the show anymore I guess it was a much bigger feature in the earlier years the biggest reason for that I suppose is now that we're more AI focused but AI is an issue that so many people are concerned about regardless of their broader moral you don't have to be concerned about being very far away in time to think it would be really good to do AI technical safety research or it would be good to think about what governance challenges are going to be created by it and of course EA should talk about it more or is that kind of just a sensible evolution I think it kind of depends on the show's goals I think my take is that it's correct and good that you don't need to buy into the whole EA package with all of its baggage to worry about misaligned AI taking over the world and to do technical AI safety research to prevent that to worry about AI driven misuse and to do research and policy to prevent that and to just generally worry about AI disruption and think about that but I don't think so I think there should be and there is like a healthy thriving like AI is going to be a big deal ecosystem that does not take EA like I think it's going to be EAs for the most part who are thinking seriously about whether AIs themselves are moral patients and like whether they should have protections and rights and like how to navigate that thoughtfully against like trade-offs with safety and other goals it's going to be EAs that by and large are still the ones that take most seriously the possibility that AI disruption could be so disruptive that like you know we end up locked in to a certain set of societal values like we gain the technological ability to like you know shape the future for millions of years or billions of years and like are thinking about how that should go like there's a lot of degrees of extremity to the AI worldview like even if you accept that AI is going to disrupt! everything in the next 10 or 20 years the people who are thinking hardest about the most intense disruptions are going to be disproportionately EAs because EA thinking challenges you to try and engage in that kind of very far seeing rigorous speculation even though there's a lot of challenges with that and it's very hard to know the future I think EAs are the ones that try hardest to peak ahead anyway yeah I think I guess digital sentience or like worrying about AIs themselves suffering is a good example I guess yeah I would definitely make the prediction that effective altruism will loom large in that a group of people working on that yeah I guess I mean for someone who's not altruistic it's a bit or isn't motivated by social impact it's a bit unclear why you would go into that area it's not particularly lucrative yeah it's and I guess it's sufficiently unconventional I think most people most of the time in their career they want to do something that's acceptable and that their parents will be proud of yeah and it's just a lot it's a lot less clear that digital sentience is going to provide you with the kind of esteem or prestige that many people or safety comfort that many people want in a career so it's maybe natural that people who are altruistically motivated and also I guess like intellectually a bit eclectic willing to be avant-garde are going to be more intellectually avant-garde like like tolerant of like quite a lot of philosophical like reasoning and speculation in a sense I think this might be like what a healthy EA community is it's like an engine that incubates cause areas at a stage when they're like not very respected they're extremely speculative the methodology isn't firm yet you you kind of just have to be extremely altruistic and extremely willing to do unconventional things and then like matures those cause areas to the point where they can stand on their own while also being a thing that many EAs work on and I think like digital sentience and maybe the other things on Will and Tom's list like space governance and thinking about value lock-in and stuff like that are other candidates for EA to kind of incubate the way it incubated worrying about AI takeover basically yeah I feel that less strongly in the case of the value lock-in thing because many of the mechanisms there would be just ways that AI ends up I guess you get a power grab by people or a power grab by AIS or somehow it undermines democracy or deliberation in a way that makes it hard for society to adapt over time I think people are worried about that regardless of both people involved in effective altruism and people who would be very skeptical of it I think that there are some versions of the value lock-in concern that go through something else overtly scary and bad happening like one person getting all of the power and that person's values get locked in and that's how we get value lock-in but I think there's a whole spectrum of things that are sort of like almost like social media plus plus it's sort of like in this distributed way this technology has made us meaner to each other and worse at thinking and has allowed individuals to live in information bubbles of their own creation you can imagine AI is getting way better at creating a curated information bubble for each individual person that allows them to continue believing whatever it is they started believing with like super intelligent help like preventing them from changing their mind and this might be something you think of as an important social problem for the long run future even if it's not if it doesn't happen via like one person getting all the power power is still relatively distributed but large fractions interesting that in thinking about what is the niche that EA can fill that others won't fill the thing you were pointing to was not primarily actually altruism although I guess that is a factor in going into digital sentience perhaps it's actually a research methodology or a research instinct which is being willing to be in that very uncomfortable space between just making stuff up and having firm conclusions that you can stand by because you've taken particular measurements it feels like for some reason that is one of the most distinctive aspects of people who are passionate about effective altruism willing to try really hard to make informed speculation about how things will go and neither just have it be a good story nor be too conservative that you're not willing to actually make hard predictions yeah absolutely and I think even the tamest of EA cause areas like global health and development has a huge dose of this like I think if you look at GiveWell's cost-effectiveness analysis they have to grapple with like how does the value of doubling one's income if you make a very low amount of money compare to a certain risk of death or like the value of like a certain painful disease you could have and they they have to try and like get their answers based on like surveys and weird studies people have done it's it's not very rigorous in the end and they have to like form their judgments and like spell out their judgments and I think the willingness to like tackle questions like this and just be like well here's here's our answer and like you know there's a lot to argue with it's very emblematic of EA organizations including all the best like AI safety EA organizations like Redwood Research yeah I guess like more standard ways to approach those questions would be to like just pick one slightly arbitrarily and then be really committed to it or to be kind of irritated at being asked the question and to say that there's like absolutely no way of knowing or there's no fact of the matter here whatsoever yeah and I guess like yeah trying to be somewhere I guess I don't know whether it's somewhere in the middle but yeah effectual autism and within EA there's kind of like a spectrum in terms of like where in the middle you want to land where like some you know everyone's kind of looking at the person more speculative than them and thinking that they're sort of like yeah they're just like building castles on sand and this is not the way to like you know do things and they're looking at people less speculative than them and they're thinking like you know they're just the streetlight effect and like they're just ignoring the most important considerations and like not working in the most important area yeah yeah so I guess for people who do have that mindset I suppose an important message would be that people should take advantage of the fact that they have this unique mentality or there's like reasonably rare mentality and go into roles that other people won't probably won't fill because they feel too uncomfortable or at least like I guess they think they could just reasonably think it's misguided but other people aren't necessarily going to do this stuff yeah I think that's right and I think it's interesting to think about like if you imagine EA as a as one piece of the world's response to crazy changes like AI there's actually a case that EA should be heavily indexed on research I think there's a the community has gone back and forth with how it thinks about this and I think at first people are just like naturally attracted to research stuff so there was like a huge glut of people who wanted to be researchers and then there was a big push including from ADK and others that know like consider like operations roles and policy roles and like other things that aren't just research and I think that was a good move at the time but I wonder if we if we think about like what is EA's comparative! advantage relative to the world maybe that suggests that some of the people who are doing operations and doing policy but maybe in their hearts just want to be like a weird truth teller like you know thinking speculative thoughts should consider like going back and doing that again my guest today has been Ajay Akotja thanks so much for coming on the 80,000 hours podcast again Ajay Akotja thanks so much for having me