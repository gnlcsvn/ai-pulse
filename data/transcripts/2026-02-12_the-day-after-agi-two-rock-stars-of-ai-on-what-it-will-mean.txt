It will help us cure cancer, it may help us to eradicate tropical diseases, it will help us understand the universe, but there are these immense and grave risks. Welcome to Radio Davos, the podcast from the World Economic Forum that looks at the biggest challenges and how we might solve them. This week, what happens when AI becomes AGI, Artificial General Intelligence, smarter than humans at just about everything. What happens after AGI arrives, really we would be in uncharted territory at that point. Two of the smartest human minds, Demis Hassabis, CEO of Google DeepMind, and Dario Amadei of Anthropic, came together in Davos for a conversation called The Day After AGI. This was for me like sharing a conversation between the Beatles and the Rolling Stones. The editor-in-chief of The Economist asks these rockstar AI founders what the rest of us should know about AGI, which could now be just a few years away. Are we going to get through the technological adolescence of this technology without destroying ourselves? I think this is a risk. This is a risk that if we all work together, we can address, we can learn through science to properly control and direct these creations that we're building. But if we build them poorly, if we're all racing and we go so fast that there's no guardrails, then I think there is risk of something going on. So what exactly is AGI? When will it happen and what will it mean? It's on this episode of Radio Davos, available wherever you get podcasts and at wef.ch slash podcasts. I'm Robin Pomeroy at the World Economic Forum. And with this look at the future of AI and of humanity... It's for us to write as humanity what's going to happen next. This is Radio Davos. Welcome to Radio Davos. This week, we're talking about artificial intelligence and artificial general intelligence. To talk about that, I'm joined by a co-host, Benjamin Larson, all the way from San Francisco. Hi, Benjamin. How are you? Hi, Robin. It's great to be here. It's great to have you. Tell us what you do at the World Economic Forum. I'm an AI safety lead. I work at the Forum Center for AI Excellence. I'm based here in San Francisco. We have what's called the AI Global Alliance, where we have a number of different work streams. And I'm leading one of these work streams called Safe Systems and Technologies, where we're looking especially at the safety and security and so on surrounding frontier models and systems and AI agents. You've joined me here to kind of set up this session that happened at the World Economic Forum's annual meeting in Davos 2026, just a few weeks ago. It's been viewed many, many times online. It is two AI founders and CEOs. Could you tell us something about them? It's Dario Amadei and Demis Hassabis. Who are they and why are they important to listen to? Yeah, definitely. So Dario Amadei and Demis Hassabis, they're essentially two of the most influential leaders that are shaping the direction of advanced AI today. And Dario, he's the CEO and co-founder of Anthropic, and that's one of the leading AI research companies. He was previously at OpenAI, actually, and is probably best known for his focus on AI safety and alignment and building systems that are powerful, but also safe and controllable. That's something that's very much front and center for Anthropic. And then there's Demis. So Demis, he's the CEO and co-founder of Google DeepMind. He was originally trained as a neuroscientist, actually, and he's known for breakthroughs such as AlphaGo, which is a system that beat the world-leading Go player back in 2016, but also is very known for scientific AI, so advanced breakthroughs such as AlphaFold, which is a protein structure prediction algorithm. And Demis, he's long framed kind of AI as a scientific project that's aimed at understanding intelligence itself. And this is where AI, AGI, it comes into the picture as well. So AGI then, artificial general intelligence. Is that a concept that's very clear? I have a feeling there's probably several definitions of it or maybe a scale of definitions. What do you think? If I'd never heard of that expression before, how would you define it for me? Yeah, so I would say perhaps one way of understanding is that you can have narrow or specific intelligence or you can have very general intelligence. So AGI, it stands for artificial general intelligence, and it really refers to a system that can understand and learn and apply knowledge across a whole range of tasks that are comparable to that of a human being. So if you think about our intelligence, Robin, we don't have intelligence in just one narrow and very specific domain, but we're able to interact and operate in the physical world. And that demands kind of a very broad and generalized sense of intelligence. So you can think of this as kind of AI systems that are moving from being very simple and scoped towards broadly understanding and being able to interact and interpret the world in ways that are actually very similar and very general, so similar to what we do as humans. For people who've been kind of bamboozled by generative AI over the last three years or so, maybe they'd say, well, it looks like some of these models can do that already because you can ask one of these large language models any question. And, well, it sounds very authoritative. You can ask it to do tasks for you as well, to work things out or to create something for you. Is that not already general intelligence to some degree? You could say to some degree it is, and one definition of it. So I'm leading what's called a global future council on AGI that's co-shed by Yoshua Benjua and Akiko Morokami as well, who's the executive director of Japan's AI Safety Institute. And the working definition of AGI that we use is that it would be a system that's able to perform tasks that demands a cognitive function that would be similar to that of a human in a range of digital settings. And you could say that to some extent, some systems, they are definitely already able to do that. But again, it may not have the general in there. So it could be that a system is able to perform one task well or perform well in one domain, but for it to perform across other domains similar to a human and how we are able to switch tasks. That would demand significant engineering still. So you're not just able to take any system and plug it into kind of any digital environment and say, perform these and those actions according to these kind of specifications. And that's something that is still missing. But we're definitely seeing trends and tendencies towards this direction. So we're definitely seeing that as we speak. And why is it such a big deal? Some people also talk about there being a singularity, which I'm not entirely sure you can help us with what that means. Is it we wake up one day and suddenly we're surrounded by incredibly intelligent machines, AIs that have this general intelligence. Why are people so fixated on AGI? Because also you hear a lot of the heads of a lot of these companies saying, this is what we're working on. Maybe there's a race to achieve AGI. Why is it such a big deal? Yeah, so the singularity, just to unpack a little bit what that is. So that's a hypothetical point in time at which AI systems, they would become capable of improving themselves faster than humans can understand or control. So in this kind of scenario, you would have accelerating progress. And that could lead to rapid or unpredictable societal change, essentially. But I think it's worth here stressing that not everyone, they believe that the singularity will happen, or at least that it will be happening anytime soon. But some, they also view it as a useful thought experiment for stress testing assumptions and, for example, about exponential growth. While others, they would maybe treat it as a real risk that demands preparation and significant safeguards already today. So how we view AGI in the work that I mentioned on the council before is much more kind of a progressive approach. It's not something that we're going to wake up to tomorrow and then we'll have AGI similar to how we understand the singularity. But you could view AGI as kind of a jacket form of intelligence that's able to kind of excel and be superhuman in some areas. For example, in terms of the underlying information that's wrapped into the existing language models and how they can respond almost like an oracle would be. And that's very different from how we would respond as humans in terms of a limited memory and understanding of some subjects. But then in other areas where we would excel these systems, we would say, why can't you even remember what we talked about five minutes ago or something like that? So there are real differences and that's why you can understand it as a jagged form of intelligence right now that's accelerating in some fields, but it's still very limited in other fields. One thing that's mentioned, there's a concept that's mentioned many times in what we're about to listen to. They talk about AI models closing the loop. Could you tell us what that means? Closing the loop, it means moving from AI systems that are only able to generate outputs. So if you prompt a large language model today, you would say, OK, I have an input and I expect an output. But closing the loop, it refers to systems that can also take actions, that can observe the results and they can then adapt their behavior accordingly and over time. So that means it's much more about a loop similar to how, again, we as humans think and learn and act. I just don't come up with an answer, but I think through what are the implications of that? Where do I need to search for additional knowledge? How do I solve a part of this task? And based on how I solve all of these different elements, I then go back and I revisit what was the original task. So that will be closing the loop, essentially. So you could say that this involves kind of iteration and it's really important because it's a step towards autonomy. You can imagine that if you're able to close this loop and you actually have greater autonomy and then you can take kind of more infinite steps at solving specific tasks. But of course, very important to also stress that when you do aim to close this loop or you do aim for greater autonomy, for example, in the genetic settings, then you also have new risks that demand new governance approaches. And that's something that we've been focusing a lot on in our work at the forum. So in a closed loop environment, the AI doesn't need any human input anymore. It can go off on its own, learn new things, decide what it's doing by itself. Or am I oversimplifying? I think that would be taking it a step too far in terms of where we currently are. But in the bigger picture, we do see, for example, computer use agents that you can simply give an agent a laptop and you can ask it to do a number of things. And then maybe you can ask it to have a degree of freedom where it can actually go out and do its own kind of thing. And it would go on and take unpredictable actions. So you don't want that in any kind of work setting at all, of course, because there's a number of risks associated with that. But you could also say that these systems are a bit more rudimentary than that in terms of how much is actually actions that are diverted or that are directed at solving or optimizing a specific problem. So that goes back to this notion of singularity that we just talked about and where does kind of direction come from? Does it come from an algorithm that's seeking to kind of maximize its own goals or objectives? Or are those goals and objectives, are they stated by a human in the loop who's actually running a system? So I think that's a core degree of difference in terms of control versus where we realistically are today. But of course, there are scenarios where you have to think much more carefully about what are the systems that we are creating. And once we have artificial general intelligence that are able to close the loop, as we talked about, and what is the outcome of that? And how are you able to control and guarantee that, essentially? There's a recent kind of hype cycle. It's funny. I'm sitting here in Geneva. You're in San Francisco. You're close to where there's interesting things going on all the time. Occasionally, stories break out of the bubble, maybe, of Silicon Valley. And one of them that a lot of people have been paying attention to was this, well, it's changed its name a few times. Open Claw, I think, is its latest iteration. The reason I mention it is because you mentioned you could give a laptop to an AI and set it off doing its thing. And as I understand it, that's kind of what's been happening here. And that's what some people have been getting excited about. A lot of people are quite skeptical about what it is as well. But could you explain to anyone who's not heard of that what it is and why some people were very excited about it? Open Claw, the latest name iteration, of course. So that would be an idea of a personal AI assistant, essentially. So let's say that I engage with Open Claw. Open Claw is entirely open source. So I can essentially have a personal assistant. I can grant it access to my email, my calendar, and my schedules. And for the first time, you kind of have this idea of a personal assistant that's actually useful and helpful. Of course, it's still brittle and it can break down a lot. But the difference, Robin, is that this time it's an open source system. Everybody is able to tweak the agent and interact and give it access to personal resources and so on. Versus a lot of the agents that we've been seeing so far, they tend to be proprietary. It's, of course, a closed source. It's companies that are developing these systems and they are much more targeted at specific use cases and so on. And that's a big difference. But what we've also seen with Open Claw, and this is a cautionary tale, once you begin to grant access to kind of early stage agents and allow them access to all of your systems and so on, it also increases the attack surface for hackers and malicious activity that are able to go in and exploit cybersecurity vulnerabilities and so on. And I certainly wouldn't feel comfortable kind of in that kind of situation or scenario having the ability of having some of my personal data exposed essentially. So it's kind of indicative of the direction of travel that we're going to see much more innovation in terms of agents and AI assistance in the time to come, also in the open source domain. And that paired with the underlying protocols, such as model context protocol and agent to agent protocols, we can expect much more in the time to come in terms of not only AI and personal assistance, but also in terms of agents, broadly speaking, and how that is changing the notion of the underlying infrastructure essentially having a much more agentic web and many more agentic surfaces. And that speaks to what we talked about earlier in terms of closing the loop that we're going to see more autonomy in AI and in software systems in general in the time to come. Right. Well, let's listen to this session then from Davos. This is the two, as the moderator of this session, Zannie Minton-Bedos, who's the editor-in-chief of The Economist. She calls, she says it's like moderate, she's done this a couple of times before with Dario Amode and Demis Hassabis. And she said it's like interviewing the Beatles and the Rolling Stones at the same time, which is a pretty good, pretty good queuing up of it. Before I let you go, though, Ben, you mentioned the Global Future Council. These are groups with a dozen or so kind of experts from around the world that the World Economic Forum brings together. In your case, it's the Global Future Council. What did you say? It's the Global Future Council on AGI. Is there already stuff people can find about that yet or is that something to look out for in the future? Yes. So we have a web page where we are publishing a number of shorter briefing papers, precision papers, essentially. So we're looking into areas such as agency and control that we've been discussing here, Robin. We're looking at international collaboration, what that looks like. We're looking at AGI and policy triggers. So, for example, implications for labor markets, economic and societal transformation and so on. And we, of course, similar to the conversation that we're about to tune into, take a stance on a number of these areas and are here to essentially help inform stakeholders, make better decisions when it comes to what is increasingly capable systems, this notion of artificial general intelligence. What does that look like in the time to come? What do we need to do now in order to be better prepared? I'll put a link to that in the show notes to this. You mentioned one of the chairs of that Global Future Council is Joshua Bengio. I met him in Davos a few weeks ago, recorded an interview, which he talks a lot about the dangers, as do the two people we're about to listen to now, of artificial intelligence. And these are people who really know what they're talking about, all three of them. So please look out for that episode coming up in the next few weeks on Radio Davos. For now, Benjamin Larson, thanks very much for joining us on Radio Davos. Robin, it was a pleasure to be here. Great. Well, let's just go ahead and listen to that. You can watch this session. I'll also put a link to that. It's on the World Economic Forum's YouTube page. It's called The Day After AGI. And the first voice you'll hear will be the chief editor of The Economist, Zannie Minton-Beros. Welcome, everybody, and welcome to those of you joining us on live stream to this conversation. I have to say I have been looking forward to for months. I was lucky enough to moderate a conversation between Dari Amadei and Demis Hassabis last year in Paris, which I'm afraid got most attention for the fact that you two were squashed on a very small love seat while I sat on an enormous sofa, which was probably my screw-up. But I said at that point that this was, for me, like, you know, chairing a conversation between The Beatles and The Rolling Stones. And you have not had a conversation on stage since. So this is, you know, the sequel. The bands get together again. I'm delighted. You need no introduction. The title of our conversation is The Day After AGI, which I think is perhaps slightly getting ahead of ourselves because we should probably talk about how quickly and easily we will get there. And I want to do a bit of a sort of update on that and then talk about the consequences. So firstly, on the timeline, Dario, you last year in Paris said, we'll have a model that can do everything a human could do at the level of a Nobel laureate across many fields by 26, 27. We're in 26. Do you still stand by that timeline? So, you know, it's always hard to know exactly when something will happen, but I don't think that's going to turn out to be that far off. So, you know, the mechanism whereby I imagined it would happen is that we would make models that were good at coding and good at AI research, and we would use that to produce the next generation of model and speed it up, to create a loop that would increase the speed of model development. We are now, in terms of, you know, the models that write code, I have engineers within Anthropoc who say, I don't write any code anymore. I just let the model write the code. I edit it. I do the things around it. I think, I don't know, we might be six to 12 months away from when the model is doing most, maybe all of what SWEs do end to end. And then it's a question of how fast does that loop close? Not every part of that loop is something that can be sped up by AI, right? There's like chips, there's manufacture of chips, there's training time for the model. So it's, you know, I think there's a lot of uncertainty. It's easy to see how this could take a few years. I don't, it's very hard for me to see how it could take longer than that. But if I had to guess, I would guess that this goes faster than people imagine. And that key element of code and increasingly research going faster than we imagine, that's going to be the key driver. It's really hard to predict, again, how much that exponential is going to speed us up. But something fast is going to happen. So you, Demis, were a little more cautious last year. You said a 50% chance of a system that can exhibit all the cognitive capabilities humans can by the end of the decade. Clearly in coding, as Darius says, it's been remarkable. What is your sense of, do you stand by your prediction? And what's changed in the past year? Yeah, look, I think I'm still on the same kind of timeline. I think there has been remarkable progress. But I think some areas of kind of engineering work, coding, or so you could say mathematics, are a little bit easier to see how they would be automated. Partly because they're verifiable, what the output is. Some areas of natural science are much harder to do than that. You won't necessarily know if the chemical compound you've built or this prediction about physics is correct. You may have to test it experimentally, and that will all take longer. So I also think there are some missing capabilities at the moment in terms of not just solving existing conjectures or existing problems, but actually coming up with the question in the first place or coming up with the theory or the hypothesis. I think that's much, much harder. And I think that's the highest level of scientific creativity. And it's not clear. I think we will have those systems. I don't think it's impossible. But I think there may be one or two missing ingredients. It remains to be seen how, you know, first of all, can this self-improvement loop that we're all working on actually close without a human in the loop? I think there are also risks to that kind of system, by the way, which we should discuss. And I'm sure we will. But that could speed things up if that kind of system does work. We'll get to the risks in a minute. But one other change, I think, of the past year has been a kind of change in the pecking order of the race, if you will. This time a year ago, we just had the deep seek moment, and everyone was incredibly excited about what happened there. And there was still a sense, you know, that Google DeepMind was kind of lagging OpenAI. I would say that now it's looking quite different. I mean, they've declared code red, right? It's been quite a year. So talk me through what specifically you've been surprised by and how well you've done this year and whether you think, and then I'm going to ask you about the lineup. Well, look, I think we were, I was always very confident we would get back to sort of the top of the leaderboards and the SOTA type of models across the board. Because I think we've always had like the deepest and broadest research bench. And it was about kind of marshalling that all together and getting the intensity and focus and the kind of startup mentality back to the whole organization. And it's been a lot of work. And but I think we're, and we're still a lot of work to do. But I think you can start seeing the, you know, the kind of the progress that's been made in both the models with Gemini 3, but also on the product side with Gemini app getting increasing market share. So I feel like we're making great progress, but there's a ton more work to do. And, you know, we're bringing to bear Google DeepMind's kind of like the engine room of Google, where we're getting used to shipping our models more and more quickly into the product surfaces. One question for you, Daria, on this aspect of it, because you've just saw you're in the process of, you know, a new round at an extraordinary valuation too. But you are, unlike them, as a, let's call it an independent model maker. And there is, I think, an increasing concern that the independent model makers will not be able to continue for long enough until you get to where the revenues come in. It's made very openly about open AI. But talk me through how you think about that. And then we'll get to the AGI itself. Yeah, I mean, you know, I think how we think about that is, you know, as we've built better and better models, there's been a kind of exponential relationship, not only between how much compute you put into the model and how cognitively capable it is, but between how cognitively capable it is and how much revenue it's able to generate. So our revenue has grown 10x in the last three years from zero to 100 million in 2023, 100 million to a billion in 2024, and 1 billion to 10 billion in 2025. And so those revenue numbers, you know, I don't know if that curve will literally continue. It would be crazy if it did. But those numbers are starting to get not too far from, you know, the scale of the largest companies in the world. So there's always uncertainty. You know, we're trying to bootstrap this from nothing. It's a crazy thing. But I have confidence that if we're able to produce the best models in the things that we focus on, then I think things will go well. And, you know, I will generally say, you know, I think it's been a good year for both Google and Anthropic. And I think the thing we actually have in common is that they're, you know, they're both kind of companies that are, you know, or the research part of the company that are kind of led by researchers who focus on the models, who focus on solving important problems in the world, right, who have these kind of hard scientific problems as a North Star. And I think those are the kind of companies that are going to succeed going forward. And, you know, I think we share that between us. I'm going to resist the temptation to ask you what will happen to the companies that are not led by researchers. Because I know you won't answer it. But let's then go on to the predictions area now. And we are supposed to be talking about the day after AI. But let's talk about closing the loop. The odds that you will get models that will close the loop and be able to, you know, power themselves, if you will. Because that's really the crux for the winner-takes-all threshold approach. Do you still believe that we are likely to see that? Or is this going to be much more of a normal technology where followers and catch-up can compete? Well, look, I definitely don't think it's going to be a normal technology. So, I mean, there are aspects already that, as Dario mentioned, that it's already helping with our coding and some aspects of research. The full closing of the loop, though, I think is an unknown. I mean, I think it's possible to do. You may need AGI itself to be able to do that in some domains. Again, where these domains, you know, where there's more messiness around them. It's not so easy to verify your answer very quickly. There's kind of MP hard domains. So, as soon as you start getting more, and, you know, I also include, by the way, for AGI, physical AI, robotics working, all of these kind of things. And then you've got, you know, hardware in the loop. That may limit how fast the self-improvement systems can work. But I think in coding and mathematics and these kind of areas, I can definitely see that working. And then the question is more theoretical one is what is the limit of engineering and maths to solve the natural sciences? Dario, you last year, I think it was last year that you published Machines of Loving Grace, which was a very, I would say, upbeat essay about the potential that you were going to see unfold. I'm told that you are working on an update to this, a new essay. So, you know, wait for it, guys. It's not out yet, but it is coming out. But perhaps you can give us a sort of a sneak preview of what a year later your big take is going to be. Yes. So, you know, my take has not changed. It has always been my view that, you know, AI is going to be incredibly powerful. I think Demis and I, you know, kind of agree on that. It's just a question of exactly when. And because it's incredibly powerful, it will do all these wonderful things like the ones I talked about in Machines of Loving Grace. It, you know, will help us cure cancer. It may help us to eradicate tropical diseases. It will help us understand the universe. But that there are these, you know, immense and grave risks that, you know, not that we can't address them. I'm not a doomer. But that, you know, we need to think about them and we need to address them. And I wrote Machines of Loving Grace first. I'd love to give some sophisticated reason why I wrote that first. But it was just that the positive essay was easier and more fun to write than the negative essay. So, you know, I finally spent some time on vacation and I was able to write an essay about the risks. And even when I'm writing about the risks, I tried, you know, I'm like an optimistic person, right? So even as I'm writing about these risks, I wrote about it in a way that was like, how do we overcome these risks? How do we have a battle plan to fight them? And the way I framed it was, you know, there's this scene from Carl Sagan's Contact, the movie version of it, where, you know, they kind of discover alien life. And it's this international panel that's like interviewing, you know, people to, you know, to be humanity's representative to meet the alien. And one of the questions they asked one of the candidates is, you know, if you could ask the aliens, anyone question, what would it be? And one of the characters says, I would ask, how did you do it? How did you manage to get through this technological adolescence without destroying yourselves? How did you make it through? And ever since I saw it, it was like 20 years ago, I think I saw that movie. It's kind of stuck with me. And that's the frame that I used, which is that, you know, we are knocking on the door of these incredible capabilities, right? The ability to build basically machines out of sand, right? I think it was inevitable the instant we started working with fire. But how we handle it is not inevitable. And so I think the next few years, we're going to be dealing with, you know, how do we keep these systems under control that are highly autonomous and smarter than any human? How do we make sure that individuals don't misuse them, right? I have worries about things like bioterrorism. How do we make sure that nation states don't misuse them? That's why I've been so concerned about, you know, the CCP, other authoritarian governments. What are the economic impacts, right? I've talked about labor displacement a lot. And, you know, what haven't we thought of, which in many cases, you know, may be the hardest thing to deal with at all. So, you know, I'm thinking through how to address those risks. And, you know, for each of these, it's a mixture of things that we individually need to do as leaders of the companies and that we can do working together. And then there's going to need to be some role for wider societal institutions like the government in addressing all of these. But, you know, I just feel this urgency that, you know, every day, you know, there's all kinds of crazy stuff going on in the outside world, outside AI, right? But, you know, my view is this is happening so fast. It is such a crisis. We should be devoting almost all of our effort to thinking about how to get through this. So I can't decide whether I'm more surprised that you, A, take a vacation. B, when you take a vacation, you think about the risks of AI and C, that your essay is framed in terms of, are we going to get through the technological adolescence of this technology without destroying ourselves? So my head is slightly spinning, but you then, and I can't wait to read it, but you mentioned several areas that can guide the rest of our conversation. Let's start with jobs, because you actually have been very outspoken about that. And I think you said that half of entry-level white-collar jobs could be gone within the next one to five years. But I'm going to turn to you, Demis, because so far, we haven't actually seen any discernible impact on the labour market. Yes, unemployment has ticked up in the US, but all of the kind of economic studies I've looked at and that we've written about suggests that this is overhiring post-pandemic, that it's really not AI-driven. If anything, people are hiring to build out AI capability. Do you think that this will be, as economists have always argued, that it's not a lump of labour fallacy, that actually there will be new jobs created? Because so far, the evidence seems to suggest that. Yeah, I mean, I think in the near term, that is what will happen, the kind of normal evolution when a breakthrough technology arrives. So some jobs will get disrupted, but I think new, even more valuable, perhaps more meaningful jobs will get created. I think we're going to see this year the beginnings of maybe impacting the junior-level, entry-level child of jobs, internships, this type of thing. I think there is some evidence. I can feel that ourselves, maybe like a slowdown in hiring in that. But I think that can be more than compensated by the fact there are these amazing creative tools out there, pretty much available for everyone, almost for free. That if I was to talk to a class of undergrads right now, I would be telling them to get really unbelievably proficient with these tools. I think to the extent that even those of us building it, we're so busy building it, it's hard to have also time to really explore almost the capability overhang even today's models and products have, let alone tomorrow's. And I think that can be maybe better than a traditional internship would have been in terms of you sort of leapfrogging yourself to be useful in a profession. So I think that's what I see happening probably in the next five years. Maybe we, again, slightly differ on timescales on that. But I think what happens after AGI arrives, that's a different question. So I think really we would be in uncharted territory at that point. Do you think it's going to take longer than you thought last year when you said half of all entry-level white-colored jobs? No, I have about the same view. I actually agree with you and with Demis that at the time I made the comment, there was no impact on the labor market. I wasn't saying there was an impact on the labor market at that moment. Now, I think maybe we're starting to see just the little beginnings of it in software encoding. I even see it within Anthropic where I can kind of look forward to a time where on the more junior end and then on the more intermediate end, we actually need less and not more people. And, you know, we're thinking about how to deal with that within Anthropic in a, you know, sense in a sensible way. I, you know, one to five years as of six months ago, I would stick with that. You know, if you kind of, you know, connect this to what I said before, which is, you know, we might have AI that's better than humans at everything in, you know, maybe one to two years, maybe a little longer than that. But those don't seem to line up. The reason is that there's this lag and there's this replacement thing, right? I know that the labor market is adaptable, right? It's just like, you know, 80% of people used to do farming. You know, farming got automated and then they became factory workers and then knowledge workers. So, you know, there is some level of adaptability here as well, right? We should be economically sophisticated about how the labor market works. But my worry is as this exponential keeps compounding, and I don't think it's going to take that long, again, somewhere between a year and five years, it will overwhelm our ability to adapt. I think I may be saying the same thing Demis is, just factored out of that difference we have about timelines, which I think ultimately comes down to how fast you close the loop on coronavirus. How much confidence do you have that governments get the scale of this and are beginning to think about what policy responses they need to have? I don't think that it's anywhere near enough work going on about this. I'm constantly surprised, even when I meet economists at places like this, that they're not more of professional economists, professors thinking about what happens. And not just sort of on the way to AGI, but even if we get all the technical things right that Dario is talking about and the job displacement is one question. We're all worried about the economics of that, but maybe there are ways to distribute this new productivity, this new wealth more fairly. I don't know if we have the right institutions to do that, but that's what should happen at that point. There should be, you know, we may be in a post-scarcity world, but then there are even the things that keep me up right now. There are even bigger questions than that at that point to do with meaning and purpose. And a lot of the things that we get from our jobs, not just economically, that's one question. But I think that may be easier to solve, strangely, than what happens to the human condition and humanity as a whole. And I think I'm also optimistic we'll come up with new answers there. We do a lot of things today from extreme sports to art that aren't necessarily directly to do with economic gain. So I think we will find meaning and maybe there'll be even more sort of sophisticated versions of those activities. Plus, I think we'll be exploring the stars. So there'll be all of that to factor in as well in terms of purpose. But I think it's really worth thinking now, even in my timelines of like five to ten years away, that isn't a lot of time before this comes. How big do you think is the risk of a popular backlash against AI that will somehow kind of cause governments to do what, from your perspective, might be stupid things? Because I'm just thinking back to the era of globalization in the 1990s when there was indeed some displacement of jobs. Governments didn't do enough. The public backlash was such that we've ended up sort of where we are now. Do you think that there is a risk that there will be a growing antipathy towards what you are doing and your companies in the kind of body politic? I think there's definitely a risk. I think that's kind of reasonable. There's fear and there's worries about these things like jobs and livelihoods. I think there's a couple of things that, I mean, it's going to be very complicated the next few years, I think, geopolitically, but also the various factors here. Like we want to, and we're trying to do this with AlphaFold and our science work and isomorphic, our spin-out companies solve all disease, cure diseases, come up with new energy sources. I think as a society, it's clear we'd want that. I think maybe the balance of what the industry is doing is not enough balance towards those types of activities. I think we should have a lot more examples. I know Dario agrees with me of like AlphaFold-like things that help sort of unequivocal goods in the world. And I think actually it's incumbent on the industry and all of us leading players to show that more, demonstrate that, not just talk about it, but demonstrate that. But then it's going to come with these other intended disruptions. And, but I don't, I think the other issue is the geopolitical competition. There's obviously competition between the companies, but also US and China primarily. So unless there's an international cooperation or understanding around this, which I think would be good actually in terms of things like minimum safety standards for deployment. I think Dario would agree on that as well. I think it's vitally needed. This technology is going to be cross-border. It's going to affect everyone. It's going to affect all of humanity. Actually, Contact is one of my favorite films as well. So it's sort of funny enough, I didn't realize it was yours too, Dario. But I think, you know, those kind of things need to be worked through. And if we can, maybe it would be good to have a bit of slow, a slightly slower pace than we're currently predicting, even my timelines, so that we can get this right societally. But that would require some coordination. That is. I prefer your timelines. Yes. I think it would be better for many reasons. That'll concede. But Dario, let's turn to this now, because one thing since we last spoke in Paris, the geopolitical environment has, if anything, I don't know, complicated, mad, crazy, whatever phrase you want to use. Secondly, the U.S. has a very different approach now towards China. It's a much more, it's a kind of no holds barred, go as fast as we can, but then sell chips to China. And that is, so you've got a different attitude towards the United States. You've got a very strange relationship between the United States and Europe right now geopolitically. Against that, I mean, I hear you talk about it, it would be nice to have a CERN-like organization. I mean, it's a million years from where we are, from the real world. So in the real world, have the geopolitical risks increased? And what, if anything, do you think should be done about that? And the administration seems to be doing the opposite of what you were suggesting. Yeah, I mean, look, you know, we're just trying to do the best we can to, you know, we're just one company and we're trying to operate in, you know, the environment that exists, no matter how crazy it is. But, you know, I think at least my policy recommendations haven't changed. That, you know, not selling chips is one of the, you know, one of the biggest things we can do to, you know, make sure that we have the time to handle this. You know, you know, I said before, you know, I prefer Demis' timeline. I wish we had five to ten years, you know, so it's possible he's just right and I'm just wrong. But assume I'm right and it can be done in one to two years. Why can't we slow down to Demis' timeline? Well, you could just slow down. Well, no, but the reason we can't do that is, you know, because we have geopolitical adversaries building the same technology at a similar pace. It's very hard to have an enforceable agreement where they slow down and we slow down. And so if we can just not sell the chips, then this isn't a question of competition between the U.S. and China. This is a question of competition between me and Demis, which I'm very confident that we can work out. And what do you make of the logic of the administration, which, as I understand it, is we need to sell them chips because we need to bind them into U.S. supply chains. So, you know, it's I think it's I think it's a question not just of timescale, but of the significance of the technology. Right. If this was telecom or something, then all this stuff about proliferating the U.S. stack and, you know, wanting to build our, you know, chips around the world to make sure that, you know, you know, this, you know, these random countries in different parts of the world, you know, build data centers that have NVIDIA chips instead of Huawei chips. You know, I think of this more as like, you know, it's a decision. Are we going to, you know, sell nuclear weapons to North Korea? And, you know, because that produces some profit for Boeing, you know, where we can say, OK, yeah, these cases were made by Boeing. Like the U.S. is winning. Like, this is great. Like I just, you know, that that analogy should just make clear how I see this tradeoff, that I just don't think it makes sense. And we've done a lot of more aggressive stuff, you know, towards towards China, China and other players that I think is much less effective than this this one this one measure. One more area for me, and then I hope we'll have time for a question or two. The other area of potential risk that doomers worry about is a kind of all powerful malign AI. And I think you've both been somewhat skeptical of the Duma approach. But in the last year, we have seen, you know, these models showing themselves to be capable of deception, duplicity. Do you think that do you think differently about that risk now than you did a year ago? And is there something about the way the models are evolving that we should put a little bit more concern on that? Yeah, I mean, you know, since since the beginning of Anthropic, we've kind of thought about this risk. I mean, you know, our research at the beginning of it was very theoretical, right? You know, we pioneered this idea of mechanistic interpretability, which is looking inside the model and trying to understand looking inside its brain, trying to understand why it does what it does. And, you know, as human neuroscientists, which we actually both have background in, try to understand the brain. And I think as time has gone on, we've increasingly documented the, you know, bad behaviors of the models when they emerge and are now working on trying to address them with mechanistic interpretability. So, you know, I think, you know, I've always been concerned about these risks. I've talked to Demis many times. I think he has also been concerned about these risks. I think I have definitely been, and I would guess Demis as well, although I'll let him speak for himself, skeptical of doomerism, which is, you know, we're doomed. There's nothing we can do or this is the most likely outcome. I think this is a risk. This is a risk that if we all work together, we can address, we can learn through science to properly, you know, control and direct these creations that we're building. But if we build them poorly, if we go, you know, if we're all racing and we go so fast that there's no guardrails, then I think there is risk of something going wrong. So I'm going to give you a chance to answer that in the context of a slightly broader question, which is over the past year, have you grown more confident of the upside potential of the technology, science, all of the areas that you have talked about a lot? Or are you more worried about the risks that we've been discussing? Look, Azana, I've been working on this for 20 plus years. So we already knew, look, the reason I've spent my whole career on AI is the upsides of solving basically the ultimate tool for science and understanding the universe around us. I've sort of been obsessed with that since a kid. And building AI is the, you know, should be the ultimate tool for that if we do it in the right way. The risks also we've been thinking about since the start, at least the start of DeepMind 15 years ago. And we kind of sort of foresaw that if you got the upsides, it's a dual purpose technology. So it could be repurposed by, say, bad actors for harmful ends. So we've needed to think about that all the way through. But I'm a big believer in human ingenuity. But the question is having the time and the focus and all the best minds collaborating on it to solve these problems. I'm sure if we had that, we would solve the technical risk problem. It may be we don't have that and then that will introduce risk because we'll be sort of, it'll be fragmented. There'll be different projects and people be racing each other. Then it's much harder to make sure, you know, these systems that we produce will be technically safe. But I feel like that's a very tractable problem if we have the time. If you are able to do it. If we have the time and space. I want to make sure there's one question. Gentlemen, keep it very short because we've got literally two minutes. Thanks for, hello. No, speak. Thanks very much. I'm Philip, co-founder of Star Cloud Building Data Centers in Space. I wanted to ask a slightly philosophical question. The sort of strongest argument for Doomerism to me is the Fermi Paradox, the idea that we don't see intelligent life in our galaxy. I was wondering if you guys have any thoughts. Yeah, I've thought a lot about that. That can't be the reason because we should see all the AIs that have. So just for everyone. The idea is, well, it's sort of unclear why that would happen. Right. So if the reason there's a Fermi Paradox, there are no aliens because they get taken out by their own technology. We should be seeing paper clips coming towards us from some part of the galaxy. And apparently we don't. We don't see any structures. Dyson spheres, nothing. Whether they're AI or natural or sort of biological. So to me, there has to be a different answer to Fermi Paradox. I have my own theories about that. But it's out of scope for the next minute. But, you know, I just feel like that my prediction, my feeling is that we're past the great filter. It was probably multicellular life, if I would have to guess. It was incredibly hard for biology to evolve that. So we're on that. You know, there isn't a comfort of like what's going to happen next. I think it's for us to write as humanity what's going to happen next. This could be a great discussion, but it is out of scope for the next 36 seconds. But what isn't? 15 seconds each. When we meet again, I hope next year, the three of us, which I would love, what will have changed by then? Well, I think the biggest thing to watch is this issue of AI systems building AI systems. How that goes, whether that goes one way or another, that will determine, you know, whether it's a few more years until we get there. Or if we have, you know, if we have wonders and a great emergency in front of us that we have to face. AI systems building AI systems. I agree on that. So we're keeping close touch about that. But also, I think outside of that, I think there are other interesting ideas being researched, like world models, continual learning. These are the things I think they'll need to be cracked. If self-improvement doesn't sort of deliver the goods on its own, then we'll need these other things to work. And then I think things like robotics may have its sort of breakout moment. But maybe on the basis of what you've just said, we should all be hoping that it does take you a little bit longer. And indeed, everybody else to give us a little more time. I would prefer that. I think that would be better for the world. Well, you guys could do something about that. Thank you both very much. Thank you. Thank you. Thank you. This episode of Radio Davos was written, presented, produced and edited by me, Robin Pomeroy and by my colleague Benjamin Larson. Studio production was by Taz Kell-Aha. Radio Davos will be back next week. But for now, thanks to you for listening and goodbye.