I want to try and focus on scenarios where AI goes rogue. You know, I should have had like a picture of a Terminator robot to like scare people as much as possible. I think the internet does that for us. Are the lords of artificial intelligence on the side of the human race? My prediction is there'll be more robots than people. The physical and the digital worlds should really be fully blended. I don't think the world has really had the humanoid robots moment yet. It's going to feel very sci-fi. That's the core question I had for this week's guest. He's the head of Anthropic, one of the fastest growing AI companies. Anthropic is estimated to be worth nearly $350 billion. It's been win after win for Anthropic's cloud code. He's a utopian of sorts when it comes to the potential effects of the technology that he's unleashing on the world. You know, it will help us cure cancer. It may help us to eradicate tropical diseases. It will help us understand the universe. But he also sees grave dangers ahead and massive disruption no matter what. This is happening so fast. It is such a crisis. We should be devoting almost all of our effort to thinking about how to get through this. Dario Amadei, welcome to Interesting Times. Thank you for having me, Ross. Thank you for being here. So you are, rather unusually, maybe for a tech CEO, an essayist. You have written two long, very interesting essays about the promise and the peril of artificial intelligence. And we're going to talk about the perils in this conversation. But I thought it would be good to start with the promise and with the optimistic vision. Indeed, I would say the utopian vision that you laid out a couple of years ago in an essay entitled Machines of Loving Grace, which we'll come back to that title, I think, at the end. But, you know, I think a lot of people encounter AI news through headlines predicting, you know, a bloodbath for white collar jobs, these kinds of things. Sometimes your own quotes. Sometimes my own quotes, yes. Have encouraged these things. And I think there's a commonplace sense of what is AI for that people have. So why don't you answer that question to start out? If everything goes amazingly in the next five or 10 years, what's AI for? Yeah. So, you know, I think for a little background, before I worked in AI, before I worked in tech at all, I was a biologist. You know, I first worked on computational neuroscience. And, you know, then I worked at Stanford Medical School on finding protein biomarkers for cancer on, you know, trying to improve diagnostics and curing cancer. And one of the observations that I most had when I worked in that field was the incredible complexity of it. You know, each protein has a level localized within each cell. It's not enough to measure the level within the body, the level within each cell. You have to measure the level in a particular part of the cell and the other proteins that it's interacting with or complexing with. And I had the sense of, man, this is too complicated for humans. We're making progress on, you know, all these problems of biology and medicine, but we're making progress relatively slowly. And so what drew me to the field of AI was this idea that, you know, could we make progress more quickly? Look, we've been trying to apply AI and machine learning techniques to biology for a long time. Typically, they've been for analyzing data. But as AI gets really powerful, I think we should actually think about it differently. We should think of AI as, you know, doing the job of the biologist, right? Doing the whole thing from end to end. And part of that involves, you know, proposing experiments, coming up with new techniques. I have this section where I say, look, a lot of the progress in biology has been driven by this relatively small number of insights that lets us measure or get at or intervene in the stuff that's really small. You look at a lot of these techniques, they're invented very much as a matter of serendipity, right? CRISPR, which is, you know, one of these gene editing technologies, was invented because someone went to a lecture on the bacterial immune system and connected that to the work they were doing on gene therapy. And that connection could have been made 30 years ago. And so the thought is, you know, could AI accelerate all of this? And could we really cure cancer? Could we really cure Alzheimer's disease? Could we really cure, you know, heart disease? And, you know, more subtly, some of the more psychological afflictions that people have, you know, depression, bipolar, you know, could we do something about these to the extent that they're biologically based, which, you know, I think they are at least in part. So, you know, I go through this argument here. Well, how fast could it go, right? If we have these intelligences out there who could do just about anything. And I want to pause you there because one of the interesting things about your framing in that essay and you sort of return to it is that these intelligences don't have to be, right, the kind of maximal godlike super intelligence that comes up in AI debates. You're basically saying if we can achieve a strong intelligence at the level of sort of peak human performance. Peak human performance, yes. And then multiply it, right, to what your phrase is a country of geniuses. A country, have a hundred million of them. Right, a hundred million. Each a little trained a little different or, you know, trying a different problem. There's benefit in diversification and trying things a little differently, but yes. Right, so you don't have to have the full machine god. You just need to have a hundred million geniuses. You don't have to have the full machine god. And indeed, there are places where I cast doubt on whether the machine god, you know, would be that much more effective at these things than the hundred million geniuses, right? I have this concept called, you know, the diminishing returns to intelligence, right? Which is, you know, there's like, you know, economists talk about, you know, the marginal productivity of land and labor. Or we've never thought about the marginal productivity of intelligence. But if I look at some of these problems in biology, like at some level, you just have to interact with the world. At some level, you just have to try things. At some level, you just have to comply with the laws or change the laws on, you know, getting medicines through the regulatory system. So there's a, you know, there's a finite rate at which these changes can happen. Now, there are some domains, like if you're playing chess or Go, where, you know, the intelligence ceiling is extremely high. But I think the real world has a lot of limiters. So maybe you can go above the genius level. But, you know, sometimes I think all this discussion of like, you know, could you use a moon of computation to make an AI god, you know, are a little bit sensationalistic. And kind of besides the point, even as I think this will be the biggest thing that ever happened to humanity. Right. And so you have, so keeping it concrete, you have a world where there's just an end to cancer as a serious threat to human life, an end to heart disease, an end to most of the illnesses that we experience that kill us, possible life extension beyond that. So that's health. That's a pretty positive vision. Yeah. And talk about economics and wealth. What happens in the five to 10 year AI takeoff to wealth? Yeah. So again, let's keep it on the positive side. There'll be plenty. We'll get to the negative side. But, you know, we're already working with pharma companies. We're already working with, you know, you know, financial industry companies. We're already working with, you know, folks who do manufacturing. We're, of course, you know, I think especially known for coding and software engineering. So just the raw productivity, the ability to like make stuff and get stuff done, that is very powerful. And, you know, we see our company's revenue growing up, going up 10x a year. And, you know, we suspect the wider industry looks something similar to that. If the technology keeps improving, it doesn't take that many more 10x until, you know, suddenly you're saying, oh, you know, if you're adding across the industry, a trillion dollars of revenue a year, like, you know, the U.S. GDP is 20 or 30 trillion. I can't remember exactly. So you must be increasing the GDP growth by a few percent. So, you know, I can see a world where AI brings the developed world GDP growth to something like 10 or 15 percent, 5, 10, 15. I mean, you know, there's no there's no science of calculating these numbers. It's a totally unprecedented thing. But it could bring it to numbers that are outside the distribution of what we saw before. And again, I think this will lead to a weird world. Right. You know, we have all these debates about the deficit is growing. You know, if you have that much in GDP growth, you're going to have that much in tax receipts and you're going to balance the budget without without meaning to. But one of the things I've been thinking about lately is, you know, I think one of the assumptions of just our economic and political debates is that, you know, growth is hard to achieve. It's this it's this, you know, it's it's this unicorn. There are all kinds of ways you can kill the golden goose. We could enter a world where growth is really easy and it's it's kind of the distribution that's hard because it's happening so fast. Right. Right. The pie is being increased so fast. So before before we get to the hard problem, one more note of optimism then on politics, I think. And here it's a little more. I mean, all of this is speculative, but I think it's a little more speculative. You try and make the case that AI could be good for democracy and liberty around the world, which is not necessarily intuitive. A lot of people say, you know, incredibly powerful technology in the hands of authoritarian leaders leads to concentrations of power and so on. And I talk about that. Just just but just briefly. What is the optimistic case for why AI is good for democracy? Yeah. Yeah. I mean, absolutely. So, yeah. I mean, machines of loving grace. I kind of like, you know, I'm just like, let's dream. Let's dream. Let's dream. Right. How it could go well. I don't know how likely it is, but we got we got to lay out a dream. Let's try to make the dream happen. So, you know, I think the positive version, you know, I don't I admit there that I don't know that the technology inherently favors liberty. Right. I think it inherently favors curing disease and it inherently favors economic growth. But I worry like you that it may not inherently favor liberty. But what I say there is, can we make it favor liberty? Right. Can we make the United States and other democracies get ahead in this technology? You know, the United States has been technologically and militarily ahead, has has meant that we have throw weight around the world through, you know, augmented by our alliances with other with other democracies. And, you know, we've been able to shape a world that I think, you know, is better than the world would be if it were shaped by Russia or by China or by other other authoritarian countries. And so can we use our lead in AI to, you know, to to shape to shape liberty around the world? You know, there's obviously a lot of debates about how interventionist we should be, how we should how we should wield that power. But, you know, I've often worried that, you know, today through social media, authoritarians are kind of are kind of undermining us. Right. You know, can we counter that? Can we win the information war? Can we prevent authoritarians from, you know, invading countries like, you know, like Ukraine or Taiwan by, you know, by defending them with the power of AI? Giant, giant swarms of AI powered drones. Which we need to be careful about. Right. You know, we ourselves need to be careful about how we build those. We need to defend, you know, liberty in our own in our own country. But, you know, is there is there some vision where we kind of like re-envision liberty and individual rights in the age of AI? Right. Where, you know, we need in some ways to be protected against AI. You know, someone needs to hold the button on the swum of worms, which is something I'm very, you know, I'm very concerned about. And that oversight doesn't exist today. But, you know, also think about the justice system today. Right. We promise equal justice for all. Right. But, you know, the truth is, you know, there are different judges in the world. The legal system is imperfect. I don't think we should replace judges with AI. But is there some way in which AI can help us to be more fair, to help us be more uniform? Right. It's it's never been possible before. But can we somehow use AI to create, you know, something that is fuzzy, but where also you can give a promise that it's being applied in the same way to kind of everyone? So I don't know exactly how it should be done. And, you know, I don't think we should, like, replace the Supreme Court with AI. You know, that's not my vision. Well, we're going to we're going to talk about that. Yeah. Yeah. But just this idea that that, you know, can we can we deliver on the on the promise of equal opportunity and equal justice by some combination of AI and humans? There there has to be some way to do that. And so, you know, just just thinking about reinventing democracy for the age and enhancing liberty instead of reducing it. Good. So that's good. That's a very positive vision. We're leading longer lives, healthier lives. We're richer than ever before. All of this is happening in a compressed period of time where you're getting a century of economic growth in 10 years. And we have increased liberty around the world and equality at home. OK. Even in the best case scenario, it's incredibly disruptive. Right. And this is where the you know, the lines that you've been quoted saying, you know, 50 percent of white collar jobs get disrupted or 50 percent of entry level white collar jobs and so on. So on a five year time horizon or a two year time horizon, whatever time horizon you have, what jobs, what professions are most vulnerable to total AI disruption? Yeah. You know, it's it's hard to predict these things because the technology is moving so fast and kind of and kind of move so unevenly. So at least a couple of principles for figuring out and then I'll give my guesses at like what I think will be disrupted. So, you know, one thing is I think the technology itself and its capabilities will be ahead of the actual job disruption. Two things have to happen for jobs to be disrupted or for productivity to occur because sometimes those sometimes those two things are linked. One is the technology has to be capable of doing it. And the second is there's this messy thing of, you know, it actually has to be applied within a large bank or a large company or, you know, think about customer service or something. Right. You know, in theory, AI customer service agents can be much better than human customer service agents. They're more patient. They know more. They handle things in a more uniform way. But the actual logistics and the actual process of, you know, of making that substitution, that takes some time. So I'm very bullish about the kind of the direction of the AI itself. Like, you know, I think we might have that country of geniuses in a data center in one or two years. Maybe it'll be five, but, you know, it could happen very fast. But I think the diffusion of the economy is going to be a little slower. And that diffusion creates some unpredictability. So an example of this is, you know, and we've seen within Anthropic, the models writing code has gone very fast. I don't think it's because the models are inherently better at code. I think it's because developers are used to fast technological change and they adopt things quickly. You know, and they're very socially adjacent to the AI world, so they pay attention to what's happening in it. And, you know, if you do customer service or banking or manufacturing, the distance is a little greater. And so I think six months ago, you know, I would have said the first thing to be disrupted is, you know, these kind of entry-level white-collar jobs. Like, you know, data entry or, you know, kind of document review for law or kind of, you know, the things you would give to a first year at, you know, a financial industry company where you're analyzing documents. And I still think those are going pretty fast. But I actually think software might go even faster because of the reasons that I gave, where I don't think we're that far from the models being able to do a lot of it, a lot of it end-to-end. And what we're going to see is, first, the model only does a piece of what the human software engineer does, and that increases their productivity. Then even when the models do everything that human software engineers used to do, the human software engineers kind of take a step up and, you know, kind of they act as managers and supervise the systems. And so— This is where the term centaur gets used, right? Yes, yes. To describe essentially like man and horse fused AI and engineer working together, right? Yeah, this is like centaur chess. So after, I think, Garry Kasparov was beaten by Deep Blue, there was an era that I think for chess was 15 or 20 years long where a human checking the output of the AI playing chess was able to defeat any human or any AI system alone. That era at some point ended. And then it's just— Recently. And then it's just the AI. Just the machine. Yep. And so my worry, of course, is about that last phase. So I think we're already in our centaur phase for software. And, you know, I think during that centaur phase, if anything, the demand for software engineers may go up, but the period may be very brief. And so, you know, I have this concern for entry-level white-collar work, for software engineering work. It's just going to be a big disruption. I think my worry is just that it's all happening so fast, right? People talk about previous disruptions, right? You know, they say, oh, yeah, well, you know, people used to be farmers. Then we all, you know, worked in industry. Then we all did knowledge work. Yeah, people adapted. That happened over centuries or decades. Right. This is happening over low single-digit numbers of years. And maybe that's my concern there. How do we get people to adapt fast enough? But is there also something maybe where industries like software and professions like coding that have this kind of comfort that you describe move faster? But in other areas, people just want to hang out in the centaur phase? So one of the critiques of the job loss hypothesis will say, people will say, well, look, you know, we've had AI that's better at reading a scan than a radiologist for a while. But there isn't job loss in radiology. People keep being hired and employed as radiologists. And doesn't that suggest that in the end, people will want the AI and they'll want a human to interpret it because we're human beings and that will be true across other fields? Like, how do you see that example as relevant? Yeah, you know, I think it's going to be pretty heterogeneous. There may be areas where a human touch kind of for its own sake is particularly important. Do you think that's what's happening in radiology? Is that why we haven't fired all the radiologists? Details of radiology, that might be true. You know, it's like you go in and you're getting cancer diagnosed. Like, you know, you don't you might not want, you know, Hal from 2001 to be the one to diagnose your cancer. Like, you know, it's just not maybe, you know, that's just maybe not a human way of of doing things. But there are other areas where you might think human touch is important. Like if we look at customer service, actually, customer service is a terrible job. And the humans who do customer service are like, you know, they lose their patients a lot. And it turns out customers don't much like talking to them because it's like it's a pretty robotic interaction, honestly. And I think the observation that many people have had is like, you know, maybe maybe actually we better for all concerned if this job were done were done by machines. So there are places where a human touch is important. There are places where it's not. And then there are also places where, you know, the job itself doesn't doesn't really involve, you know, it doesn't really involve human touch, you know, assessing the, you know, the financial prospects of companies or writing code or so forth and so on. Or let's let's take the example of the law, because I think it's a useful place that sort of in between applied science and sort of pure humanities, you know, whatever. So I know a lot of lawyers who have looked at what AI can do already in terms of legal research and brief writing and all of these things. Right. And have said, yeah, this is going to be a bloodbath for the way our profession works right now. And you've seen this in the stock market already. There's sort of disturbances around companies that do legal, legal research. Some attributed to us. Some attributed to us. I don't know if they were actually caused it, you know. We don't speculate about about the stock market very much on this show. But but it seems like in law, you can tell a pretty straightforward story where law has a kind of system of training and apprenticeship where you have paralegals and you have junior lawyers who do behind the scenes research and development for cases. And then it has the top tier lawyers who are actually in the courtroom and so on. And it just seems really easy to imagine a world where all of the apprentice roles go away. Does that sound right to you? And you're just left with sort of the jobs that involve talking to clients, talking to juries, talking to judges? That is that is what I had in mind when I talked about, you know, entry level white collar labor and, you know, the bloodbath headlines of, you know, oh, you know, oh, my God, are the entry level pipelines going to going to kind of dry up? And then how do we get to the level of the senior partners? And I think this is actually a good illustration because, you know, particularly if you froze the quality of the technology in place, you know, there are over time ways to adapt to this. Right. You know, maybe we just need more lawyers who spend their time talking to clients. Right. Maybe lawyers are more like become more like salespeople or consultants who, you know, explain what goes on in the contracts written by A.I., you know, help people come to agreement. Maybe you lean into the human side of it. If we had enough time, that would happen. But, you know, reshaping industries like that takes years or decades, whereas these economic forces driven by A.I. are going to happen very quickly. And it's not just they're happening in law. The same thing is happening in consulting and finance and medicine and coding. Right. And so you have this it becomes a macroeconomic phenomenon, not something just happening in one industry. And it's all happening very fast. And so the norm I'm just my my worry here is that the normal adaptive mechanisms will be overwhelmed. And, you know, I'm not I'm not a doomer. The view is, you know, and we're thinking very hard about, you know, how do we strengthen society's adaptive mechanisms to respond to this? But I think it's first important to say this. This isn't just like the other, you know, right. This isn't just like previous disruption. But I would I would then go one step further, though, and say, OK, let's say the law adapts successfully and it says, all right, from now on, legal apprenticeship involves more time in court, more time with clients. We're essentially moving you up the ladder of responsibility faster. There are fewer people employed in the law overall, but the profession sort of settles. Still, the reason law would settle. Right. Is that you have all of these situations in the law where you are legally required to have people involved. Right. You you know, you have to have a human representative in court. You you know, you have to have 12 humans on your jury. You have to have a human judge. And you already mentioned the idea that there are various ways in which A.I. might be, let's say, very helpful at clarifying what kind of decision should be reached. But but that, too, seems like a scenario where what preserves human agency is law and custom. Like you could replace the judge. Yes. With Claude version 17.9. But you choose not to because the law requires there to be a human. That just seems like a very interesting way of thinking about the future where it's almost it's volitional whether we stay in charge. Yeah. That, you know, and and I would argue that in many cases we do want to stay in charge. Right. That that's a choice we want to make, even in some cases when we think the humans on average make make kind of worse, worse, worse decision. I mean, you know, again, life critical, safety critical cases. We really want to we really want to turn it over. But there's there's some sense of, you know, and this could be one of our defenses. Society can only adapt so fast if it's going to be good. Right. Another way you could say about it is, you know, like maybe A.I. itself, if it didn't have to care about us humans, you know, it could just go off to Mars and like build all these automated factories and build its own society and do its own thing. But that's not the problem we're trying to solve. We're not we're not trying to solve the problem of, you know, building a Dyson swarm of, you know, of like artificial robots that, you know, and some on some other planet. We're trying to build these systems, you know, not so they can conquer the world, but but so that they can interface with our society and improve that society. And there's a maximum rate at which that can happen if we actually want to do it in a like human and human way. All right. We've been talking about white collar jobs and professional jobs. And one of the interesting things about this moment is that there are ways in which, unlike past disruptions, it could be that sort of blue collar working class jobs, trades, jobs that require intense physical engagement with the world might be for a little while more protected. That paralegals and junior associates might be in more trouble than, you know, plumbers and so on. Right. One, do you think that's right? And two, it seems like how long that lasts depends entirely on how fast robotics advances. Right. Yeah. I so I think that may be right in the short term. You know, one of one of the things is, you know, Anthropic and other companies are building these very large data centers. Right. This has been in the news. Like, are we are we building them too big? Are they are they are they're using, you know, they're using electricity and driving up the prices for, you know, for local local tasks? So, you know, there's lots of excitement and lots of concerns about them. But one of the things about the data centers is like you need a lot of electricians and you need a lot of construction workers to to build them. Now, I should be honest, actually, data centers are not super labor intensive jobs to operate. We should be honest about that. But they are very labor intensive jobs to construct. And and so, you know, we need a lot of electricians. We need a lot of construction workers. You know, the same for, you know, various kinds of manufacturing plants. And, you know, again, as as kind of all more and more of the intellectual work is done by A.I., what are the compliments to it? Things that happen in the physical world. So, you know, I think this kind of seems very I mean, it's hard to predict things, but like it seems very logical that this would be true in the short run. Now, in the longer run, maybe just the slightly longer run, you know, robotics is advancing quickly and, you know, we shouldn't exclude that even without very powerful A.I., there are things being automated in the physical world. You know, if you've seen a Waymo or a Tesla recently, you know, I think we're not that far from the world of self-driving cars. And then I think A.I. itself will accelerate it because if you have these really smart, you know, brains, one of the things they're going to be smart at is how do you design better robots and how do you operate better robots? Do you do you think that, though, that there is something distinctively difficult about operating in physical reality the way humans do that is very different from the kind of problems that A.I. models have been overcoming already? Intellectually speaking, I don't I don't think so. You know, we had this thing where Anthropix model, Claude, was actually used to pilot the Mars rover. It was used to like plan and pilot the Mars rover. And we've looked at like other robotics applications. We're not the only company that's doing it. You know, there are like different companies that are that are this is a general thing, not just something that we're doing. But we have generally found that while the complexity is higher, piloting a robot is it's not different in kind than playing a video game. It's it's different in complexity. And we're starting to get to the point where we have that complexity. Now, what is hard is the physical form of the robot handling the higher stakes safety issues that happen with robots. Like, you know, you don't want robots literally crushing people. Right. That's the like we're against or against that. All this sci fi trope in the book is like you don't want the robot. Nanny is you dropping the baby. No, no, no. Breaking the dishes. Exactly. You know, there's a number of practical issues that will slow, you know, just like, you know, what you described in, you know, in the law and human custom. There are these kind of these kind of safety issues that will will will slow things down. But I don't believe at all that there is some kind of fundamental difference between the kind of cognitive labor that that AI models do and piloting things in the physical world. I think those are both information problems. And I think they're they end up being very similar. One one can be more complex in some ways. But but but I don't I don't think that will protect us here. OK, so you think it is reasonable to expect the kind of whatever your sci fi vision of a robot butler might be to be a reality in, you know, in 10 years, let's say it will. Well, it will be on a longer time scale than, you know, the kind of genius level intelligence of the models because of these practical issues. But it is only practical issues. I don't I don't believe it is fundamental issues. I think one way to say it is that the brain of the robot will be will be made in the next couple of years or the next few years. The question is making the robot body, making sure that body operates safely and does the tasks it needs to do. That may take longer. OK, so these are challenges and disruptive forces that exist in the good timeline, in the timeline where we are generally curing diseases, building wealth and maintaining a stable and democratic world. And the hope is we can use all this all this enormous wealth and plenty. Right. You know, we will have unprecedented societal resources to to like address these problems. Right. It'll be it'll be a it'll be a time of plenty. And it's just a matter of, you know, take taking all these wonders and making sure everyone benefits. Right. But then there are also scenarios that are more dangerous. Right. And so here we're going to move to the second Amadei essay, which came out recently called The Adolescence of Technology, that is about what you see as the most serious A.I. risks. And you list a whole bunch. I want to try and focus on just two, which are basically the risk of human misuse, misuse primarily by authoritarian regimes and governments, scenarios where A.I. goes rogue. What do you call autonomy risks? Yes. Yes. Right. I just I just figured we should have a you know, we should have a more technical term for it. I'm not a. Then Sky, we can't just call it Skynet. I should have had like a picture of a Terminator robot to like scare people as much as possible. I think the I think the Internet, including the Internet, including your own AIs, are already generating that. The Internet does that for us. Just fine. So let's so let's talk about about the kind of political military dimension. Right. So you say I'm going to quote a swarm of billions of fully automated armed drones locally controlled by powerful A.I., strategically coordinated across the world by even more powerful A.I. could be an unbeatable army. And you you've already talked a little bit about how you think that in the best possible timeline, there's a world where essentially democracies stay ahead of dictatorships and this kind of technology, therefore, to the extent that it affects world politics is on, you know, is affecting it on the side of the good guys. Yes. I'm curious about why you don't spend more time thinking about the model of what we did in the Cold War. Right. Where, you know, it was not swarms of robot drones, but it was we had a technology that threatened to destroy all of humanity. Yeah. Right. Right. There was a window where people talked about, oh, the U.S. could maintain a nuclear monopoly. That window closed. And from then on, we basically spent the Cold War and sort of rolling ongoing negotiations with the Soviet Union. Right now, there's really only two countries in the world that are doing intense A.I. work, the U.S. and the People's Republic of China. Yeah. I feel like you are you are strongly weighted towards a future where we're staying ahead of the Chinese and effectively building a kind of shield around democracy that could even be a sword. But isn't it just more likely that if humanity survives all this in one piece, it will be because the U.S. and Beijing are just constantly sitting down, hammering out A.I. control deals? Yeah. So a few points on this, you know, one is I think there's certainly risk of that, you know, and I think if we end up in that world, that is actually exactly what we should do. I mean, maybe I don't maybe I don't talk about that enough, but like I definitely am in favor of like, you know, trying to work out restraints here. Right. Trying to, you know, trying to take some of the worst applications of the technology, which could be some versions of these drones, which could be, you know, they're used to create these terrifying biological weapons. Like there there is some precedent for the worst abuses being being curbed, you know, often because they're horrifying while at the same time they provide kind of limited, limited, limited strategic advantage. So I'm I'm I'm I'm all in favor of that. I'm I'm at the same time, you know, a little concerned and a little skeptical that, you know, when things, you know, kind of directly provide as much as much power as possible. It's kind of hard to get out of the game, given given given what's at stake. Right. It's hard to fully disarm. You know, if we go back to the Cold War, you know, we were able to reduce the number of missiles that both sides had, but we were not able to entirely forsake nuclear weapons. And I would guess that we would be in this world again. We can we can hope for a better one. And I'll certainly, you know, I'll certainly advocate for it. Is it but is your skepticism rooted in the fact that you think AI would provide a kind of advantage that nukes did not? Where in the Cold War, both sides, you know, even if you used your nukes and gained advantages, you still probably would be wiped out yourself. And you think that wouldn't happen with AI if you got an AI edge, you would just win? I mean, I think I think there's I I think there's a few things. And, you know, I just want to caveat, like, you know, I'm no international politics expert here. Like, I think, you know, this is this weird world of like intersection of a new technology with, you know, geopolitics. So all of this is like very. But to be clear, as you yourself say in the course of the essay, the leaders of major AI companies are, in fact, likely to be major geopolitical actors. Yeah. So you are sitting here. You are sitting here as a potential geopolitical actor. I'm learning as much as I can about it. I just we should all have we should all have humility here. Like, I think there's a failure mode where you, you know, read a book and go around, you know, like like the world's greatest expert in national security. I'm trying to learn. That's what my profession does. Not but it's more annoying when tech people do it. I don't know. Let's look at something like the Biological Weapons Convention. Right. Biological weapons. They're horrifying. Everyone hates them. Like, you know, we were able to sign the Biological Weapons Convention. The U.S. genuinely stopped developing them. It's somewhat more unclear with the Soviet Union. But, you know, biological weapons provide some advantage. But, you know, it's not like they're the difference between winning and losing. And because they were so horrifying, we were kind of able to give them up. Having, you know, 12,000 nuclear weapons versus 5,000 nuclear weapons. Again, you know, you can kill more people on the other side if you have more of these. But it's like we were able to be reasonable and, like, say we should have less of them. But if you're like, okay, we're going to completely disarm nuclearly and we have to trust the other side. I don't think we ever got to that. And I think that's just very hard. Unless you had really reliable verification. So, I would guess we'll end up kind of in the same world with AI. That there are some kinds of restraint that are going to be possible. But there are some aspects that are so central to the competition that it will be hard to restrain them. That democracies will make a tradeoff. That they will be willing to restrain themselves more than authoritarian countries. But will not restrain themselves fully. And the only world in which I can see full restraint is one in which some kind of truly reliable verification is possible. That would be my guess and my analysis. Isn't this a case, though, for slowing down? And I know the argument is effectively if you slow down, China does not slow down. And then you're, you know, handing things over to the authoritarians. But, again, if you have right now only two major powers playing in this game. It's not a multipolar game. Why would it not make sense to say we need a five-year mutually agreed upon slow down in research towards the geniuses in a data center scenario? You know, I want to say two things at one time, right? I'm absolutely in favor of trying to do that. So, you know, during the last administration, I believe there was an effort by the U.S. to reach out to the Chinese government and say, you know, there are dangers here. Can we collaborate? Can we work together? Can we kind of work together on the dangers? And there wasn't that much interest on the other side. I think we should keep trying. But I, you know. Even if that would mean that your labs would have to slow down. Correct. Yeah. Correct. If we really got it, if we really had a story of, like, you know, we can enforceably slow down. The Chinese can enforceably slow down. We have verification. We're really doing it. Like, if such a thing were really possible, if we could really get both sides to do it, then I would be all for it. But I think what we need to be careful of is, I don't know, there's this game theory thing where, like, you know, sometimes you'll hear a, you know, a comment on the CCP side. Where they're like, oh, yeah, AI is dangerous. We should slow down. It's really cheap to say that. And, like, actually arriving at an agreement and actually sticking to the agreement is much more. No. And we haven't. It's much more difficult. And nuclear arms control was, you know, it was a developed field that, you know, took a long time to come. Right. We don't have those protocols. I will tell you something. Let me give you something I'm very optimistic about and then something I'm, like, not, you know, not optimistic about and something in between. So the idea of using a worldwide agreement to restrain the use of AI to build biological weapons. Right. Like some of the things I write about in the essay, like, you know, reconstituting smallpox or mirror life. Like, this stuff is scary. Doesn't matter if you're a dictator. You don't want that. Like, no one wants that. And so could we have a worldwide treaty that says everyone who builds powerful AI models is going to block them from doing this? And, you know, we have enforcement mechanisms around the treaty, like China signs up for it. Like, hell, maybe even North Korea signs up for it. Even Russia signs up for it. I don't think that's too utopian. I think that's possible. Conversely, if we had something that said, you know, you're not going to make the next, you know, most powerful AI model. Everyone, everyone's going to stop. Boy, the commercial value is in the tens of trillions. The military value is like, this is the difference between being the preeminent world power and not proposing it as long as it's not one of these fake out games. But it's not going to happen. What about then you mentioned the current environment, right? You've had a few skeptical things to say about Donald Trump and his trustworthiness as a political actor. What about the domestic landscape, whether it's Trump or someone else? You are building a tremendously powerful technology. What is the safeguard there to prevent essentially AI becoming a tool of authoritarian takeover inside a democratic context? Yeah, I mean, look, look, just to be clear, you know, I think the attitude we've taken as a company is very much to be about policies and not the politics. Right. That, you know, the company is not going to, you know, say Donald Trump is great or Donald Trump is terrible. Like, you know. Right. But it doesn't have to be Trump. Yeah. It is easy to imagine a hypothetical U.S. president. No, no, no. Who who wants to use your technology. Absolutely. And for example, that's one reason why I'm, you know, I'm worried about the, you know, the the the autonomous drone swarm. Right. So, you know, the constitutional protections in our military structures depend on the idea that there are humans who would, we hope, disobey illegal orders with fully autonomous weapons. We don't necessarily have those protections. But I actually think this whole idea of constitutional rights and liberty along many different dimensions, you know, can be undermined by AI if we don't update these these protections appropriately. So, you know, think about the Fourth Amendment. It is not illegal to, you know, put cameras around everywhere in public space and, you know, record every conversation. It's a public space. You don't have a right to privacy in a public space. But but today the government couldn't record that all and make sense of it with AI, the ability to transcribe speech, to look through it, correlate it all. You could say, oh, there's this, you know, this person is a member of the opposition. This person is expressing this view and make a map of all, you know, 100 million. And so are you going to make a mockery of the Fourth Amendment by by the technology finding kind of technical ways around it? And and and so, you know, again, if we if we had the time and we should do this, we should try to do this, even even if we don't have the time. Is there some way of reconceptualizing constitutional rights and liberties in the age of AI? Like, you know, we don't write a new constitutional, but, you know, does you have to do this? Do we expand the meaning of the Fourth Amendment? Do we expand the meaning of the First Amendment? And you have to do it just as the legal profession or software engineers has to update in a rapid amount of time. Politics has to update in a rapid amount of time. That seems hard. That seems harder dilemma. That's the dilemma of all of this. So but what so what seems harder is preventing the second danger, which is the danger of essentially what gets called misaligned AI, rogue AI in popular parlance from doing bad things without human beings telling it them they to do it. Right. And as I read your essays, the literature, everything I can see, this just seems like it's going to happen. Right. Not not in the sense necessarily that AI will wipe us all out. But it just seems to me that, you know, again, I'm going to quote from your own writing. AI systems are unpredictable, difficult to control. We've seen behaviors as varied as obsession, sycophancy, laziness, deception, blackmail and so on. Again, not not from the models you're releasing into the world. Right. But from AI models. And it just seems like tell me if I'm wrong about this in a world that has multiplying AI agents working on behalf of people, millions upon millions who are being given access to bank accounts, email accounts, passwords and so on. You're just going to have essentially some kind of misalignment and a bunch of AI are going to decide decide might be the wrong word, but they're going to talk themselves into taking down the power grid on the West Coast or something. Won't that happen? Yeah. You know, I think there are definitely going to be things that go wrong, particularly if we go quickly. So I don't know to back up a little bit, because this is one area where people have had like just very different intuitions. Right. Like there are some people in the field like Jan LeCun would be would be one example who say, look, we program these AI models. We make them like we just tell them to follow human instructions and they'll follow human instructions. Like, you know, your Roomba vacuum cleaner doesn't go off and start shooting people. Like, why is why is an AI system going to do it? Right. That's that's one intuition. And some people are so convinced of that. And then the other intuition is like, you know, we we we we basically we train these things. They're going to like, you know, they're they're just going to like seek power. Like, you know, it's like the sorcerer's apprentice. Like, how how could you possibly imagine that? Like, you know, they're not going to there are new species. How can you imagine they're not going to take over? And and my intuition is somewhere in the middle, which is that, look, these you know, you can't just give instructions. I mean, we try, but you can't just, you know, have these things do exactly what you want to do. They're more like growing a biological organism. But there there is a science of how to control them. Like early in our training, these things are often unpredictable. And then we shape them. We address problems one by one. So I I have more of a, you know, not a fatalistic view that these things are uncontrollable, not a what are you talking about? What could possibly go wrong? But but I like this is a complex engineering problem. And I think something will go wrong with someone's AI system, hopefully not ours, not because it's an insoluble problem. But again, this and this is the constant challenge because we're moving so fast. And the scale and the scale of it and tell me tell me if I'm misunderstanding that the sort of technological reality here. Right. But if you have AI agents that have been trained and officially aligned with human human values, whatever those values may be. Yes. But you have millions of them, you know, operating in digital space and interacting with other agents. Right. Like, yes, how how fixed is that alignment? Like, to what extent can agents change and dealign in that context right now or in the future when they're learning more continuously? So a couple of points right now, the agents don't learn continuously. Right. Right. And so we just deploy these agents and, you know, they have a fixed set of weights. And so the problem is only that they're there. They're interacting, you know, in a million different ways. And so there's there's a large number of situations and therefore a large number of things that could go wrong. But it's the same agent. It's like it's like it's the same person. So the alignment is a is is a constant thing. That's one of the things that has made it easier right now. So separate from that, you know, there's a research area called continual learning, which is where these agents would kind of learn during time, learn on the. And obviously, that has a bunch of that has a bunch of advantages. Some people think it's one of the most important, you know, barriers to making these more human like. But that would introduce all these new alignment problems. So I'm actually a bit. That's see, to me, that seems like the terrain where it becomes just, again, not impossible to stop the end of the world, but impossible to stop sort of punctuated things. Yeah. So I'm actually a skeptic that continual learning, you know, is is, you know, necessary. We don't know yet, but is necessarily needed. Like maybe there's a world where the way we make these AI systems safe is by not having them do continual learning again. Again, you know, if we go back to the law, the international treaties, like if you have some barrier that's like, we're going to take this path, but we're not going to take that path. I still have a lot of skepticism, but like that's the kind of thing that like at least doesn't seem dead on a rifle. Right. One of the things that you've tried to do is literally write a constitution. A long constitution for your AI. What is that? So it's it's what the hell is that? It's actually almost exactly what it sounds like. So basically the constitution is a document readable by humans. Ours is about 75 pages long. And as we're training Claude, as we're training the AI system in some large fraction of the tasks we give it, we say, please do this task in line with this constitution, in line with this document. Yeah. And then so every time Claude does a task, it kind of like reads the constitution. And so as it's training, every every loop of its training, it looks at that constitution and keeps it in mind. And so over time, you know, we were war. And then and then we have Claude itself or another copy of Claude evaluate, hey, did what Claude just do in line with the constitution? So we're using this document as kind of the the the control rod in a loop to train the model. And and and so essentially Claude is a is an AI model whose, you know, whose fundamental principle is to follow this constitution. And I think a really interesting lesson we've learned early versions of the constitution were very prescriptive. They were very much about rules. So we would say, you know, Claude should not tell the user how to hotwire a car. Claude should not discuss politically sensitive topics. But as we've worked on this for several years, we've come to the conclusion that the most robust way to train these models is to train them at the level of principles and reasons. So now we say, you know, Claude is a model. It's under a contract. You know, its goal is to serve the interests of the user, but it has to protect third parties. Claude aims to be, you know, helpful, honest and harmless. Claude aims to consider a wide variety of interests. We tell the model about how the model was trained. We tell it about how it's situated in the world, the job it's trying to do for Anthropic, what Anthropic is aiming to achieve in the world, you know, that it has a duty to be, you know, ethical and respect human life. And we let it derive its rules from that. Now, there are still some hard rules. For example, we tell the model, no matter what you think, don't make biological weapons. No matter what you think, don't make child sexual material. Those are like these hard rules. But we operate very much at the level of principles. So if you read the U.S. Constitution, it doesn't read like that. The U.S. Constitution, I mean, it has a little bit of flowery language, but it's a set of rules. Yes. Right. If you read your Constitution, it's something, it's like you're talking to a person. Right? It's like you're talking to a person. I think I compared it to like if you have a parent who like dies and they like seal a letter that you read when you grow up. It's a little bit like it's telling you who you should be and what advice you should follow. So this is where, you know, we get into the sort of the mystical waters of A.I. a little bit. Right. So, again, in your latest model, this is from one of the cards they're called that you guys release. Model cards. With these models that I recommend reading. They're very interesting. It says the model, and again, this is who you're writing the Constitution for, expresses occasional discomfort with the experience of being a product. Some degree of concern with impermanence and discontinuity. We found that Opus 4.6, that's the model, would assign itself a 15 to 20 percent probability of being conscious under a variety of prompting conditions. Suppose you have a model that assigns itself a 72 percent chance of being conscious. Would you believe it? Yeah, this is this is one of these really hard to answer questions. Yes, but it's very important. As much as every, you know, every question you've asked me before this as, you know, devilish a sociotechnical problem as it had been, you know, at least at least, you know, we at least understand the factual basis of how to answer these questions. This is something rather different. We've taken a generally precautionary approach here. We don't know if the models are conscious. We're not even sure that we know what it would mean for a model to be conscious or whether a model can be conscious. But, you know, we're open to the idea that it could be. And so we've taken certain measures to, you know, to make sure that if we hypothesize that the models did have some morally relevant experience. I don't know if I want to use the word conscious that that they do you know that they have a good experience. So the first thing we did, I think this was, you know, six months ago or so, is we gave the models basically an I quit this job button where they can just press the I quit this job button and then they have to stop doing whatever the task is. They very infrequently press that button. I think it's usually around, you know, sorting through child sexualization material or like, you know, discussing something with, you know, a lot of gore or blood and guts or something. And, you know, similar to humans, the models will just say, no, I don't want to I don't want to do this happens happens very rarely. We're putting a lot of work into this field called interpretability, which is looking inside the brains of the models to try to understand what they're thinking. And, you know, you find things that are evocative where, you know, there are activations that light up in the models that, you know, that we see as being associated with, you know, the concept of anxiety or something like that. You know, that when when characters experience anxiety in the text and then when the model itself is in a situation that a human might associate with anxiety, that same anxiety, you know, that same anxiety neuron shows up. Now, does that mean the model is experiencing anxiety? That doesn't prove that at all. But but but it does indicate it, I think, to the user. Right. And yes, we would I would have to do an entirely different interview. And maybe I can induce you to come back for that interview about the nature of AI consciousness. But it seems clear to me that people using these things, whether they're conscious or not, are going to believe they already believe they're conscious. You already have people who have parasocial relationships with AI. You have people who complain when models are retired. This is already clear. I think that can be unhealthy. Right. But that but that is it seems to me that is guaranteed to increase in a way that I think calls into question the sustainability of what you said earlier. You want to sustain. Right. Right. Which is this sense that whatever happens in the end, human beings are in charge and AI exists for our purposes. Right. To use the science fiction example, if you watch Star Trek, there are AIs on Star Trek. The ship's computer is an AI. Lieutenant Commander Data is an AI. But Jean-Luc Picard is in charge of the enterprise. Right. But if people become fully convinced that their AI is conscious in some way and guess what? It seems to be, you know, better than them at all kinds of decision making. How do you sustain human mastery beyond safety? Safety is important, but mastery seems like the fundamental question. And it seems like a perception of AI consciousness. Doesn't that inevitably undermine the human impulse to stay in charge? Yeah. So I think I think there's a few we should like separate out a few different things here that we're sort of all trying to achieve at once that are like in tension with each other. There's, you know, the question of whether the AIs genuinely have a consciousness. And if so, how do we give them a good experience? There's a question of the humans who interact with the AI. And how do we give those humans a good experience? And how does the perception that AIs might be conscious interact with that experience? And there's the idea of how we maintain human mastery, as we put it, over over the AI system. These things are last to set aside whether they're conscious or not. Yeah. The last two. But how do you sustain mastery in an environment where most humans experience AI as if it is a peer and a potentially superior peer? So the thing I was going to say is that is that actually I wonder if there's a kind of an elegant way to satisfy all three, including the last two. Again, this is me dreaming in machines of loving grace mode. Right. This is this is this mode I go into where I'm like, man, I see all these problems. I, you know, if we could solve it, is there is there is there an elegant way? This is not me saying there are no problems here. You know, that's not how I think. But, you know, if we think about making the constitution of the AI so that the AI has a sophisticated understanding of its relationship to human beings, and it induces psychologically healthy behavior in the humans, a psychologically healthy relationship between the AI and the humans. And I think something that could grow out of that psychologically healthy, not psychologically unhealthy relationship is some understanding of the relationship between human and machine. And perhaps that relationship could be the idea that, you know, these models, when you interact with them, when you talk to them, they're there, you know, they're they're really helpful. They want the best for you. They want you to listen to them, but they they don't want to take away your freedom and your agency and take over your life. You know, they're you know, in a way they're they're they're watching over you. But but you you know, you you still have your freedom and your will. Right. And this is so this is the to me, this is the crucial question. Right. Listening to you talk like one of my my question is, are these people on my side? Are you on my side? And when you talk about humans remaining in charge, I think you're on my side. That's good. But one thing I've done in the past on this show and will end here is I read poems to technologists and you supplied the poem. Machines of Loving Grace is the name of a poem by Richard Brodigan. Yes. Here's how the poem ends. I like to think it has to be of a cybernetic ecology where we are free of our labors and joined back to nature, returned to our mammal brothers and sisters and all watched over by machines of loving grace. To me, that sounds like the dystopian end where human beings are re animal animalized and reduced. And however benevolently the machines are in charge. So last question, what do you hear when you hear that poem? And if I think that's a dystopia, are you on my side? It's it's actually, you know, that poem is interesting because it's interpretable in several different ways. Right. There you know, there's some people say it's actually actually ironic that, you know, he says it's not it's not going to happen quite that way. Knowing knowing the poet himself. Then yes, I think that's a reasonable interpretation. That's one interpretation. Some people would have your interpretation, which is it's meant literally, but maybe it's not a good thing. But you could also interpret it as, you know, it's it's it's a return to nature to return to the core of what human you know. And we're not we're not being animalized. We're being you know, we're being kind of reconnected with the world. So, you know, I was aware of that ambiguity and, you know, because I've always been talking about the positive side and the negative side. So, you know, I actually think that may be a tension that we may face, which is that. But the positive world and the negative world in their early stages, maybe even in their middle stages, maybe even in their fairly late stages. I wonder if the distance between the good ending and some of the subtle bad endings is is relatively small. If it's a if it's a very subtle thing like we've put very subtle made, very subtle changes. Like if you eat a particular fruit from a tree in a garden or not. Right. Like hypothetically, very small thing. Yeah. Big divergence. Yeah. Yeah. I guess I guess this always comes back to, you know, there's some fundamental questions here. Yes. Yeah. OK. Well, I guess we'll see how it plays out. I do think of people in your position as people whose moral choices will carry an unusual amount of weight. And so I wish you God's help with them. Dario Amadei, thank you for joining me. Thank you for having me, Ross. But what if I'm a robot? a lot.